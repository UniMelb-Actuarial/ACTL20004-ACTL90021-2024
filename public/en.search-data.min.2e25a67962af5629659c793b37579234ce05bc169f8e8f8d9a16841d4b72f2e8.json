[{"id":0,"href":"/docs/0-subject-guide/Eligibility/","title":"Eligibility and Requirements","section":"Subject Guide","content":"To view the eligibility and requirements, including prerequisites, corequisites, recommended background knowledge and core participation requirements for this subject, please see the University Handbook:\n  ACTL20004 handbook  ACTL90021 handbook  See also Excel assumed knowledge.\n"},{"id":1,"href":"/docs/0-study-plan/week-1/","title":"Week 1 Study Plan","section":"Weekly Study Plan","content":"Week 1 Overview #  This week, we will cover the following topics:\n Introduction to the course: what are the topics we will cover in this subject, and how are they connected? Risk and Insurance (Module 1.1-1.3): to put it in the way the CM2 notes puts it, how do we model the behaviours of agents on a market? What is a possible mathematical model for ‚Äúrational‚Äù behaviour, that could explain why people purchase insurance? We will also introduce the Excel component of the subject (Module 6.1)  See also detailed learning outcomes of 1.2 (utility theory) of the CM2 syllabus here.\n --  -- If you wish to watch the embedded videos from Lecture Capture, you need to have logged in and entered Lecture Capture via Canvas once for each session. This is to restrict access to students enrolled at the University of Melbourne only. -- Main references and lectures #  Prerequisite knowledge #  Please review the Prerequisite knowledge page for this course. This is not exhaustive by any means, but provides minimum knowledge you should be comfortable with. Note that Tutorials in Week 1 are your opportunity to ask about those topics.\nModule 1: Utility, Risk and Insurance #  Read: Module 1: Utility, Risk and Insurance\nAnnotate: slides\nWatch: refer to your lecture recording under ‚ÄúLecture Capture‚Äù ( UG/ PG). This is where annotated slides will be made available, too.\nModule 6: Utility, Risk and Insurance #  Read: Module 6: Excel topics\nAnnotate: slides\nWatch: refer to your lecture recording under ‚ÄúLecture Capture‚Äù ( UG/ PG). This is where annotated slides will be made available, too.\nAdditional preparation and resources #  Mandatory #   Read the Subject Guide. Review the prerequisite knowledge. Chapter 7.1-7.3 and 8.1-8.3 of Joshi (2013) (for Module 1)  Optional #   Unit 2 of IoA (2023) Chapter 2 of Insurance Risk and Ruin (2016)  Tutorials #  Please let us know of any mistake or required update on Ed.\nTutorial materials #  This week #  To be announced after the lectures.\n --  -- If you wish to watch the embedded videos from Lecture Capture, you need to have logged in and entered Lecture Capture via Canvas once for each session. This is to restrict access to students enrolled at the University of Melbourne only. -- Next week (week 2) #  To be announced after the lectures.\nPreparation for assessment #  Mid-semester (15%) and final (60%) exams #   Nothing to do for now - just don‚Äôt fall behind üòÑ.  Assignment (25%) #   Nothing to do for now.  References #  Insurance Risk and Ruin. 2016. 2nd ed. Cambridge University Press.\n IoA. 2023. Course Notes and Core Reading for Subject CM2 Models. The Institute of Actuaries.\n Joshi, Mark Suresh. 2013. Introduction to Mathematical Portfolio Theory. Cambridge University Press.\n  "},{"id":2,"href":"/docs/0-subject-guide/SILO/","title":"Learning Outcomes","section":"Subject Guide","content":"Subject Intended Learning Outcomes (SILO) #  To view the subject objectives and the generic skills you will develop through successful completion of this subject, please see the University Handbook.\nACTL20004 SILO #  The ACTL20004 subject intended learning outcomes (SILO) are as follows:\n Derive and apply results for simple stochastic models for investment returns. Describe and apply techniques for analysing a delay (or run-off) triangle and projecting the ultimate position. Illustrate the concept of ruin and analyse the significance of the adjustment coefficient in ruin theory. Use utility theory to describe the main aspects of risk and insurance. Build and manipulate financial models in spreadsheet software. Apply pre-requisite mathematical and statistical concepts to the solution of problems on the above topics.  ACTL90021 SILO #  The ACTL90021 subject intended learning outcomes (SILO) are as follows:\n Critically review simple stochastic models for investment returns. Compose and assess techniques for analysing a delay (or run-off) triangle and projecting the ultimate position. Elucidate the concept of ruin and commentate on the significance of the adjustment coefficient in ruin theory. Contextualise the main aspects of risk and insurance using utility theory. Compose and transform financial models in spreadsheet software. Compose pre-requisite mathematical and statistical concepts to solve problems related to the above topics.  Mapping to the IFoA Financial Engineering and Loss Reserving (CM2) subject #  This subject is one of three subjects covering the contents of the CM2 ‚ÄúFinancial Engineering and Loss Reserving‚Äù subject of the Institute and Faculty of Actuaries. This counts towards the ‚ÄúFoundation Program‚Äù of the Australian Actuaries Institute professional curriculum and syllabus.\nIn particular, ACTL20004/90021 is meant to cover Items 1.2, 3 and 5 of the CM2 syllabus. This is detailed in the following table, with mapping towards the course modules (and major reference in parentheses; see ‚ÄúSubject Resources‚Äù below), as well as subject intended learning outcomes (SLO).\nA detailed mapping is available in the subject guides - UG, PG\n"},{"id":3,"href":"/docs/0-subject-guide/Activities/","title":"Activities","section":"Subject Guide","content":"Lectures #  Lecture Times #  Lectures will be delivered live, in dual mode (both on Zoom and in person) in the following venue:\n [PG] FBE Theatre 2, Mondays, from 10:05am to 11:55am [UG] FBE Prest Theatre, Mondays, from 2.20pm to 4.10pm  Notes:\n Lectures and tutorials run in week 6 as normal. The mid-semester exam will be on Wednesday night. Details about readings are available below under \u0026ldquo;Subject Resources\u0026rdquo;. They can be downloaded from the Canvas Readings Online.  Note this subject is one-quarter of a standard load. That means, including attendance at classes, you should be putting the equivalent of one quarter of a week‚Äôs work into this subject, i.e. 9 to 10 hours.\nLecture Slides and Materials #  All materials are available on this website.\nThe most salient exception are lecture recordings, with annotated slides, which are cohort specific, and which will be available with the associated recording on \u0026ldquo;Lecture Capture\u0026rdquo;.\nRecorded Lectures #  Audio and video recordings of lectures delivered in this subject will be made available for review. These recordings allow you to revise lectures during the semester, or to review them in preparation for the end of semester exam.\nYou can access recorded lectures by clicking on the \u0026ldquo;Lecture Capture\u0026rdquo; menu item on the LMS page for this subject.\nRecordings are not a substitute for attendance; rather they are designed for revision. On rare occasions the recordings can fail to take place due to technical reasons. In such cases, a substitute recording will not be made available.\nTutorials #  Tutorial Times #  Tutorials start in week 1. Please check your timetable and the University timetable for details. You must attend the tutorials you are enrolled in.\nTutorials will not be recorded.\nTutorial Details and Materials #  The tutorial schedule is provided in the plan above.\nThere will be two types of tutorial questions:\n Pre-tutorial exercises, to be completed prior to the tutorial, and with solutions available straight away. Tutorials won‚Äôt discuss those questions in the tutorials, but will, of course, answer questions if you have any. Tutorial exercises have been especially selected as more advanced exercises facilitating revisions and discussions during the face to face tutorials. Solutions will only be released in the morning of the tutorials, to encourage students to have a go by themselves prior to the tutorial, but still allowing them to have something to annotate during the tutorial.  In the past, solutions were only released after the tutorials. Feedback from the ESS in 2023 suggested that releasing them immediately prior to the tutorial would be beneficial. We are trying this new approach in 2024.  Private Tutoring Services #  The Faculty has become increasingly concerned about the existence of a number of private tutoring services operating in Melbourne that heavily target University of Melbourne students enrolled in FBE subjects.\nStudents are urged to show caution and exercise their judgement if they are considering using any of these services, and to please take note of the following:\n Any claim by any of these businesses that they have a \u0026ldquo;special\u0026rdquo; or \u0026ldquo;collaborative\u0026rdquo; or \u0026ldquo;partnership\u0026rdquo; style relationship with the University or Faculty is false and misleading. Any claim by a private tutoring service that they are in possession of, or can supply you with, forthcoming University exam or assignment questions or \u0026ldquo;insider\u0026rdquo; or \u0026ldquo;exclusive\u0026rdquo; information is also false and misleading. The University has no relationship whatsoever with any of these services and takes these claims very seriously as they threaten to damage the University‚Äôs reputation and undermine its independence.  It is also not appropriate for students to provide course materials (including University curricula, reading materials, exam and assignment questions and answers) to operators of these businesses for the purposes of allowing them to conduct commercial tutoring activities. Doing so may amount to misconduct and will be taken seriously. Those materials contain intellectual property owned or controlled by the University.\nWe encourage you to bring to the attention of Faculty staff any behaviour or activity that is not aligned with University expectations or policy as outlined above.\n"},{"id":4,"href":"/docs/0-subject-guide/Resources/","title":"Resources","section":"Subject Guide","content":"Website #  The current website contains (or links) to all the resources you need for the course (compare the following list with left hand side menu):\n  Subject guide (you are reading it!)  Prerequisite knowledge: assumed knowledge in Excel  Utility theory (CM2 1.2): the contents covering Section 1.2 of the CM2 syllabus.  Reserving (CM2 5.2): the contents covering Section 5.2 of the CM2 syllabus.  Excel (CM2 Exam B): the contents covering the Excel component of the CM2 syllabus.  Stochastic interest (CM2 3): the contents covering Section 3 of the CM2 syllabus.  Ruin Theory (CM2 5.1 5.3): the contents covering Section 5.1 and 5.3 of the CM2 syllabus.  Ed discussion forum: link to the main interaction fora  Tutorial exercises: to be discussed in tutorials. Solutions will be posted at the end of the relevant week.  Details about assessment will be available under Subject Guide / Assessment\nPrescribed readings #  Prescribed references are:\n [Joshi] For Module 1 (Chapters 7 and 8) Joshi, M., Paterson, J. M. (2013), Introduction to Mathematical Portfolio Theory, Cambridge University Press, ISBN 9781107042315 [Taylor] For Modules 2-5 (Chapters 1 to 5) Taylor, G. (2000) Loss Reserving: An Actuarial Perspective Huebner International Series on Risk, Insurance and Economic Security (HSRI volume 21) https://link.springer.com/book/10.1007/978-1-4615-4583-5 [EE19] For Excel Slager, D., Slager, A. (2020) Essential Excel 2019 Apress, 2nd Edition, ISBN 978-1-4842-6208-5, https://doi.org/10.1007/978-1-4842-6209-2 [AE23] For Excel Katz, A. I. (2023) Up Up and Array! Dynamic Array Formulas for Excel 365 and Beyond, Apress, ISBN 978-1-4842-8965-5, https://doi.org/10.1007/978-1-4842-8966-2 [Dickson] Dickson, D. C. M. (2016) Insurance Risk and Ruin, 2nd Edition, Cambridge University Press, https://doi.org/10.1017/9781316650776  Most of those references can be downloaded from the Readings Online section of the Canvas community website.\n"},{"id":5,"href":"/docs/0-subject-guide/Assessment/","title":"Assessment","section":"Subject Guide","content":"Assessment Overview #  Assessment structure #  Your assessment for this subject comprises the following:\n  a 50 minute closed book mid-semester exam (15%)  an individual assignment (25%)  a 2 hour closed book final exam (60%)  Hurdle requirement: To pass this subject students must pass the end of semester examination.\nIn the following sections, details are provided (and will be populated throughout semester) about those assessments.\nMid-semester exam #  Date #  The mid-semester exam will take place on Wednesday 28 August 2024 between 5pm and 7pm. Please arrive by 5.15pm at the latest. The exam won\u0026rsquo;t start before 5.30pm, but it may start later, so please make sure you are available until 7pm.\nType #  The mid-semester exam will be a \u0026ldquo;traditional\u0026rdquo;, closed book, pen on paper exam.\nYou will be allowed to bring one A4 sheet of notes, double sided. You can write as much as you want on that one sheet of paper (text and/or formulas), on both sides, but it cannot be more than one A4 sheet.\nYou will be allowed to bring a Casio FX-82 (with or without suffix) calculator.\nVenue #   Wilson Hall\nDuration #  50 minutes sharp, plus 5 minutes reading time. You are not allowed to write during reading time.\nScope #  The mid-semester exam will assess contents from Modules 1 to 4.2 (lectures of weeks 1-5) and associated tutorial exercises, readings, spreadsheets, and course contents. This focuses on the risk and insurance, as well as all of reserving components of the subject.\nPractice #  The exam will be of the \u0026ldquo;traditional\u0026rdquo; type, in line if the IFoA CM2A exams, as well as the 2023 Mid-semester Exam here; see below for details.\nAdditional details #  See General Advice for further details.\nIndividual assignment #  Assignment instructions #  The assignment will be an assignment which will need to be performed in Excel, which will be specifically assessed.\nThe assignment will assess skills from Excel and Stochastic returns (TBC) and associated tutorial exercises, readings, spreadsheets, and course contents. Communication skills will also be assessed.\nAdditional details and instructions will be available later (around mid-semester exam time).\nAssignment submission #  Assignment submission is via the LMS Assignment Submission link for all written assignments. Please refer to the Turnitin section of the LMS website via for detailed submission instructions if needed.\nPlease note that you are required to keep a copy of your assignment after it has been submitted as you must be able to produce a copy of your assignment at the request of teaching staff at any time after the submission due date.\nPenalties for Late Submission #  In order to ensure equality for all students, assignments and examinations (where relevant) must be submitted by specified deadlines. Late assignments, where approval for late submission has not been given, will be penalised at the rate of 20% of the total mark per day, for up to 5 days, at which time a mark of zero will be given.\nStudents with a genuine and acceptable reason for not completing an assignment (or other assessment task), such as illness, can apply for special consideration. Special Consideration assists students who have been significantly affected by illness or other serious circumstances during the semester. The following website contains detailed information relating to who can apply for Special Consideration and the process for making an application: http://students.unimelb.edu.au/admin/special\nFinal exam #  Type #  The final exam will be an invigilated exam on campus. There will be a 10 minute reading time. The focus of the final exam will be on the whole subject, with particular focus on the second half of the subject. Subject to confirmation, you may have a university-provided laptop to perform some calculations on Excel.\nYou will be allowed to bring one A4 sheet of notes, double sided. You can write as much as you want on that one sheet of paper (text and/or formulas), on both sides, but it cannot be more than one A4 sheet.\nYou will be allowed to bring a Casio FX-82 (with or without suffix) calculator.\nDate #  The date of the final exam is published on the University of Melbourne Exams Timetable website.\nDuration #  2 hours, plus 15 minutes of reading time.\nScope #  The final exam will assess all contents of the course, with particular focus on the materials taught beyond week 5.\nExaminable materials include associated tutorial questions (it is important that you understand the details of the examples /questions in these materials and that you are able to replicate the solutions independently and efficiently), associated mandatory readings, as well as the assignment task and the mid-semester exam.\nMarks and allocation to topics #  Will be available later.\nExam consultation #  Will be announced later.\nAdditional details #  See also the University of Melbourne exam webpage for students.\nPrecision of numerical answers #  Students often ask about the required significant figures / decimal places.\nIf it is not specified:\n In an open question we will look at the reasoning so you should not be too worried about it (within reason). Use, say, 5 significant figures/digits \\(*\\) Otherwise the precision needs to match that of the possible answers (if we have 5 significant figures ( \\(*\\) ) in the answers then work with 6, say).  \\(*\\) Please move away from talking about decimal places - this does not take the scale of your problem into account. The correct way to think about this is in terms of significant figures/digits. For more details, see e.g. Wikipedia or this video from Khan academy.  Exam preparation #  Assessments in this course will generally:\n test whether you can apply the methods in a realistic environment (in particular, with data and spreadsheets); test whether you understand the theory behind those methods, and can explain the associated strengths and limitations.  To prepare for this exam we suggest you review the tutorial exercises and past CM2 exam questions. You should also make sure you read all prescribed readings and have understood the main arguments.\nMore specifically:\n Finalise your own summaries and formula sheets in preparation for the exam. I recommend you review carefully the Detailed Learning Outcomes pages: Of course, review also all tutorial exercises. For each question:  What skill(s) is this exercise testing? Where are the required concepts in your summaries? in your formula sheets? What did you find challenging in this exercise, and how comfortable are you with this now?    General Advice #  Past CM2 exams #  This subject is one of three subjects leading to exemption from the IFoA CM2 exam; see also the SILO page. This provides with a very extensive range of practice exams. Note:\n CM2A exams are closed book pen on paper exams CM2B exams are Excel based Past exams and solutions are downloadable here  Tips #  Remember there are different ways of answering a multiple choice/answer question (especially if it involves numbers):\n Work out the answer as if you were in a standard pen and paper invigilated exam, then see if the answer is in the list. Work out the answer with tools that would not otherwise be available, such as Excel spreadsheet, R, or even the internet. (but be careful not to collude or otherwise be guilty of academic misconduct ‚Äì check https://academicintegrity.unimelb.edu.au/ for further details) In some circumstances, it may be easier to simply ‚Äúplug back‚Äù the different answers to see which one works. Don‚Äôt forget these three options exist for multiple choice/answer open book exams! Hence make sure you have Excel and R available and working before the exam starts.  Furthermore, some more advice / exam technique / tips:\n Read attentively the exam questions at least twice before attempting the question. Work out what skills you are meant to demonstrate. Make sure you understand what is being asked in the question ‚Äì consider key words indicating what you have to do. When you have finished, re-read the question to make sure you have not missed anything. Key is to demonstrate understanding. Attempt all questions where possible. If you run out of time, explain how to you would approach the task (where possible). If this is right this should give you marks. Do questions you are most confident with first. Prioritise. Observe the number of marks allocated. Manage time accordingly. Reassess your available time regularly (say, half way through for a 1 hour exam, and perhaps 2 times for a 2 hour exam). For instance, if the exam is one hour and there are 100 marks, then you have 36 seconds per mark.  Pay close attention to the wording of the question - this is not chosen randomly. Pay particular attention to the verb (for instance: \u0026ldquo;State\u0026rdquo; and \u0026ldquo;Show\u0026rdquo; do not mean the same thing. With \u0026ldquo;State\u0026rdquo; you just need to give the answer, with \u0026ldquo;Show\u0026rdquo; you need to actually prove the answer) and adverb if any (for instance: \u0026ldquo;briefly\u0026rdquo; is not the same as \u0026ldquo;in detail\u0026rdquo;). Examples are:\n ‚Äò\u0026hellip;Justify your answer‚Äô ‚ÄòFind‚Äô ‚ÄòExplain in words‚Äô ‚ÄòDerive‚Äô ‚ÄòShow that‚Äô ‚ÄòBriefly list‚Äô ‚ÄòPresent in detail\u0026hellip;‚Äô ‚ÄòBriefly explain‚Äô ‚ÄòBriefly describe‚Äô ‚ÄòState (without justification) 4 examples of \u0026hellip;‚Äô  "},{"id":6,"href":"/docs/2-reserving/m2-basic-reserving-concepts/","title":"M2 Basic Reserving Concepts","section":"Reserving (CM2 5.2)","content":"Introduction - Claims liabilities #  Definition #   Generally in insurance, premiums are collected first, before coverage is provided. The insurer can‚Äôt consider this premium as income before coverage is provided. Furthermore, as times goes by, the insurer does not know instantly how much losses it will have to pay (e.g., delays in notification, delays in payments).    This gives rise to two types of liabilities:  Unearned premium liability: an amount set aside for covering losses related to coverage not yet provided, but where a premium has already been collected; Outstanding claim liability: an amount set aside for for covering losses corresponding to coverage that has already been provided, but where payments are still expected to occur.   These take a significant proportion of an insurer‚Äôs balance sheet, often multiple times its equity, as exemplified in the IAG example (forthwith). It is the actuary‚Äôs responsibility to determine those amounts  It‚Äôs a huge responsibility!\nThe case of IAG #  Insurance Australia Group is one of the largest Australian general insurers (the largest under some measures)\nIts latest Balance Sheet (as at 31 December 2022, see page 20) shows\n Assets  Total: 34,428m (100%)   Liabilities  Unearned premium liability: 7,084m (20.6%) Outstanding claims liability: 13,560m (39.4%)   Equity  Total: 6,819m (19.8%)    It is worth noting that IAG holds 2.01 times the APRA ‚ÄúPrescribed Capital Amount‚Äù (PCA), see F on page 12.\nSee https://www.iag.com.au/results-and-reports to dowload the\nlatest financial statements of IAG.\nThe claims process #  Main reference #  Note that figures, notation, examples and structure are mostly drawn from Taylor (2000), which is the main reference for Modules 2‚Äì5.\nTime line of a claim and IBNR #  From \\(t_1\\) to \\(t_2\\):\n A liability exists (an insured event occurred) However, the insurer is unaware of the claim During this interval, the claim is said to be incurred but not reported (or ‚ÄúIBNR‚Äù).   From \\(t_2\\) and \\(t_9\\):\n There will be future payments, even though the insurer may not know with certainty about them These should be captured by an appropriate reserve.  Inflation #   Because claims are paid at different points in time, they are subject to inflation. Inflation will generally be different from CPI, and is typically specific to a line of business. Let \\(\\lambda(t)\\) be an index describing such claims inflation. Inflation may continue beyond the point of occurrence of a claim, up to the point of payment.  This can have a very long duration (e.g.¬†income in workers compensation)     We define: $$C^* = \\sum_{j=3,4,5,8} C(t_j)$$ as the sum of payments \\(C(t_j)\\) all expressed in dollars as of \\(t_1\\) (date of occurrence), and $$C = \\sum_{j=3,4,5,8} C(t_j) \\frac{\\lambda(t_j)}{\\lambda(t_1)},$$ as the sum of the nominal (actual) amounts paid.\n(Note that superimposed inflation - Section 1.1.4 - is not in scope this year)\nEstimates of outstanding loss liability #  General #  Now assume that an evalutaion of liability is required at time \\(t\\):\n Note\n if \\(t\\ge t_1\\): the liability exists, and ideally the claim should be considered in some shape or form. if \\(t_1 \\ge t \\ge t_2\\): claim is unknown, but the insurer knows that there are unknowns. It will try to estimate those as ‚ÄúIBNR‚Äù (see also next module). if \\(t\\ge t_2\\): claim will be known, and the insurer will try to estimate its cost (case reserve). between \\(t_6\\) and \\(t_7\\), as well as after \\(t_9\\), the insurer‚Äôs estimate will be zero (because the claim is deemed closed).  Time dimensions of ‚Äúperiods‚Äù #  Let:\n a \\(period\\) be some appropriate measure of time length, such as a month, quarter or year.  Now when referring to a specific claim payment, it can be classified according to:\n \\(i\\) being the period of origin where \\(i=0,\\ldots, I\\)  this is the period when the claim occurred (in most cases) or reported (in the rare case of ‚Äúclaims-made‚Äù policies)   \\(j\\) being the development period where \\(j=0,\\ldots\\)  this is difference in period index between occurrence and payment (0 if the payment happens in the same period as occurrence)   \\(k=i+j\\) being the calendar period where \\(k=0,\\ldots\\)  \\(k=0\\) is the earliest period with data also called experience period     This illustrates those concepts:\nRelation between periods of origin, development and experience.\nFurther notation #   \\(C(i,j)\\): paid losses in the \\((i,j)\\) cell Outstanding loss liability at the end of \\(j\\) (or \\(k\\)) for \\(i\\): $$ P(i,j)\\equiv \\overline{P}(i,k) \\equiv \\sum_{m=j+1}^\\infty C(i,m)$$ (note you can replace \\(j\\) with \\(k-i\\) in the lower bound) Total outstanding loss liability at the end of \\(k\\) - for all periods of origin: $$ \\overline{P}(k) = \\sum_{i=0}^I \\overline{P}(i,k) = \\sum_{i=0}^I \\sum_{m=k-i+1}^\\infty C(i,m).$$ Both \\(\\overline{P}(i,k)\\) and \\(\\overline{P}(k)\\) are aggregate amounts (not claims based). While the textbook says that most actuarial estimates are aggregate, individual claims reserving has gathered a lot of momentum\nsince then, thanks to data availability, computing power\nand machine learning capabilities.  Components of outstanding loss liability #  Cash flows #  The equation above is about ‚Äúcompleting the rectangle‚Äù,\nas illustrated above.\nClaims inflation #  Usually, the reserving technique will yield estimates of \\(C^*\\), that is, as of dollars of some base date (usually current date), so that $$C(i,j) = C^*(i,j)\\frac{\\lambda(k)}{\\lambda_0},$$ where\n \\(\\lambda(k)\\) is the relevant claims inflation index; \\(\\lambda_0\\) is the value of that index on the base date.  (again, superimposed inflation is out of scope, but can conceptually be seen as adjusting \\(\\lambda\\) appropriately)\nLoss adjustment expenses #  We consider:\n allocated loss adjustment expenses (ALAE)  These can be linked to specific claims. These are typically included in the \\(C(i,j)\\) and don‚Äôt require extra treatment.   unallocated loss adjustment expenses (ULAE)  These cannot be easily linked to specific claims (think overhead costs, for instance). These are typically not in the \\(C(i,j)\\) and will require specific consideration if they are to be included. Denote $$ e(i,j)$$ as the proportion of paid losses to be added to costs for ULAE.    Investment income #   Liabilities will have assets covering them on the balance sheet. These may be invested, and it may be required to consider corresponding investment returns. That is, if the $10 you have to pay in 5 years can be covered with $9 today (even after allowing for inflation), then you may be required/allowed to reserve $9 instead of $10. Let \\(d(k)\\), \\(k=I+1, I+2, \\ldots\\), be the discount factor to be applied for payments made in experience period \\(k\\) (in the example above, we would have \\(d(I+5)=9/10\\)) . Note that \\(\\lambda \\neq d\\): there is no reason why claims inflation should evolve in the same identical way to your investments.   In the end, we have:\n\\begin{eqnarray*} \\overline{P}(k) \u0026amp;=\u0026amp; \\sum_{i=0}^I \\sum_{m=k-i+1}^\\infty C(i,m) \\left[1+e(i,m)\\right]d(i+m) \\\\ \u0026amp;=\u0026amp; \\sum_{i=0}^I \\sum_{m=k-i+1}^\\infty C^*(i,m) \\frac{\\lambda(i+m)}{\\lambda_0}\\left[1+e(i,m)\\right]d(i+m). \\end{eqnarray*}\nRecoveries #  Possible recoveries (offsetting claims):\n reinsurer payments; third parties at fault; salvage value (for property insurance).  Generally speaking, insurers have a subrogation right, which allows them to seek compensation on behalf of the insured if they believe it is worth the effort.\nThis can be considered as negative \\(C(i,j)\\), or may require another triangle altogether. Specific treatment is outside scope here.\nLoss reserving #  General #   So far, we have defined the ‚Äúoustanding loss liability‚Äù as a random variable - this is the actual amount that will have to be paid. In general this is unknown, and now the question is how to estimate this, and how much to set aside for covering those liabilities. It makes sense that you would reserve its expectation plus some margin, so that we define:  Expected outstanding loss liability: this is the expectation, also called central estimate; Prudential margin or risk margin: this is the margin you add to the central estimate to allow for estimation error, and ‚Äúworse than average‚Äù cases.   In the end we have: $$\\text{loss reserve }= \\text{ central estimate }+ \\text{ risk margin}.$$  In Australia #  Legislative framework #  The Standard governing actuarial valuations of insurance liabilities is found at www.legislation.gov.au\n Section 8 states that ‚ÄúAn insurer must determine a value for both its outstanding claims liabilities and its premiums liabilities for each class of business underwritten by the insurer.‚Äù Sections 9 defines ‚Äúoutstanding claims liabilities‚Äù as relating ‚Äúto all claims incurred prior to the valuation date, whether or not they have been reported to the insurer‚Äù. Section 10 defines ‚Äúpremium liability‚Äù as relating ‚Äúto all future claim payments arising from future events post the valuation date that will be insured under the insurer‚Äôs existing policies that have not yet expired‚Äù.  Outstanding claims liability in Australia #  APRA‚Äôs GRS 210 (where \u0026quot;RS\u0026quot; stands for \u0026quot;Reporting standards\u0026quot;) says:\nThe valuation of outstanding claims liabilities for each class of business must comprise:\n a central estimate (refer below); and a risk margin (refer below) that relates to the inherent uncertainty in the central estimate values.   Furthermore, it continues on by clarifying:\nThe valuation of insurance liabilities (i.e.¬†outstanding claims liabilities and premiums liabilities) reflects the individual circumstances of the Level 2 insurance group. In any event, the value of insurance liabilities must be the greater of a value that is:\n determined on a basis that is intended to value the insurance liabilities of the Level 2 insurance group at a 75 percent level of sufficiency; and the central estimate plus one half of a standard deviation above the mean for the insurance liabilities of the Level 2 insurance group.  There‚Äôs a lot more information in the document.\nSolvency capital requirements in Australia #   Additionally, solvency requirements (look for ‚ÄúGPS‚Äù, where ‚ÄúPS‚Äù stands for ‚ÄúPrudential Standards‚Äù) require insurers to hold capital corresponding (typically) to a 99.5-ile at the company‚Äôs level (including diversification benefits), which requires knowledge of the distribution of future payments quite far in the tail, and how they interact across lines of business. This is similar in Europe with Solvency II. Of course, it is a little more complicated than that, but the main point is that you might need to work out the distribution of payments quite far in the tail.  Data #  Nature #  Generally, available data includes at least:\n claim counts (open / closed); claim payments; case estimates.  Nowadays, one also typically has:\n details of payment dates for each claim; details of case reserve amounts and associate revision dates for each claim; additional data such as policy data, claims qualitative data (description of the event), etc‚Ä¶  These can be (are nowadays increasingly) used for improving reserving, but this is outside scope of this subject.\nAggregate data #  Incremental data #  In the case of aggregate data, each cell \\((i,j)\\) will have data of four types:\n \\(N(i,j)\\) are the counts of claims notified; \\(C(i,j)\\) are paid losses; \\(U(i,j)\\) are the counts of claims open ; \\(Q(i,j)\\) are the case estimates of outstanding liability.  Because those numbers refer to cell \\((i,j)\\) only, we often refer to the as incremental data.\nCumulative data #  It is sometimes convenient to express data in cumulative terms.\nWe define: $$A(i,j) = \\sum_{m=0}^j N(i,m)$$ as cumulative counts data, with $$N(i) = A(i,\\infty)$$ being the ultimate count of incurred claims.\n Payment data can be similarly accumulated: $$D(i,j) = \\sum_{m=0}^j C(i,m)$$ such that $$I(i,j) = D(i,j) + Q(i,j)$$ are the estimated amounts of incurred losses (note that even if this is indexed \\((i,j)\\) it is of cumulative nature).\nReferences #  Main reference:\nTaylor, Greg. 2000. Loss Reserving: An Actuarial Perspective. Huebner International Series on Risk, Insurance and Economic Security. Kluwer Academic Publishers.\n  "},{"id":7,"href":"/docs/0-subject-guide/","title":"Subject Guide","section":"Docs","content":"Subject guide information\n\nIn this section we outline the main elements of the full subject guide UG, PG, which you should read as it includes additional relevant information (such as for special consideration) which is not included here but might be relevant to you, and which we will assume you are aware of.\nNote that consultation hours with Professor Avanzi and Dr Chen will be:\n during semester on Wednesdays, 11:00-12:00, in FBE 323 (weeks 1-6) or 318 (weeks 7-12)   Note that there won‚Äôt be consultation during the non-teaching week, and exam consultation will be advertised later.\nStudents should ask all course content questions in Ed, during tutorials, or during consultation hours. E-mail queries should be restricted to those of an administrative and personal nature. Any query that could be of relevance to other students will only be answered during face-to-face tutorial and consultation time, or in Ed.\n"},{"id":8,"href":"/docs/1-utility/m1-risk-insurance/","title":"M1 Utility, Risk and Insurance","section":"Utility theory (CM2 1.2)","content":"Introduction #  Decision making under risk and uncertainty #  Context #   In an Enterprise Risk Management context (ERM), ‚Äúrisk‚Äù is defined as the ‚Äúeffect of uncertainty on objectives‚Äù So essential ingredients here are:  having an objective (e.g.¬†maximise profit under some solvency constraint) having to make a decision (e.g.¬†decide on how much to (re)insure) the object is subject to uncertainty (e.g.¬†the amount of losses an insurer will have to pay)    Understand decisions #   In economics (and social science in general), one can be interested in understanding / modelling how and why individuals make decisions. The ‚Äúproblem‚Äù in social sciences (as opposed to ‚Äúhard‚Äù sciences) is that human behaviour is subject to complex forces. How a human makes a decision is not as deterministic as calculating, for instance, how long a rock would take to reach the ground if dropped at a certain altitude and under known conditions. Things can get complicated but in science you can theoretically calculate this to any level of precision. Here we assume that we know what the objectives and constraints are, and that decisions will depend on our model of the ‚Äúrisk‚Äù. The question is then - how do people make decisions in that context? A sub-question, of particular interest in this subject, is why do individuals choose to insure, even though insurance covers are typically charged at a much higher cost than expected value?  Preliminary: the easy case #  First order stochastic dominance #    If possible outcomes of the risk can be ranked according to stochastic dominance then we are in relatively simple cases.\n  First order stochastic dominance is defined as follows. Outcome \\(A\\) dominates \\(B\\) if (assuming \\(x\\) is formulated in terms of gains) $$F_A(x) \\le F_B(x)\\text{ for all }x,\\text{ and }$$ $$F_A(x)\u0026lt;F_B(x)\\text{ for (at least) some value of }x,$$ that is, the probability that \\(B\\) will yield an outcome higher then a given threshold is never bigger than for \\(A\\), and there is at least one instance where it is strictly less.\n  This is strong dominance, and no-one in their right mind would choose \\(B\\) over \\(A\\) from a pure financial perspective.\n  Note that we could still have an outcome of \\(A\\) being lower than\nfor \\(B\\) - the dominance is in probabilistic terms.\n   For example, two normal distributions with different means, and same variance:\nSecond order stochastic dominance #   Second order stochastic dominance is a slightly weaker version, whereby, in ‚Äúaggregate‚Äù and from the ‚Äúground up‚Äù, \\(A\\) dominates \\(B\\). Mathematically, this means (assuming \\(x\\) is formulated in terms of gains) that if $$\\int_{-\\infty}^x F_A(y) dy \\le \\int_{-\\infty}^x F_B(y) dy \\text{ for all }x,$$ $$\\text{ with strict equality holding for some value of }x,$$ then \\(A\\) displays second order stochastic dominance over \\(B\\). Rather than comparing \\(F\\)‚Äôs at all \\(x\\) individually, we look at the whole surface under it, from \\(-\\infty\\) to \\(x\\). This means that we allow from some local violations (‚Äúhigher‚Äù \\(F\\)‚Äôs) of the first order stochastic dominance, provided they are compensated by at least as many ‚Äúgains‚Äù (‚Äúlower‚Äù \\(F\\)‚Äôs) in aggregate ‚Äúso far‚Äù (up to \\(x\\)). Of course, first order stochastic dominance implies second order stochastic dominance.   For example, if two normal disributions have the same mean but different variances, the one with lower variance displays second order dominance over the other.\n We have second order dominance, but not first order dominance:\nIn absence of stochastic dominance #   Then things get less clear‚Ä¶ This is explored in the next section  Some motivating examples #  Umbrellas vs Ice creams #  Ella is considering investing all her savings in either selling umbrellas or ice creams over the next season. Her profit will depend on the weather as follows. What is she going to choose?\n   Weather Umbrellas Ice creams Probability     Bad 30,000 2,000 1/4   OK 4,000 8,000 1/2   Sunny 2,000 16,000 1/4     In a probability course, one would calculate the expected value of each option, using probabilities for the different states of nature For umbrellas, this is 10,000; for ice creams, this is 8,500; One would then conclude that Ella will choose to sell umbrellas (due to expectation maximisation).  \\(\\;\\;\\;\\) What would YOU choose?\nExample 7.2 of Joshi (2013) #  In a game, a true coin is tossed.\n If the coin comes up heads, you get 11 dollars If the coin comes up tails, you must pay 10 dollars  Would you play this game?\nExample 7.3 of Joshi (2013) #  In a game, a true coin is tossed.\n If the coin comes up heads, you get \\(X\\) dollars If the coin comes up tails, you must pay 100 dollars  For what values of \\(X\\) would you play this game?\nThe St Petersburg paradox #  In a game,\n a true coin is tossed until the first tail comes up let \\(N\\) be the number of tosses that were required you will receive \\(2^N\\) dollars  How much money would you be willing to pay?\nAssume that the game organiser has infinite wealth.\n Because $$\\Pr[N=n]=\\left(\\frac{1}{2}\\right)^n$$ we have that the expected pay-off for this game is $$\\sum_{n=1}^\\infty 2^{-n}\\cdot 2^n = \\infty \\;!$$ So why isn‚Äôt anyone playing this game?\n the odds of winning a large amount of money are (exponentially) small perhaps winning $2 mio is not twice as good as winning $1 mio\n(what about $200 and $100?)   Also, we assumed infinite wealth, which really is what makes it blow up to infinity (those very, very, very unlikely but very, very, very large outcomes). What if \\(N\\) was capped to a \\(n_\\text{max}\\)?\nThe expectation of the game would be $$\\sum_{n=1}^{n_\\text{max}} 2^{-n}\\cdot 2^n = n_\\text{max}$$ and the variance would become $$\\sum_{n=1}^{n_\\text{max}} 2^{-n}\\cdot \\left(2^n \\right)^2 - n_\\text{max}^2 = \\sum_{n=1}^{n_\\text{max}} 2^n - n_\\text{max}^2,$$ which clearly goes to infinity as \\(n_\\text{max}\\) goes to infinity.\nWould you play this game?\nUtility functions #  The Expected Utility Theorem (EUT) #  The concept of utility #   It is a fact that humans don‚Äôt behave according to expectation maximisation. The theory around this is huge, and we only explore the first baby steps. How can we build a model that explains human decisions?  The idea is:\n We ‚Äúmap‚Äù wealth in $ to a quantitative value of ‚Äúhappiness‚Äù, ‚Äúkeen-ness‚Äù, ‚Äúpleasure‚Äù that this wealth procures to the individual. Individuals then make decisions based on this transformed quantity \\(\\rightarrow\\) they will want to maximise the ‚Äúpleasure‚Äù they get from their wealth. This notion of ‚Äúhappiness‚Äù or ‚Äúpleasure‚Äù is called ‚Äúutility‚Äù in economics.  EUT vs expectation maximisation #  According to IoA (2023), the ‚ÄúExpected Utility Theorem‚Äù states that a function, \\(u(w)\\), can be constructed as representing an investor‚Äôs utility of wealth, \\(w\\), at some future date. Decisions are made in a manner to maximise the expected value of utility given the investor‚Äôs particular beliefs about the probability of different outcomes.‚Äù\nNote:\n this means that rather than maximising \\(E[X]\\) for random \\(X\\), we maximise the transformed $ of \\(X\\) into utility, that is, \\(E[u(X)]\\). of course, if \\(u\\) is linear then we just have a change of currency (and potential shift) and both approaches are equivalent. rather than ‚Äúinvestor‚Äù we will refer to ‚Äúdecision maker‚Äù (DM).  Axioms #  There are a number of axioms behind EUT which we need to review.\nNotation:\n Let \\(A\\) and \\(B\\) be two alternative choices \\(A \\succ B\\) means that \\(A\\) is preferred to \\(B\\) \\(A \\sim B\\) means that neither \\(A\\) or \\(B\\) are preferred\nWe say that the DM is ‚Äúindifferent: between \\(A\\) and \\(B\\) \\(A \\succsim B\\) means that \\(B\\) is not preferred over \\(A\\)\n(we can have either \\(A \\succ B\\) or \\(A \\sim B\\))   Axioms:\n Comparability: a DM can state a preference between all available outcomes. Transitivity: $$ A \\succ B \\text{ and } B \\succ C \\Longrightarrow A \\succ C.$$ Independence: If \\(A \\sim B\\) then \\begin{equation*} \\left\\{ \\begin{array}{cccl} A \u0026amp; \\text{ w.p. }\u0026amp; p \u0026amp; \\text{ and } \\\\ C \u0026amp; \\text{ w.p. }\u0026amp; 1-p \\end{array}\\right. \\;\\;\\sim\\;\\; \\left\\{\\begin{array}{cccl} B \u0026amp; \\text{ w.p. }\u0026amp; p \u0026amp; \\text{ and } \\\\ C \u0026amp; \\text{ w.p. }\u0026amp; 1-p \\end{array}\\right. \\end{equation*} Certainty equivalence:  Suppose \\(A \\succ B\\) Suppose \\(B \\succ C\\) Then there exists \\(p\\) such that $$ B ;;\\sim;; \\left{\\begin{array}{cccl} A \u0026amp; \\text{ w.p. }\u0026amp; p \u0026amp; \\text{ and } \\ C \u0026amp; \\text{ w.p. }\u0026amp; 1-p \\end{array}\\right. $$    Examples #  Umbrellas vs Ice creams #  Assume now that Ella makes decision according to a power utility function with \\(\\gamma=0.5\\), that is, $$u(x) = \\frac{x^\\gamma-1}{\\gamma}.$$ A table of utility of outcomes is (rounded from exact)\n   Weather Umbrellas Ice creams Probability     Bad 344.4 87.4 1/4   OK 124.5 176.9 1/2   Sunny 87.4 251.0 1/4   Expectation 170.2 173.0     According to this criterion, Ella will choose to sell ice creams instead.\nSt Petersburg #  Let us now investigate the case of the St Peterburg paradox, using a log utility function, that is, $$u(x)= \\log x.$$ It follows that the expected utility is $$E\\left[\\log \\left(2^N\\right)\\right] = \\sum_{n=1}^\\infty \\log \\left( 2^n\\right)\\cdot 2^{-n},$$ or (much more simply) $$E\\left[\\log \\left(2^N\\right)\\right] = E[N \\log 2] = 2 \\log 2.$$ Note:\n these are not dollars, but ‚Äúutility‚Äù! the expectation is no longer infinite  Utility functions #  General properties #  The attitudes of the DM towards risk will be reflected in the mathematical properties of \\(u(x)\\). We have:\n Non-satiation: assuming that more money is always preferred, we must have $$u'(x)\u0026gt;0.$$ Risk aversion: a risk averse individual will be satisfied by a positive gain \\(c\\) less than it will be dissatisfied by a loss of \\(-c\\). In other words, it will not proceed with a game that pays \\(c\\) or \\(-c\\) with equal probability. Since this game needs to have a negative value, $$ u(x+dx)-u(x) \u0026lt; u(x) - u(x-dx) \\Longrightarrow u(x+dx)-2 u(x) + u(x-dx) \u0026lt; 0$$ which leads to (for small \\(dx\\)) $$u''(x)\u0026lt;0.$$ An graphical intuition of this will be provided a little later.    Risk seeking: conversely, if $$u''(x)\u0026gt;0$$ then our DM will seek more risk than an expectation maximiser Risk neutral: as stated before, if $$u''(x)=0,$$ the DM is indifferent between gains and losses of equal amount with equal probabilities. This special case is equivalent to expectation maximisation.  Some utility functions #   quadratic: $$ u(x) = x -bx^2, \\quad \\infty \u0026lt; x \u0026lt; \\frac{1}{2b}.$$ If \\(b\u0026gt;0\\) the DM is risk averse. log: $$u(x) = \\log x,\\quad x\u0026gt;0.$$ The DM is always risk averse. power: $$u(x) = \\frac{x^\\gamma-1}{\\gamma}, \\quad x\u0026gt;0,\\; \\gamma\u0026lt;1.$$ Here, the DM is always risk averse (a consequence of \\(\\gamma\u0026lt;1\\) due to the non-satiation requirement). This is a special case of so-called ‚Äúhyperbolic absolute risk aversion‚Äù (a.k.a. ‚ÄúHARA‚Äù) functions. exponential: $$u(x) = 1-e^{-x}.$$ The DM is always risk averse.  Utility and risk: explaining insurance #  A graphical explanation #  A simple situation #  Consider the following situation:\n Hugo has initial wealth \\(W_0\\) He is faced with a risk with two potential outcomes:  loss of \\(c\\) with probability \\(p\\) (say \\(p=0.5\\)) no loss   Hugo is risk averse with a concave utility function  We will compare expectation maximisation with utility maximisation graphically.\n Certainty equivalent #   Denote the initial situation as \\(A\\). We have $$ E[u(A)] = p u(W_0-c) + (1-p) u(W_0).$$ Denote now situation \\(B\\), whereby Hugo has wealth \\(I\\) with certainty. We have $$ E[u(B)] = u(I).$$ We call \\(I\\) the ‚Äúcertainty equivalent‚Äù to \\(A\\) if $$A \\sim B \\Longleftrightarrow E[u(A)] = E[u(B)] \\Longleftrightarrow I = u^{-1}(E[u(A)]).$$  Indifference price and risk premium #   Since the DM will be indifferent between \\(A\\) and $ \\(I\\) with certainty, the quantity $$I-W_0 \\equiv P \u0026lt; 0$$ is the maximum amount the DM should be willing to pay in order to remove the risk included in \\(A\\). It is called the Indifference price.\n(Recall that \\(E[u(B)]=E[u(I)]\\equiv E[u(W_0+P)]\\) as per the above.) Because of concavity, $$ P \u0026lt; -p \\cdot c,$$ the expectation of the loss, which creates a potential market for insurance. The quantity $$-p\\cdot c - P \u0026gt; 0$$ is called the Risk premium.  General case #   Assume now that the loss is no longer \\(c\\), but a general random variable $ \\(X \\in \\mathbb{R}\\). The certainty equivalent is $$I = u^{-1} \\left( \\; E\\left[ u(W_0+X)\\right]\\; \\right).$$ The indifference price is $$ P = I - W_0.$$ The risk premium is $$\\text{risk premium} = E[X]-P.$$ This is the maximum amount an insurer would be able to charge for full protection against \\(X\\), if the DM faced with \\(X\\) has utility function \\(u(\\cdot)\\), and if the DM follows the EUT.  Examples #  Umbrellas vs Ice cream #  The certainty equivalent for each option is $$ (\\gamma \\cdot E[u(W_0+X)]+1)^{1/\\gamma},$$ that is, noting that the numbers in the table ore of the type ‚Äù \\(u(W_0+X)\\) ,\n $7,414 for umbrellas, and $7,661 for ice creams.  This means that (according to this utility function)\n Ella would be better off keeping her savings ( \\(W_0\\) ) if they exceed those amounts (if \\(W_0\u0026gt;I\\)); Ella should be prepared to pay $247 more for the ice cream project than for the umbrella one.  St Petersburg paradox #  The certainty equivalent of $ \\(X\\) here is $$ \\exp{ E[u(X)]} = \\exp{ 2 \\log 2} = 4.$$\nThis means that an individual with log utility and no money at all (here \\(W_0=0\\) would be indifferent between receiving $4 with certainty, and playing the game.\nThe result would be different in general depending on the initial amount of wealth of the individual.\nCharacterising risk aversion #  Introduction #  Umbrellas vs Ice creams: What if Ella‚Äôs wealth changes? #  Assume now that Ella‚Äôs savings were $100,000, plus the cost of either projects. The table of utilities becomes\n(computed using $100,000 plus profit for each case)\n   Weather Umbrellas Ice creams Probability     Bad 719.1 636.7 1/4   OK 643.0 655.3 1/2   Sunny 636.7 679.2 1/4   Expectation 660.5 656.7      We are back to preferring Umbrellas! That is because Ella has sufficient wealth to accept the much higher level of risk of Umbrellas. This extra risk no longer trumps the\nextra level of expected profit.  Umbrellas vs Ice creams: What if \\(\\gamma\\) changes? #  Setting up functions in R:\npower \u0026lt;- function(x, gamma) { (x^gamma - 1)/gamma } umbrella \u0026lt;- function(gamma) { expectedutility \u0026lt;- (power(30000, gamma) + 2 * power(4000, gamma) + power(2000, gamma))/4 return((expectedutility * gamma + 1)^(1/gamma)) } icecream \u0026lt;- function(gamma) { expectedutility \u0026lt;- (power(2000, gamma) + 2 * power(8000, gamma) + power(16000, gamma))/4 return((expectedutility * gamma + 1)^(1/gamma)) }  Plot R code (result on the next slide):\npar(cex = 1.4) curve(umbrella(x), from = 0.01, to = 0.99, lwd = 2, col = \u0026#34;blue\u0026#34;, xlab = \u0026#34;gamma\u0026#34;, ylab = \u0026#34;Certainty equivalent\u0026#34;) curve(icecream(x), from = 0.01, to = 0.99, add = TRUE, lwd = 2, col = \u0026#34;red\u0026#34;) abline(v = 0.5849920142672, col = \u0026#34;green\u0026#34;) text(0.6, 6000, paste(signif(0.5849920142672, digits = 3), sep = \u0026#34;\u0026#34;), col = \u0026#34;green\u0026#34;, adj = 0) text(0.6, 7600, \u0026#34;Ice cream\u0026#34;, col = \u0026#34;red\u0026#34;, adj = 0) text(0.78, 9000, \u0026#34;Umbrellas\u0026#34;, col = \u0026#34;blue\u0026#34;, adj = 1) [Note R codes are not assessable!]\n Discussion #  Observations:\n We have seen above that results in the Umbrella vs Ice cream example depend on the level of wealth, and also on the parameter of the utility function. More generally, results will depend on the shape of the utility function, and the region around which decisions are made (level of wealth \\(W_0\\)). When referring to ‚Äúresults‚Äù here, we refer to the attitudes of the decision maker towards risk, or ‚Äúrisk aversion‚Äù.  Is there a way of characterising risk aversion, that takes into account the utility assumptions and wealth, which we can use for comparisons?\n(akin to mean and variance for location and spread of a distribution, for instance)\nAbsolute Risk Aversion #  Definition #   Risk aversion means that we have a concave utility function. Concavity is equivalent to the fact that the DM has decreasing marginal utility from more wealth as they become richer: \\(u'\\), which is positive, becomes less and less positive as wealth increases: \\(u''\u0026lt;0\\). The more concave, the higher the risk premium (look back at the graph for the intuition). So a good first approximation for an (absolute) risk aversion ratio would be to look at \\(u''\\), standardised with \\(u'\\), say with a negative sign to make it positive for convenience. It turns out to be exactly that! The Absolute Risk Aversion (‚ÄúARA‚Äù) of wealth \\(W\\) is defined as $$ A(W) = - \\frac{u''(W)}{u'(W)}.$$ This is further ‚Äúbrought to life‚Äù in the next section   Examples:\n log utility \\(u(w)=\\log w\\): we have $$u'(w) = \\frac{1}{w} \\text{ and } u''(w) = -\\frac{1}{w^2}$$ and hence $$A(w) = \\frac{1}{w},$$ which is directly inversely proportional to wealth. exponential utility \\(u(w)=1-e^{-aw}\\) ( \\(a\u0026gt;0\\) ): we have $$u'(w)=a e^{-aw} \\text{ and }u''(w)=-a^2 e^{-aW}$$ so that $$A(w) = a,$$ which is constant, and provides a nice interpretation for\nparameter \\(a\\) in the case of exponential utility.  Interpretation: risk premia via Taylor‚Äôs expansion #   A DM with utility function \\(u\\) has initial wealth \\(W_0\\) and faces risk \\(X\\). We have, by Taylor‚Äôs expansion (of \\(u\\) around \\(W_0\\)), that $$u(W_0+X) \\approx u(W_0) + u'(W_0) X + \\frac{1}{2}u''(W_0) X^2.$$ Assume (w.l.o.g.) that \\(E[X]=0\\) and hence that \\(Var(X)=E[X^2]\\equiv \\sigma^2\\), which implies that (plugging those above) $$E[u(W_0+X)] \\approx u(W_0) + \\frac{1}{2} u''(W_0)\\sigma^2.$$ Separately, also by Taylor‚Äôs expansions, $$u(I)=u(W_0+P)\\approx u(W_0) + u'(W_0) P.$$    By definition, $$u(I) = E[u(W_0+X)],$$ and hence $$\\frac{1}{2} u''(W_0)\\sigma^2 \\approx u'(W_0) P \\Longleftrightarrow P \\approx \\frac{1}{2}\\frac{u''(W_0)}{u'(W_0)} \\sigma^2 = - A(W_0) \\sigma^2.$$ This shows that the indifference price is (approximately) proportional to the absolute risk aversion, which makes a lot of sense. Here the risk premium is \\(-P\\) (remember \\(E[X]0\\) ), which means that it is simply \\(A(W_0)\\) scaled up by \\(\\sigma^2\\).  Effect of wealth on ARA #   The formula for the ARA \\(A(w)\\) is a function of \\(w\\) This means that the ARA varies for different of wealth, which makes sense: a millionaire is more likely gamble $100 than a poor student with no money. In general, one expects the ARA to be decreasing with wealth, that is, $$A'(w) \u0026lt; 0.$$ If, on the other hand \\(A'(w)=0\\), this means that the risk premium is constant for any levels of wealth. Examples:  log utility: \\(A'(w) = - w^{-2} \u0026lt; 0\\). exponential utility: \\(A'(w) = 0\\). Here the risk premium is insensitive to wealth.    Relative Risk Aversion #  Think in relative terms #   Think now in relative terms. Rather than ‚Äúadding‚Äù random \\(X\\) to \\(W_0\\), we multiply \\(W_0\\) by a random factor \\(Z\\) with \\(E[Z]=1\\) and \\(Var(Z)=E[Z^2]-1\\equiv \\sigma^2\\). So our wealth after risk has been applied is \\(W_0\\cdot Z\\). We must now define the certainty equivalent as $$ u(I) = E[u(W_0\\cdot Z)].$$ The indifference price should also be defined in relative terms, that is, $$I-W_0 \\equiv - \\pi_R W_0\\text{ so that } \\pi_R=\\frac{W_0-I}{W_0}.$$  Help via Taylor‚Äôs expansion #   Let‚Äôs expand \\(u(W_0 Z)\\) around \\(W_0\\). Noting that $$W_0 Z-W_0= W_0 (Z-1),$$ which as mean 0 and variance \\(W_0^2 \\sigma^2\\), we get $$u(W_0 Z) \\approx u(W_0)+u'(W_0) (W_0 Z-W_0) + u''(W_0)\\frac{(W_0 Z-W_0)^2}{2},$$ and hence that $$E[u(W_0 Z)] \\approx u(W_0) + \\frac{1}{2} \\sigma^2 u''(W_0) W_0^2.$$    Similarly, noting that $$I = W_0 - \\pi_R W_0,$$ we get (via Taylor) that $$u(I) \\approx u(W_0) - \\pi_R u'(W_0) W_0.$$ Combining those two results as before yields $$\\pi_R = -\\frac{\\sigma^2}{2}W_0 \\frac{u''(W_0)}{u'(W_0)}.$$  This result is analogous to the one we had for the ARA, but in relative terms. This leads to the definition in the next slide.\nDefinition #  The Relative Risk Aversion (‚ÄúRRA‚Äù) for a DM with utility function \\(u\\) is defined as $$R(w) = -w\\frac{u''(w)}{u'(w)}.$$\nExamples:\n log utility: $$R(w) = wA(w) = 1.$$ In this case, the log utility is the one with constant RRA. The DM only cares about the proportion of their wealth they will lose, irrespective of their wealth level. exponential utility: $$R(w) = wA(w) = aw,$$ which increases with wealth. In this case, the DM realises that a constant proportion of a larger amount of money is more money,\nand so their wealth will impact their RRA.  Additional considerations #  How do you come up with a utility function? #  There are essentially two different types of use:\n ‚ÄúAssume‚Äù a utility function, which we believe is appropriate to the context, to draw insights from a stylised theoretical model.  For instance, one might ask what the optimal way of investing money in retirement is (a hot topic!).   Gather data (choices) about an individual, and try to ‚Äúfit‚Äù an appropriate utility function to those choices.  The result could be informative in itself (such as in supporting assumptions such as those made in the previous case), and/or could be used to support decision making.    When one function is not enough #   There is no guarantee that a single utility function will fit well:  for all wealth levels: we may need to paste different functions for different areas; in all circumstances: the risk aversion of the DM may be affected by some ‚Äústate‚Äù (e.g., being married vs widowed, or healthy vs sick, for a retiree).   Hence, there may be circumstances where state-dependent, and/or pasted utility functions may be warranted.  Limitations to the EUT #   The EUT was gigantic step forward towards being able to describe decision making of humans. It suffers from a number of flaws and limitations, though. Here are a few:  It assumes we can determine someone‚Äôs utility function, which is a strong assumption. It requires being able to describe risk with certainty. In reality there is uncertainty about outcomes, which complicates things. Whether a company has a utility function is debatable (and debated!). At best, it makes decision on the basis of a combination of different utility functions (the decision makers). In the worst case, it is simply impossible to describe decision making with a utility function. One can also argue that risk aversion is a trait of human nature, which is not relevant to a company.    Alternatives to describing behaviour #  There are many refinements and alternatives to the EUT, to support/describe decision-making. The ones mentioned in the syllabus (and taught in ACTL30006) are\n mean-variance portfolio theory (see Joshi (2013)) ‚Äî this is very similar to using a quadratic utility function; behavioural finance (Section 3 of IoA 2023).  This is a whole (fascinating) field in itself, but we need to stop here for now!\nReferences #  Selected references:\nIoA. 2023. Course Notes and Core Reading for Subject CM2 Models. The Institute of Actuaries.\n Joshi, Mark Suresh. 2013. Introduction to Mathematical Portfolio Theory. Cambridge University Press.\n  "},{"id":9,"href":"/docs/2-reserving/m3-reserving-claim-counts/","title":"M3 Reserving Claim Counts","section":"Reserving (CM2 5.2)","content":"Exposure (2.1) #  General idea #  Assume that we there exists \\(e(i)\\) so that we can write\n$$E[N(i,j)] \\equiv e(i) \\mu(j),$$ where (this is important), \\(\\mu(j)\\) does not depend on \\(i\\).\n \\(e(i)\\) will be called the exposure in period of occurrence \\(i\\) (for instance, ‚Äúcar-years‚Äù). \\(\\mu(j)\\) may be interpreted as relative claim frequency in period \\(j\\) (per unit of exposure). The cumulative version $$\\frac{E[N(i,\\cdot)]}{e(i)} = \\mu(\\cdot)$$ is the relative frequency of claim occurence per period.   In practice, this is not always achievable, that is, $$\\frac{E[N(i,j)]}{e(i)} = \\mu(i,j),$$ with only weak dependency of \\(\\mu(i,j)\\) on \\(i\\).\nIBNR claims - Exposure based methods (2.2.2) #  Allowing for an \\(i\\) effect in \\(\\mu\\) #  Assume $$ \\mu(i,j) = f(i) v(j),$$ where \\(f(i)\\) is some known function (otherwise determined).\nWe have then $$E[N(i,j)] = e(i)f(i)v(j).$$ We will discuss how to work with this.\nExamples of \\(f(i)\\) #   A simple example could be: $$ f(i) = \\alpha + \\beta i,$$ which leads to a linear adjustment across rows (periods of origin \\(i\\)) to ‚Äúbase frequency‚Äù of development \\(v(j)\\) (for given development period \\(j\\)). It is unlikely to hold across all columns without modification, so one could extend this to $$ \\mu(i,j) = f_j(i) v(j)$$ so that \\(\\alpha\\) and \\(\\beta\\) will (potentially) depend on \\(j\\) as well This could lead to a highly over-parametrised model.    In practice, changes in ‚Äúdevelopment speed‚Äù often occur in the first two development periods mostly, and in opposite direction (justifying a separate \\(\\alpha\\) and \\(\\beta\\)). We could then use \\begin{eqnarray*} \\mu(i,0) \u0026amp;=\u0026amp; f_0(i) = \\alpha_0 + \\beta_0 i, \\\\ \\mu(i,1) \u0026amp;=\u0026amp; f_1(i) = \\alpha_1 + \\beta_1 i, \\\\ \\mu(i,j) \u0026amp;=\u0026amp; v(j),\\; j=2, 3, \\ldots. \\end{eqnarray*}  Estimating \\(N(i,j)\\) #  Now, assume $$N(i,j)\\sim \\text{Poisson}(e(i)f(i)v(j))$$ so that (assuming independence across periods of origin) $$N(\\cdot,j)\\sim \\text{Poisson}\\left(v(j) \\sum_i e(i)f(i)\\right).$$ Then we can show that $$\\hat{v}(j) = \\frac{N(\\cdot,j)}{\\sum_i e(i)f(i)}$$ (over existing data) is a maximum likelihood, minimum variance, unbiased, consistent estimator (in short, a good one!).\n In the end, our estimator for \\(E[N(i,j)]\\) is $$\\widehat{E[N(i,j)]} = e(i)f(i)\\hat{v}(j)$$ with IBNR $$\\text{IBNR}(i) = e(i)f(i)\\left[ \\sum_{j=I-i+1}^I + \\sum_{j=I+1}^\\infty \\right] \\hat{v}(j).$$\n the first \\(\\sum\\) include \\(\\hat{v}\\)‚Äôs estimated from available data the second \\(\\sum\\) cannot be estimated from data, and will be extrapolated from the former set (e.g.¬†linear regression or more typically, log-linear regression as exemplified later).  Example #  Taylor (2000), Table 2.1 and Table 2.2 provide an example of such calculations, where \\(f(i)=1\\).\nSee the spreadsheet here.\n Notes on the extrapolation:\n We fit a linear regression to \\(\\ln \\hat{v}(j)\\) for \\(j=4,\\ldots,10\\) and tell Excel that \\(x \\equiv j-5\\). Using SLOPE and INTERCEPT functions we get \\(\\beta_1\\) and \\(\\beta_0\\), so that $$ \\tilde{v}(j) = \\exp\\left[\\beta_0+\\beta_1 (j-5)\\right] = e^{\\beta_0}\\times\\left(e^{\\beta_1}\\right)^{j-5} = 3.87\\times \\left(0.54\\right)^{j-5} \\quad \\text{(2.14)}.$$ Note you can also get \\(\\beta+\\beta_1 (j-5)\\) with the Excel function FORECAST. Now, for the tail estimate (‚Äú11 and later‚Äù): $$\\sum_{j=11}^\\infty \\tilde{v}(j) = e^{\\beta_0}\\left(e^{\\beta_1}\\right)^{6} \\sum_{j=0}^\\infty \\left(e^{\\beta_1}\\right)^{j} = e^{\\beta_0}\\left(e^{\\beta_1}\\right)^{6}\\frac{1}{1-e^{\\beta_1}} \\quad \\text{(2.15).}$$  IBNR claims - Normalised methods (2.2.3) \\(\\maltese\\) #  Motivation #   In the previous section, one hoped that claim notification rates \\(\\mu\\) (as proportions of exposure \\(e\\) ) would be consistent (constant) across periods of origin \\(i\\), or at least approximatively or predictively so. There may not always exist such an exposure. For instance, consider Public Liability of a manufacturer of toys:  Would time a good measure of exposure? or revenue? This would unlikely to be satisfactory as the mix of business (which toys are sold and in what quantities) is likely to change all the time.    The idea #   We keep the idea of multiplicative structure, but \\(\\mu\\) would multiply something else than \\(e(i)\\), say \\(\\alpha(i)\\): $$ E[N(i,j)] = \\alpha(i)\\mu(j).$$ Then we automatically have $$\\frac{E[N(i,j)]}{E[N(i,0)]}= \\frac{\\mu(j)}{\\mu(0)}$$ which is independent of \\(i\\). Now Jensen‚Äôs inequality teaches us that $$\\frac{E[N(i,j)]}{E[N(i,0)]} \\neq E\\left[\\frac{N(i,j)}{N(i,0)}\\right];$$ Nevertheless Taylor (2000) argues that the difference is small and\nthat we can use the ratios \\(N(i,j)/N(i,0)\\) to estimate the \\(\\mu\\)‚Äôs.     Taking a weighted average, we get $$\\hat{v}(j) = \\frac{\\sum_i N(i,j)}{\\sum_i N(i,0)}$$ as an (approximately) unbiased estimator of \\(\\mu(j)/\\mu(0)\\).\n  Hence, $$E[N(i,j)] \\text{ is estimated by }N(i,0)\\hat{v}(j)$$ and $$ \\text{IBNR}(i) = N(i,0) \\left[ \\sum_{j=I-i+1}^I + \\sum_{j=I+1}^\\infty \\right] \\hat{v}(j).$$ This is analogous to the previous ‚Äúexposure‚Äù based formula.\n  Note that even if we don‚Äôt have \\(E[N(i,j)] = \\alpha(i)\\mu(j)\\) but rather \\(E[N(i,j)] = \\alpha_j(i)\\mu(j)\\), the triangle of normalised counts \\(N(i,j)/N(i,0)\\) are useful to investigate, to look for trends.\n  Generalisation #   We will now ‚Äúanchor‚Äù our prediction not just on \\(N(i,0)\\), but on on a subset \\(S\\) of existing data. This generalises what we had before to $$\\frac{E[N(i,j)]}{E\\left[\\sum_{m \\in S} N(i,m)\\right]} = \\frac{\\mu(j)}{\\sum_{m \\in S} \\mu(m)}.$$ where \\(S\\) is any subset of \\(\\{0,1,\\ldots,I\\}\\), but typically the first \\(m\\) development periods for each period of origin \\(i\\). It is assumed that this ratio is independent of \\(i\\). This should work well if \\(S\\) is deemed to be a good indicator of the propensity to claim in a given period \\(i\\).    The prediction of \\(N(i,j)\\) becomes $$\\left(\\sum_{m \\in S} N(i,m)\\right) \\hat{v}(j)$$ with \\(\\hat{v}(j)\\) the estimate of the the ratio on the previous slide. Note that the most recent origin years may require special treatment (see example), as we do not have observations for all \\(m\\in S\\).  Example #   Table 2.3 and 2.4 revisit the previous example with \\(S=\\{0,1\\}\\). This can be useful if some disturbance to the claim notification process has caused a movement between development periods 0 and 1 but has had no other effect. See the spreadsheet here.   Notes on treatment of the last row:\n The last row in the triangle requires special treament, as only \\(N(I,0)\\) is available. We extrapolate $$N(I,0)+N(I,1) \\text{ as }N(I,0) \\frac{\\hat{v}(0)+\\hat{v}(1)}{\\hat{v}(0)}.$$ Now, because \\(\\hat{v}(0)+\\hat{v}(1) = 1000\\) (by definition since we take them from the same type of average - ‚Äúorange‚Äù here) we end up with $$ N(I,j)\\text{ is estimated by }\\left(N(I,0) \\frac{\\hat{v}(0)+\\hat{v}(1)}{\\hat{v}(0)}\\right)\\frac{\\hat{v}(j)}{1000} = N(I,0)\\frac{\\hat{v}(j)}{\\hat{v}(0)}.$$  IBNR claims - Chain Ladder (2.2.4) #  Introduction #   Probably the most famous (and basic) loss reserving technique. Quoting Taylor (2000): ‚ÄúThe name is understood to refer to the chaining of a sequence of ratios (the age to age factors below) into a ladder of factors (the age to ultimate factors below) which enable one to climb (i.e.¬†project) from experience recorded to date to its predicted ultimate value.‚Äù We will see three derivations, which are informative about the meaning of the method.  Derivation 1 - Heuristic approach #   We start from $$ E[N(i,j)] = \\alpha(i)\\mu(j)$$ as before, but consider $$\\frac{E[A(i,j+1)]}{E[A(i,j)]} = \\frac{\\sum_{m=0}^{j+1} \\mu(m)}{\\sum_{m=0}^j \\mu(m)} \\equiv v(j)$$ instead, where $$ A(i,j) = \\sum_{m=0}^j N(i,m)$$ are the **cumulative** claim counts for period of origin \\(i\\), as of development period \\(j\\).    This leads to an estimator $$\\hat{v}(j) = \\frac{\\sum_i A(i,j+1)}{\\sum_i A(i,j)},$$ where the summations run over those values of \\(i\\) where we have both \\(A\\)‚Äôs available. These are called chain ladder factors, age to age factors, or link ratios.    Future values of \\(A(i,j)\\) are predicted by $$\\hat{A}(i,j) = A(i,I-i) \\hat{v}(I-i)\\hat{v}(I-i+1)\\cdots \\hat{v}(j-1),$$ where \\(A(i,I-i)\\) is the latest observation for period \\(i\\) (on the diagonal). In particular, ultimate counts are $$\\hat{A}(i,\\infty) = A(i,I-i) \\hat{\\pi}(I-i),$$ where $$\\hat{\\pi}(I-i) = \\prod_{m=0}^\\infty \\hat{v}(I-i+m), $$ called **age to ultimate factors**.  Example #   Table 2.5, 2.6 and 2.7 revisit the previous example using the chain ladder example. See the spreadsheet here. Notes on the extrapolation: assuming that it is the excess over 1 that decays exponentially, we fit \\(\\ln\\left[\\hat{v}(j-6)-1\\right]\\) for \\(j=6,\\ldots,9\\). The extrapolation to infinity is then added to 1 to get an age to ultimate factor. This is an ad hoc approach, but it‚Äôs the one‚Äôs taken in the book.  Derivation 2 - Poisson approach #   Assume that $$ N(i,j) \\sim \\text{Poisson}(\\alpha(i) \\mu(j)),$$ and that they are mutually independent. Because $$\\alpha(i)\\mu(j) = \\left[k \\cdot \\alpha(i)\\right] \\left[ \\frac{1}{k} \\cdot \\mu(j)\\right],$$ \\(\\alpha\\) and \\(\\mu\\) are not uniquely defined unless we introduce an extra condition, which we conveniently choose to be $$\\sum_{j=0}^J \\mu(j) = 1,$$ ($J$ being the largest value of \\(j\\) for which any \\(N(i,j)\\) is observed) so that \\(\\mu(j)\\) can be interpreted as the proportion of claims being notified in development year \\(j\\). It can be shown (see Taylor (2000)) that the estimators seen\nbefore are the MLE estimators in this context.   Because $$\\frac{1}{\\pi(j)} = \\frac{A(i,j)}{A(i,\\infty)}$$ is the proportion of claims notified by period \\(j\\), $$ \\frac{1}{\\pi(j)}- \\frac{1}{\\pi(j-1)} \\equiv \\mu(j)$$ is the proportion of claims notified in development period \\(j\\) (assume \\(\\pi(-1)=1\\) ).\n Example: The age to ultimate factors and corresponding proportions \\(\\mu(j)\\) are calculated under Table 2.5.  Derivation 3 - Non-parametric approach #   The Poisson derivation is a parametric one, and starts with rather strong assumptions. What assumptions do we need to make at a minimum, in order to retrieve the chain ladder estimators according to some rigorous calculations? (in particular, do we need the Poisson assumption?) It turns out that if we assume $$ E[N(i,j+1)|A(i,j)] = x(j) A(i,j)$$ for some \\(x(j)\\), and that $$ Var(N(i,j+1)|A(i,j)) = \\sigma^2(j)A(i,j),$$ for some constant \\(\\sigma^2(j)\u0026gt;0\\), then minimising weighted least squares between this model and the data yields \\(x(j)\\) such that we get the chain ladder estimators again. This does not require any distributional assumption!  The Chain Ladder setting #   Chain ladder won‚Äôt always work. In particular, it requires a fixed notification pattern - the \\(\\mu(j)\\) can‚Äôt vary across origin periods. One can think of many reasons why notifications may come quicker (e.g.¬†IT systems) or slower (COVID-19), either permanently or temporarily (same examples, respectively). As a solution, one can either ‚Äútweak‚Äù the chain ladder procedure, or proceed with an entirely different approach, which allows for such changes. This is (mostly) out of scope of this introductory treatment of reserving techniques.  Claim frequency (2.3) #  Claim frequency #   If an exposure exists, it is possible to estimate a claim frequency, that is, the number of claims per unit of exposure. This is mathematically defined as $$f(i) = \\frac{A(i,\\infty)}{e(i)}$$ for period of origin \\(i\\), and is estimated by $$\\hat{f}(i) = \\frac{\\hat{A}(i,\\infty)}{e(i)}.$$ Examination of such claim frequencies may help with model validation, and may help identify anomalies.  Notified claim frequency #   This is defined as $$\\frac{A(i,I-i)}{e(i)}$$ for period of origin \\(i\\). This corresponds to the frequency of claims that have already been notified (obviously, a decreasing number in \\(i\\)). Comparing this with \\(\\hat{f}(i)\\) gives an idea of the proportion of ultimate claims that have already been notified.  Example #   Table 2.8 (see spreadsheet here for calculations) displays estimated claim frequencies from the three methods presented in the previous section.  1991 and 1992 seem to reverse a strong downward trend from 1984 to 1990. is this due to a method issue (introduced when ‚Äúcompleting the rectangle‚Äù), or due to a real effect?   Table 2.9 (see spreadsheet here for calculations) and associated Figure 2.2 compares the notified claims frequency with the estimated claims frequencies of Table 2.8.  It appears that the reversal of trend is already present in the actual data, and is not a result of the modelling. This may reassure the actuary that the reversal is real. She would need to gather more data from the claims department to ascertain whether that is a reasonable feature of the model.    References #  Main reference:\nTaylor, Greg. 2000. Loss Reserving: An Actuarial Perspective. Huebner International Series on Risk, Insurance and Economic Security. Kluwer Academic Publishers.\n  "},{"id":10,"href":"/docs/2-reserving/m4-reserving-claim-amounts/","title":"M4 Reserving Claim Amounts","section":"Reserving (CM2 5.2)","content":"Case estimation (3.1) #  Case estimates: Definition #   When a claim is notified, the insurer‚Äôs employee who manages the claim (the ‚Äúclaims adjuster‚Äù) typically formulates an estimate of the (remaining) cost of the claim and records it in the system. This estimate is typically adjusted over time as payments are made and additional information becomes available. These is called a ‚Äúcase estimate‚Äù (equivalently, ‚Äúindividual estimate‚Äù, ‚Äúmanual estimate‚Äù, ‚Äúphysical estimate‚Äù). This is contrast to an ‚Äúaggregate estimate‚Äù, which would be inferred from past data, and some sort of statistical (reserving) procedure.  Two approaches #  You can think of the evolution of a claim costs as two parallel paths; consider those two examples (Figure 2 of (AvTaWa23?)):\n  Contrary to the evolution of aggregate payments (which lead to the ‚Äúaggregate estimate‚Äù), ‚Äúincurred‚Äù estimates are meant to be centered around the expected ultimate cost from the start. Remember they are defined as $$I(i,j) = D(i,j) + Q(i,j),$$ where \\(D(i,j)\\) are cumulative payments up to time \\(j\\), and \\(Q(i,j)\\) is the case estimate at that same time. The ‚Äúgood‚Äù thing about case estimates is that they are specific to the claim, and are an educated, intelligent guess of the cost of them, rather than a cold, myopic statistical estimate. The ‚Äúbad‚Äù thing is that they are very subjective, and are subject to (potentially dangerous) systematic biases.  So which one should you use?\nProposition 3.1 #  Quote from Taylor (2000):\n When outstanding loss liability is to be estimated in respect of a large number of claims, an aggregate estimate will usually exhibit performance, as measured by the relative error, superior to that of case estimates. When the liability is to be estimated in respect of a small number of claims, superior performance will usually be exhibited by case estimates corrected for bias.‚Äù  This is discussed/applied in Section 4.4 of Taylor (2000) (outside scope).\nChain ladder (3.2) #  Introduction #   We used chain ladder for claim counts in Module 3. We can use it for forecasting other variables \\(Y(i,\\cdot)\\) which can be expressed in the cross-classified structure (i.e.¬†\\(i\\times j\\) ) in much the same way, from a triangle of aggregate quantities $$Y(i,j) = \\sum_{m=0}^j X(i,m),$$ where \\(X(i,j)\\) are observed (incremental) quantities in cell \\((i,j)\\). Assumptions of proportionality can be made in much the same way, and the method applies in much the same way. Note that you can apply this on either paid or incurred losses (that is, \\(X\\) would be either paid losses or incurred losses), as discussed in the following two subsections. Applications are quite wide and apply in a range of contexts and\nfields!  Chain ladder on paid losses #  Unadjusted chain ladder #   We focus on paid losses, that is, $$ X(i,j) = C(i,j).$$ Hence \\(\\hat{Y}(i,j)\\) will yield estimates of future loss payments in future cells \\((i,j)\\). Everything else is the same as before.  Example #   See the spreadsheet Chapter3.xlsx for details of the calculations. The first tab sets out payment data corresponding to the example we studied in Module 3. Table 3.1-3.3 demonstrate how chain ladder can be used on payment data, in absence of inflation adjusment.  Inflation adjusted chain ladder #   We are now dealing with payment data, which is typically distorted by inflation. This cannot be ignored. Not adjusting the data corresponds to making a rather strong assumption about inflation, as explained below. Remember that the asterisk \\(*\\) denoted claims data brought back/forward to some reference date, and that the quantities without \\(*\\) were nominal amounts.    Assuming the multiplicative structure of the chain ladder model, if $$ E\\left[ C^(i,j) \\right] = \\alpha^(i) \\mu^(j)$$ then we have the ‚Äúinflation adjusted‚Äù chain ladder model $$ E\\left[ C(i,j) \\right] = \\alpha^(i) \\mu^*(j) \\frac{\\lambda(k)}{\\lambda_0},$$ which is to be compared with the ‚Äúunadjusted‚Äù chain ladder model $$ E\\left[ C(i,j) \\right] = \\alpha(i) \\mu(j).$$ Note that Tables 3.1-3.3 corresponded to the ‚Äúunadjusted‚Äù version.   Proposition of Taylor (2000):\nIf \\(\\alpha(\\cdot)\\), \\(\\mu(\\cdot)\\) are not restricted, then the inflation adjusted and unadjusted chain ladder models are consistent if an only if the rate of claims inflation is constant over the whole experience.\nThe last sentence means that we require $$\\frac{\\lambda(k)}{\\lambda(k-1)}\\text{ to be constant for all }k\u0026gt;1.$$\n Theorem 3.3 of Taylor (2000) implies that when claims inflation is constant at rate \\(f\\), $$\\begin{aligned} \\alpha(i) \u0026amp;= \\frac{\\lambda(0)}{K\\lambda_0} \\alpha^*(i) (1+f)^i, \\\\ \\mu(j) \u0026amp;= K\\mu^*(j) (1+f)^j, \\text{ with} \\end{aligned}$$ Note\n One can choose \\(K=1\\) or such that \\(\\sum_j \\mu^*(j) = 1.\\) (this will generally not be the case). In the example below we have \\(K=1\\). If all payments are brought to the date of the diagonal, then the adjustment \\(\\lambda(0)/\\lambda_0\\) would typically just be an adjustment due to the fact that payments are not all made on the first or last day of the periods (typically, we assume in the middle on average).   Proposition 3.4 of Taylor (2000) states that ‚ÄúSimilar estimates of outstanding loss liability will be produced by:\n the unadjusted chain ladder; the inflation adjusted chain ladder, with a future inflation rate roughly equal to the average over the period of claims experience on which the estimates are based.‚Äù  Note:\n This is because the unadjusted chain ladder age to age factors include inflation implicitly, and that these are hence similarly projected in the future. If inflation was very heterogeneous in the past \\(I\\) periods, and/or if it is assumed to be inconsistent with future inflation, an inflation adjusted chain ladder will lead to better forecasts (although it will require assumptions to be made about future inflation). See also Corollary 3.5 in Taylor (2000). Results hold by analogy in presence of superimposed inflation.  Example #   See the spreadsheet Chapter3.xlsx for details of the calculations. Table 3.4 displays \\(C^*(i,j)\\) as of 31 December 1995 using the inflation index set of Appendix B.2.  Note the accumulation factor of 1995 is not 1; it likely reflects the fact that payments in cells \\(k=1995\\) were spread throughout 1995, so some adjustment is required to bring them to 31 December.   Table 3.5 determines age to age factors, which we analyse from the point a view of the discussion above before proceeding to projections.    Table 3.6 compares \\(\\widehat{\\mu}^*(j)\\) of Table 3.5 with \\(\\widehat{\\mu}(j)\\) of Table 3.1. It is a consequence of Theorem 3.3 that $$f = \\left(\\frac{\\mu(j)}{\\mu^*(j)}\\right)^{1/j}-1$$ and hence we can derive an ‚Äúimplied‚Äù rate \\(f_j\\) for each period \\(j\\): $$f_j = \\left(\\frac{\\widehat{\\mu}(j)}{\\widehat{\\mu}^*(j)}\\right)^{1/j}-1$$ This is also in Table 3.6, and is informative of the evolution of claims inflation over 1978-1995. The weighted average uses \\(\\widehat{\\mu}^*(j)\\) as weights. It is now clear how inflation distorted the evolution of claim payments, making them look longer tailed than they would otherwise have been.    It is important to note that the results of Table 3.6 were influenced by our choice of age to age factors:  the ‚ÄúAll‚Äù averaged factors over years 1978-1995 the ‚ÄúLast 6‚Äù over years 1989-1995 the ‚ÄúLast 3‚Äù over 1992-1995   The average inflation rate over those years in Appendix B.2 was 6.8%, 4%, and 3.3% respectively (see calculations in spreadsheet). This illustrates Theorem 3.3.    Now, Tables 3.7 and 3.8 forecast paid losses in 31/12/1995 dollar values  Note the forecast of $374.8mio is evidently lower than the $428.4mio forecast of the unadjusted chain ladder, as the latter implicitly allowed for inflation, and the former not.   Table 3.9 lifts those forecasts to allow for future inflation  Note the half year adjustment \\(\\lambda(0)/\\lambda_0\\) required for payment in the middle of the year on average. Using 3.6% for future inflation leads to a forecast of $421.1mio, which is very close to the unadjusted chain ladder forecast. This illustrates Proposition 3.4.    Chain ladder on incurred losses #  Introduction #   Here we will produce a triangle of incurred losses, as an indicator of how our estimates typically evolve over time, and converge to ultimate:  the ‚Äúaggregate‚Äù equivalent will be the incurred losses the ‚Äúincremental‚Äù equivalent are adjustments to incurred losses   Note that this means that age-to-age factors of incurred losses generally won‚Äôt be \\(\u0026gt;1\\) since the incurred loss process is not a mostly nondecreasing process any more; this is illustrated in the two examples of claim developments earlier..\n(Aggregate payments were generally nondecreasing, even though there can be negative payments sometimes.)  Example #   See the spreadsheet Chapter3.xlsx for details of the calculations. Table 3.10 displays incurred losses \\(I(i,j)\\) (the ‚Äúcumulative‚Äù data), which is decomposed in the next table (not in book) into \\(\\Delta I(i,j)\\) (the ‚Äúincremental‚Äù data) - the heat map on that triangle illustrates the generally increasing, then decreasing nature of aggregate incurred losses. Table 3.11 age to age factors on the cumulative data. Note that the factors are no longer all \\(\u0026gt;1\\), which requires a new smoothing method, namely here a 3-period moving average; see (3.61) in Taylor (2000). Finally, Table 3.12 implements the chosen age to age factors into outstanding liabilities.  Commentary #   The difference between \\(I(i,\\infty)\\) and \\(I(i,j)\\) can be decomposed into:  an IBNR component (due to unreported claims); an IBNER component (‚ÄúIncurred But Not Enough Reported‚Äù), due to future revisions (inaccuracy) of case estimates.   Tables 3.3 and 3.12 are very different (see comparison under Table 3.12)  Paid CL is much higher than Incurred CL for 1986 and later. This is due to using last-3-year averages, where payments were heavier than before. The leverage on immature years is illustrated with the age-to-ultimate factors in gray in tab ‚ÄúTable 3.1‚Äù. Using all-year averages would reduce liability by 34-39% for the three most immature years. A natural question would then be: are those changes permanent?    Separation method (3.3) \\(\\maltese\\) #  Main rationale #   A choice of inflation rate can be controversial, especially if it is significant or erratic. Unfortunately, this is also when it is the most impactful‚Ä¶ In that context, the unadjusted chain ladder is unreliable, and the adjusted one will lead to conflict. Can we ‚Äúlet the data speak for themselves‚Äù?  A simple answer is the separation method.\n Here we keep the multiplicative structure we are familiar with. However, the genious idea is to use indices \\(i\\) and \\(k\\) instead of \\(i\\) and \\(j\\). As a result, the \\(k\\) factors will give a sense of calendar period effects (to which inflation belongs), and we will retain nice ease of interpretation of the parameters.  Model #  Algebraic structure #   We assume $$E[C(i,j)] = N(i) \\mu^*(j) \\lambda(k),$$ but now \\(\\lambda(k)\\) will be inferred from the triangle. Hence dividing \\(C(i,j)\\) by \\(N(i)\\) will yield the following algebraic structure of the separation model (Figure 3.1 in Taylor (2000)):  Interpretation #   If we assume (wlog) $$ \\sum_{j=0}^\\infty \\mu^*(j) = 1,$$ then $$\\sum_{j=0}^\\infty E\\left[ \\frac{C(i,j)}{N(i)}\\right] = \\sum_{j=0}^\\infty \\mu^*(j) \\lambda(k) = \\lambda(k).$$ This is the average size of claims from period of origin \\(i\\)  Proposition 3.6 (Taylor (2000)): In the separation model as above, \\(\\lambda(k)\\) denotes the average size of claims which would occur if costs were the same in all experience periods as in period \\(k\\).\nEstimation #  Taylor (2000) derives estimators: $$\\begin{aligned} \\hat{\\mu}^*(j) \u0026amp;= \\frac{\\sum_{i=0}^{I-j} C(i,j)/\\hat{N}(i)}{\\sum_{k=j}^I \\hat{\\lambda}(k)} \\\\ \u0026amp;= \\frac{\\text{row sum of payments per claim}}{\\text{diagonal sum of }\\lambda\\text{'s from }j\\text{ to }I} \\\\ \\hat{\\lambda}^*(k) \u0026amp;= \\frac{\\sum_{i=0}^{k} C(i,k-i)/\\hat{N}(i)}{1-\\sum_{j=k+1}^\\infty \\hat{\\mu}^*(j)} \\\\ \u0026amp;= \\frac{\\text{diagonal sum of payments per claim for calendar period } k}{\\text{proportion of claims paid after }k\\text{ development periods}} \\end{aligned}$$\n This cannot be implemented directly Furthermore, we do not have the \\(\\hat{\\mu}^*(j)\\) beyond \\(I\\).   Define:\n$$v^*(j) = \\frac{\\mu^*(j)}{\\sum_{m=0}^I \\mu^*(m)}, \\quad \\kappa(k) = \\lambda(k)\\sum_{m=0}^I \\mu^*(m).$$\nNote that \\(v^*(j)\\) still is a proportion of a ‚Äúperiod of origin‚Äù paid losses, but the base runs only up to development period \\(I\\).\n The following clever manipulation, which rewrites the model as a function of the (column) \\(v^*(j)\\) and (diagonal) \\(\\kappa(k)\\) allows us to use the triangle easily for estimation. We have $$v^*(j)\\kappa(k) = \\mu^*(j)\\lambda(k) = E\\left[\\frac{C(i,j)}{N(i)}\\right] \\text{ with } \\sum_{j=0}^I v^*(j)=1.$$ Substitution in the estimators written above yields $$\\hat{v}^*(j) = \\frac{\\displaystyle \\sum_{i=0}^{I-j}\\frac{C(i,j)}{\\hat{N}(i)} }{ \\sum_{k=j}^I\\hat{\\kappa}(k) } = \\frac{\\text{sum of triangle column }j}{\\sum_{k=j}^I\\hat{\\kappa}(k)}$$ and $$ \\hat{\\kappa}(k) = \\frac{\\displaystyle \\sum_{i=0}^{k}\\frac{C(i,j)}{\\hat{N}(i)} }{ 1-\\sum_{j=k+1}^I\\hat{v}^*(j) } = \\frac{\\text{sum of triangle diagonal }k}{1-\\sum_{j=k+1}^I\\hat{v}^*(j)}$$which can be calculated sequentially$$\\hat{\\kappa}(I),\\hat{v}^*(I),\\hat{\\kappa}(I-1),\\ldots,\\hat{v}^*(0).$$`\nExample #   See the spreadsheet Chapter3.xlsx for details of the calculations. Table 3.13 starts by computing the \\(C(i,j)/N(i)\\), where \\(N(i)\\) comes from Table 2.4. It also displays diagonal and column sums. Those sums are used in Table 3.14 to calculate the \\(\\hat{\\kappa}(k)\\) and \\(v^*(j)\\) in the sequential way described above.    The estimates above can be translated into corresponding \\(\\lambda(k)\\)‚Äôs in that the \\(\\kappa\\) are proportional (see (3.73)). These were the ‚Äúinflation‚Äù equivalent we were trying to get from the model (‚Äúlet the data speak‚Äù); see Figure 3.2 of Taylor (2000):   This suggests negative claims inflation up to 1988 (-4.9%), and strongly positive afterwards (12.1%). The ensuing \\(v^*(j)\\) are very different from that stemming from\nthe inflation adjusted chain ladder; see Table 3.15.    Table 3.16 calculates model paid losses for past and future. The \\(\\hat{\\kappa}(k)\\) for \\(k\u0026gt;I\\) need to be projected. We use $$ \\hat{\\kappa}(k) = \\hat{\\kappa}(1995) \\cdot (1.075)^{k-1995}.$$ Population of the table needs care because the \\(\\hat{\\kappa}(k)\\) factors (including with projection) apply diagonally. Comparison of model and actual paid losses to date is interesting:  By construction, column and diagonal sums should be identical, or very close. The row sums, however, have no guarantee of matching. These are the paid losses to date, per period of origin. Examination of actual vs model such aggregate losses is a good indication of model fit. We can also do this cell by cell with a heat map (done below, not in the book). It turns out that those sums are matching relatively well, with a slight tendency to underestimate in early years, and to overestimate in later years.    Commentary #   The interpretation of \\(\\lambda(k)\\) requires care, as it is a ‚Äúcatch all‚Äù indicator. For instance, a change of speed of payment / notification across calendar years would be caught (would distort) by \\(\\lambda(k)\\).  The average payments per claim incurred (PPCI) method #  Main rationale #   Assume you have data on aggregate payments and claim counts. One can then try to project counts, and payments per claim separately. The product can then be used as an estimate for ultimate. One need to be careful to have consistent claims severity and frequency data, typically:  If paid (fully) claims, match to finalised claims. If claims incurred, match to reported claims.   The former leads to the PPCF, the latter to the PPCI. We focus on the latter. One major advantage is in the flexibility offered by the method to include superimposed inflation (this is illustrated in Taylor (2000), but is outside scope here).  Process #   Project incurred claim counts as per Chapter 2 (frequency). Work out average payments per claim (severity) incurred in a triangle:  These will evolve over time. We can analyse this evolution in a ‚Äúchain ladder‚Äù type way.   The method rests on the assumption that this evolution is sufficiently regular that we can use it to ‚Äúdevelop‚Äù our latest value of incurred claim cost (in the diagonal) into a final ‚Äúpayments per claim incurred‚Äù, to be multiplied by our projected total number of claim incurred.  Example #   See the spreadsheet PPCI.xlsx for details of the calculations, which are based on the data of Taylor (2000), but do not follow the (more complicated) calculations of Chapter 4. Start from Table 3.10 of incurred losses, and incurred claims counts from Table 2.2. We need to transform the latter into cumulative form, so we can have both of same nature (cumulative). The result of the division is an average payment per claim incurred:  Analysis of age to age actors shows that recent years are different, especially for later years. The age-to-ultimate factors are those used to ‚Äúdevelop‚Äù the latest average PPCI, into an ultimate one.   In the end, forecast outstanding liabilities are not dissimilar to those obtained form the incurred CL, although a little higher, perhaps due to slightly more responsiveness to the recent changes in claim development.   A comparison and recap of all methods seen in this module is provided at the bottom.\nWhich one would you choose?\nReferences #  Taylor, Greg. 2000. Loss Reserving: An Actuarial Perspective. Huebner International Series on Risk, Insurance and Economic Security. Kluwer Academic Publishers.\n  "},{"id":11,"href":"/docs/2-reserving/m5-reserving-combination/","title":"M5 Reserving Combination","section":"Reserving (CM2 5.2)","content":"Background #  Summary #  We have seen covered a number of reserving methods:\n unadjusted chain ladder on paid losses inflation adjusted chain ladder on paid losses unadjusted chain ladder on incurred losses inflation adjusted chain ladder on incurred losses separation payments per claim incurred (PPCI)  The IBNR claims (counts) were estimated according to a number of methods, too:\n exposure normaliser chain ladder  Motivation #   All those methods are producing outstanding liability $ estimates in nominal terms. There are a number of differences:  whether inflation is accounted for explicitly (2, 4, 5, 6) or not (1, 3) whether inflation is assumed for past data (2, 4), or a result of the method (1, 3, 5, 6) whether paid (1, 2, 6) or incurred (3, 4, 5) losses are used whether aggregate amounts (1, 2, 3, 4) or amounts per claim (5, 6) are used. The latter require an estimate of IBNR counts.   All methods have strengths and weaknesses.  There is obviously a good mix of assumptions and approaches. Which one should we choose? Or should we combine them?\nCriteria for choice #  Decision factors include:\n analytical properties of the various models  e.g., we know there has been a change in the past, that affected the development of claims. Can the method allow for that?   average claim sizes for various periods of origin  e.g., do those average claims make sense, given our (also qualitative) knowledge of the claims development processes?? e.g., do trends in average claim size agree with our beliefs around claims inflation and superimposed inflation?   relation of forecasts of liability to case estimates  e.g., they are not directly comparable, but the evolution of the ratio should be smooth enough.    Combining the results of the different models #  Idea #   Let \\(\\hat{P}_h^*(i,j)\\) be the estimate of outstanding liability of model \\(h=1,2\\ldots\\), at end of development period \\(j\\) and for period of origin \\(i\\). Assume the we want to combine such estimates (at the end of experience period \\(k\\) ) as follows: $$\\overline{\\hat{P}^*}(i,k) = \\sum_h w_h(i) \\overline{\\hat{P}^*_h}(i,k),$$ where \\(\\overline{\\hat{P}^*_h}(i,k) = \\hat{P}_h^*(i,k-i)\\), \\(w_h(i)\\) are weights allocated to to model \\(h\\), $$\\sum_h w_h(i)=1.$$ Note that in general those weights depend on \\(i\\) as well; different reserving models will typically perform better for different levels of maturity (in complex environments - understand ‚Äúnon chain ladder like‚Äù).   Those weights can be determined in different ways:\n Judgmentally, by considering the properties of the models available, and their respective strenghts and weaknesses for different \\(i\\). With respect to some sort of objective criteria.  This is done to some extent in the book (Chapter 12). This has been done a lot more rigorously only very recently by Avanzi et al. (2023) via ensembling, which was awarded the 2023 Hachemeister Prize by the American Casualty Actuarial Society (CAS). It is still a relevant topic!    Allowance for prior expectations #  Idea #   Imagine one might to combine a ‚Äúprior expectation‚Äù (or belief) with the estimate of liability provided by a method (or ensemble thereof). This can be done in a way which is routinely referred to as ‚Äúcredibility weighting‚Äù by actuaries: $$ \\text{estimate} = [1-z(i)] \\overline{P_0^}(i,k) + z(i) \\overline{\\hat{P}^}(i,k),$$ where  \\(\\overline{\\hat{P}^*}(i,k)\\) is the quantity defined earlier \\(\\overline{P_0^*}(i,k)\\) is the prior expectation (examples later), and \\(z(i)\\) is the credibility assigned to the model estimate \\(\\overline{\\hat{P}^*}(i,k)\\)   One probably should give more credibility to models in more mature years (small \\(i\\) ). The formula above will yield equivalent results if \\(z(i)\\) is applied on incurred amounts instead (see Taylor (2000)).  Choice of credibility weights #  Bornhuetter-Ferguson #  Bornhuetter and Ferguson (1972) suggested the most simple approach, which is to use either\n \\(z(i)=0\\): outstanding liability is exclusively calculated on the basis of prior expectations; or \\(z(i)=1\\): outstanding liability is entirely based on models, ignoring prior expectations totally.  The ‚Äútextbook‚Äù, ‚Äúplain‚Äù vanilla Bornhuetter-Ferguson (BF):\n applies \\(z(i)\\) on all or only some subset of the most immature years calculates the prior expectation based on premium and loss ratios  An example is provided below.\nMore generally #  The following would make sense:\n \\(z(i)=0\\) when no information has been collected (start of development period 0); \\(z(i)=1\\) at the end of the running off period (when all claims and their costs are known and certain); some monotonic progression between those two extremes.  For instance, \\(1/\\pi\\) from chain ladder:\n It satisfies the criteria above: It is somewhat reflective of the amount of information gathered so far In particular, a highly leveraged line, which would benefit from averaging with prior expectation, will have a very low \\(z(I) = 1/\\pi(0)\\).  Prior expectations #  In this section we review some possible choices for the ‚Äúprior expectations‚Äù.\nBF loss ratio #  The loss method proceeds as follows. For each \\(i\\)\n Define \\(EP(i)\\) as the gross aggregate premium earnt for period of origin \\(i\\). Define \\(C(i)\\) as the aggregate sum of all payments made for period of origin \\(i\\) (the ultimate). The loss ratio is then defined as $$ LR(i) = \\frac{C(i)}{EP(i)}.$$  The method projects \\(LR(i)\\) from past values and infers \\(C(i)\\) from observable \\(EP(i)\\). We have then (assuming chain ladder development patterns) $$\\overline{P_0^*}(i,k) = LR(i)EP(i)\\left( 1-\\frac{1}{\\pi(k)}\\right)$$ which is typically applied with \\(z(i)=1\\) in immature (or all) years.\nExample #  Consider the following triangle (cumulative claims):\n   Origin EP DY1 DY2 DY3 DY4     2020 860 473 620 690 715   2021 940 512 660 750    2022 980 611 700     2023 1,020 647       It is assumed that the ultimate loss ratios for underwriting years 2021-2023 are expected to be in line with year 2020.\n First calculate the development factors: $$\\begin{aligned} \\widehat{f}_1 \u0026amp;= \\frac{620+660+700}{473+512+611} = 1.2406 \\\\ \\widehat{f}_2 \u0026amp;= \\frac{690+750}{620+660} = 1.125 \\\\ \\widehat{f}_3 \u0026amp;= \\frac{715}{690} = 1.0362 \\end{aligned}$$ Then calculate the loss ratio \\(LR\\) and the prior expectations of ultimate: $$\\begin{aligned} LR(1)EP(1) \u0026amp;= LR \\cdot 860 = 715 \\Longrightarrow LR = 0.8314 \\\\ LR(2)EP(2) \u0026amp;= LR \\cdot 940 = 781.52\\\\ LR(3)EP(3) \u0026amp;= LR \\cdot 980 = 814.77\\\\ LR(4)EP(4) \u0026amp;= LR \\cdot 1020 = 848.02 \\end{aligned}$$\n Now, we have\n$$\\begin{aligned} \\text{outstanding} \u0026amp;= \\sum_{i=1}^4 \\overline{P_0^*}(i,4) \\\\ \u0026amp;= \\sum_{i=1}^4 LR(i)EP(i)\\left(1-\\frac{1}{\\pi(i)}\\right) \\\\ \u0026amp;= 0+27.30+115.82+261.64 \\\\ \u0026amp;=404.76. \\end{aligned}$$\nRemember that \\(\\pi\\) is normally defined as a \\(\\pi(j)\\), so that \\(\\pi(1)\\) applies to that year where we have only \\(j=1\\) cell available - the last row. Similarly, \\(\\pi(3)\\) is the one that applies to the amount in the diagonal that is at \\(j=3\\) - here 781.41.\nExtensions #   The problem with the plain vanilla BF is that it does not specify objectively how \\(LR(i)\\) is determined. A priori, this is too judgmental. There are a number of methods which try to make this choice more objective, such as ‚Äúmodified BF‚Äù, ‚ÄúCape Cod‚Äù, ‚Ä¶  References #  Avanzi, Benjamin, Yanfeng Li, Bernard Wong, and Alan Xian. 2023. ‚ÄúEnsemble Distributional Forecasting for Insurance Loss Reserving.‚Äù\n Bornhuetter, R., and R. Ferguson. 1972. ‚ÄúThe Actuary and IBNR.‚Äù Proceedings of The Casualty Actuarial Society 59: 181‚Äì95.\n Taylor, Greg. 2000. Loss Reserving: An Actuarial Perspective. Huebner International Series on Risk, Insurance and Economic Security. Kluwer Academic Publishers.\n  "},{"id":12,"href":"/docs/3-excel/m6-excel/","title":"M6 Excel Topics","section":"Excel (CM2 Exam B)","content":"Introduction and Assumed Knowledge #  Why Excel? #   Excel is great to spread data and calculations in a tabular form, and have a visual overview. Most financial modelling is done in Excel (at least initially). In actuarial work, many more advanced codes (in R, Python, C++, C#, VBA, ‚Ä¶) often starts with someone playing around in Excel, and once proof of concept is approved, this moves to proper coding. It is assessed in CM2-B (!).  Issues with Excel #  Excel is notoriously problematic in certain areas:\n Lack of transparency - one can‚Äôt see the formulas unless you click in a cell; alternatively, you can‚Äôt see the formulas and understand where the numbers come from unless you are in Excel (a problem for reports, presentations etc). Mistakes can be tiny but have huge consequences in the end (the job of Excel auditor actually exists!). Lack of good documentation capability (as opposed to code); this makes collaboration and audit difficult, and creates an operational risk (e.g.¬†builder leaves).    Lack of rigour in the construction of a model (input, assumptions, intermediary calculations, output). Can‚Äôt handle (seriously) large data sets. Lack of good and flexible data cleaning and manipulation capabilities. Sometimes code is a lot easier (e.g.¬†flip a vector around, sum over a diagonal, ‚Ä¶).  I know there are counter arguments for all of those, but this presupposes you know what the solutions are. You‚Äôll learn some of those here!\nAssumed knowledge #  See prerequisite knowledge on the website. Some extracts:\n Autofill: Chapter 3, p.¬†105-116, and p.¬†298-301 Named ranges and constants: Chapter 7, page 312-332 Absolute and mixed cell references ($): Chapter 7, pages 332-342 New Excel 2019 functions (IFS, MAXIFS, MINIFS): Chapter 8, pages 381-398 Formula Auditing: Chapter 9, p.¬†436-439 Paste special (incl, e.g.¬†Transpose): Chapter 11, pages 518-530  Page references are for Slager and Slager (2020), see link here.\nAlso, see tab Assumed Knowledge in the module 6 spreadsheet.\nSome keyboard shortcuts #  There are many keyboard shortcuts that can help you work with Excel efficiently. Some (but not all) useful shortcuts are:\n F4: Cycles through combinations of absolute and relative references ctrl + shift + arrow key: Extend the selection of cells to the last non-blank cell towards the specified direction ctrl + PageDown/PageUp: Move to the next/previous sheet shift + F11: Insert new worksheet  See Microsoft - Keyboard shortcuts in Excel for list of all keyboard shortcuts.\nData Wrangling and Exploratory Data Analysis #  Data Wrangling #   Sometimes we need to work on data from an external source. The data could be messy to work with.  There could be non-printable characters such as tabs (\\t), new lines (\\n) in the original data.  Check List of ASCII values for ASCII codes for non-printable characters.   We may want to process the strings to retrieve information. Data formats are inconsistent.   It is important to clean our data before analysis!  Some data cleaning steps #   Editing Texts  CLEAN(), TRIM(), and SUBSTITUTE() to deal with any non-printable characters. Merging and splitting columns with functions like LEFT(), RIGHT() and MID() and SEARCH(). Wildcard characters ? and * are also useful.   Removing duplicate rows: Data ‚Üí Data Tools ‚Üí Remove Duplicates. Converting table into ‚Äúmachine-readable‚Äù formats.  An example in Kolokolov, 2023 (Chapter 1, P.2-9). Macros would be useful here!   There are much more functions and steps can be used to clean your data - see Microsoft - Top ten ways to clean your data.  Pivot tables #   Pivot tables are often considered as very difficult to master, but they are not that difficult to start with. Example (in module 6 spreadsheet): FIFA WWC  Insert / Pivot Table. See how you can use variables as filters, rows, or columns. Move them around. See how columns can display other things than Sum, such as Count, Average, Max, Min, or Product   Note there are recommended Pivot Tables (automated recommendation within Excel); in the case of FIFA WWC it is not very helpful. Reference: Chapter 15 of Slager and Slager (2020).  Pivot charts #   A pivot chart can be created from the Pivot Table but also directly from the data. A major difference with start charts is that you it will be somewhat ‚Äúinteractive‚Äù - there will be buttons you can use to alter the chart. Example (in module 6 spreadsheet): FIFA WWC  Insert / Pivot Chart; In the example I changed the style of graph to ‚ÄúCombo‚Äù to allow for the two different scales; I also included a ‚Äúslicer‚Äù (click on chart / insert slicer), in order to easily filter by squad.   Reference: Chapter 15 of Slager and Slager (2020).  Dynamic Arrays #  Spilling #   One advantage of programs like R are the easy use and manipulation of vectors. Excel can do similar things, and the vectors are called arrays. This is new (post Office 365), and is a bit of a game changer. Before Office 365, Excel was incapable of depositing results beyond just 1 cell. This is called ‚Äúspilling‚Äù. Note Excell will need the required space to spill. Main reference is Katz (2023) - we‚Äôll only introduce this here.  Example #   Here we introduce array formulas.  If you calculate the sum of an array you‚Äôll get a single number. The result of an array formula (such as LEN()), when you input an array, will give you an array.   See module 6 spreadsheet:  LEN() gives an array. You could then get the sum without having to put the array anywhere: SUM(LEN(B3:B6)). Note that when I wrote the above, it became SUM(LEN(B3#)) automatically - more on that later.   SUM(LEN(B3:B6)) will work in any version of Excel because it requires only one cell to output, but not LEN(B3:B6) as it requires several cells (‚Äúspilling‚Äù). Note you can spill named ranges, too!  Think in vectors #   Once you understand you can create vectors and either display or manipulate them, Excel becomes a lot more powerful. You can also use arrays in arguments of known formulas such as VLOOKUP(),  For instance VLOOKUP(.,.,{2,5}) will return value from the 2nd and 5th columns row-wise. If you use VLOOKUP(.,.,{2;5}) (with the semicolon) they will display columnwise. See examples in module 6 spreadsheet.    The # sign #   The # sign when added to a reference to a cell where a dynamic array is written will duplicate that array (and spilled results). It is shorter, but also it will dynamically change the size of the array  This can be desired or not. See example in module 6 spreadsheet.    New formulas #   There are a number of new formulas which were available in coding languages like R for a long time, which can be useful, and which are now available in Excel  for instance SEQUENCE(), UNIQUE(), FILTER(), RANDARRAY()‚Ä¶   Some of those are exemplifed in module 6 spreadsheet. You are encouraged to browse through. They really bring data handling in Excel a little closer to coded languages such as R.  Lookup functions #   Lookup functions are essential for Excel users. Useful when you need to search within a single row or column to find a corresponding value in the same position in a second row or column. Multiple lookup functions are available. All of them works with arrays of lookup values!  VLOOKUP() #   Available for all versions of Excel, so it is reliable when sharing spreadsheets. Some drawbacks of VLOOKUP():  Lookup Column Must Be the First Column Can be confusing when specifying the return column   HLOOKUP() is the horizontal version of VLOOKUP(). Check this video to see how it works.  XLOOKUP() #   An upgraded version for VLOOKUP() released in 2020 - not available in Excel 2016 and Excel 2019.  Robust lookup function: You can search any direction. Can return multiple arrays as well. The lookup array does not have to be the first column/row. Different search modes available: from first to last, last to first, etc. An if not found argument to allow combinations with functions like IFERROR, IFNA.   Watch this video to see how it works.  MATCH(), XMATCH() #   Both functions work similarly by finding the index of your lookup value within the lookup array. XMATCH() is more robust by providing new match mode and search mode. By combining with INDEX(), it returns the value instead of the index inside the lookup array.  Check module 6 spreadsheet for comparisons between these lookup functions.\nReference: Murray, 2022 (Chapter 7, 11)\nDynamic references #   Sometimes we have to work with references that changes position or size over time. For instance, we need to adjust our range of references for each development period when modelling the development effects. This is tedious to do manually if we have a large dataset! Dynamic Referencing will be useful here.  OFFSET() #   Allows you to move your initial references to any direction by any number of cells. You can select the height and width of the returned range.  INDIRECT() #    Goes to the address specified by the reference text you entered.\n e.g.¬†INDIRECT(B2) goes to the cell A2 if cell B2 contains the text A2.    Your reference text can be a table, a named range, etc.\n  Combining with ADDRESS() allows you to reference dynamically.\n  Both OFFSET() and INDIRECT() are volatile functions.\n Excel recalculates them even if you changed a cell without these functions! Can be computationally inefficient if the spreadsheet relies on them heavily.    ADDRESS() #   Gives you the address of your specified columns and rows. This allows you to obtain dynamic addresses by dynamically changing your specified rows and columns.  See reference tab in module 6 spreadsheet for demonstrations.\nReference: Murray, 2022 (Chapter 11)\nGeneral etiquette, auditing, and tools #  Etiquette #  You should build your spreadsheet with (at least) the following objectives in mind:\n so as to minimise chances or error (accuracy); so as to minimise unnecessary calculations (efficiency); so as to make the structure as clear as possible (transparency); so as to make updates, changes and extensions possible and easy (extendibility); so as to allow someone else to use it easily (user friendliness); so as to allow someone else to verify it easily (auditability).  Those are, of course, interconnected. We could add more (for instance, automation of data input via an API, automation of communication objects such as charts, etc‚Ä¶).\nPrinciples #  There is no single way to achieve the objectives above, but there are a number of principles that one could list, and that will contribute to meeting those objectives:\n Have a separate tab that collects all your assumptions that are valid for the whole spreadsheet (1, 3, 4, 5, 6).  Include some explanations about the source/justification of those assumptions. Give your assumptions names (for instance, the technical rate of interest to calculate life insurance could be called techint or similar, for ease of later reference, and to make formulas more easily readable).   Also include your data sets in separate tabs (1, 3, 4, 5, 6).  Name your data (including columns and/or rows if possible; e.g.¬†FIFA WWC in the spreadsheet; see also ‚ÄúNamed ranges and constants‚Äù: Chapter 7, page 312-332).      Consider colouring / contouring input and output differently (3, 5)  This is not always advisable, but if you have a large model with relatively few input (e.g.¬†purchase price and interest rate for a property mortgage schedule) and/or few outputs (e.g.¬†NPV) then this achieves many of the objectives.   Use named ranges and variables as much as possible (1, 3, 5, 6), unless the variable is going to be used only once. Use more advanced formulas if shorter, and avoid too much nesting (1, 3, 4, 5, 6).  Tools for auditing #  Dependents and precendents #  Use of dependents / precedents, e.g.:\n go to tab Dependents - Precedents - PPCI; click on one of the outstanding loss projected amounts; make sure the formula tool tab is live; click on the ‚Äútrace precedents‚Äù sequentially.  This will highlight dependence of each cell to previous cells, sequentially, with arrows.\nThis is useful for\n understanding the structure of a spreadsheet; check formulas (audit); debug issues.  (Formula Auditing: Chapter 9, p.¬†436-439)\nShow formulas #   The ‚ÄúFormulas‚Äù tool tab should have a ‚ÄúShow formula‚Äù tile. This will replace all numeric values by formulas.  This is helpful to check what numbers are hard coded, and which are results of calculations. Together with dependents, it helps seeing if everything is as dynamic as it should.   If you want a formula to be shown all the time start with an apostrophe ', and the formula will show as text.  (Formula Auditing: Chapter 9, p.¬†436-439)\nSimulation, Macros and VBA #  Simulation #  Why simulation? #   Suppose we have some model with given inputs. Inputs can be uncertain.  Weather, arrival speed of claims, interest rates,‚Ä¶   We want to derive the distributions of our output.  We want to know the statistical properties (e.g.¬†mean, SD) of our output. Sometimes it can be complicated to derive the distribution of the output explicitly.   By simulating the uncertain inputs, we can generate random observations from our model.  These generations will result in an empirical distribution of the output. We can then use this distribution to estimate moments, quantiles, ranges, and probabilities.   Simulation is widely used in the work by actuaries.  Generating random numbers 1 #   RAND() generates random numbers from a uniform distribution on (0,1). RANDARRAY() can generate an array of random numbers with specified dimension.  You can specify the range of numbers. You can also specify whether the generations are discrete or continuous.   What if we want to generate numbers that are not uniformly distributed?  Generating random numbers 2 #   Inverse functions can be useful to generate numbers under different distributions.  Use NORM.S.INV(RAND()) to generate numbers with standard normal distribution. In general, to generate random numbers with specified distributions, we can apply their inverse functions to RAND().   Some inverse functions available in Excel:  BETA.INV() for Beta distribution LOGNORM.INV() for lognormal distribution   To see more inverse/probability functions Excel, go to Formulas ‚Üí More Functions ‚Üí Statistical.  Discrete random variables #   We can also simulate outcomes coming from discrete distributions. Suppose we have a random variable with three outcomes: \\(A\\) with \\(p=0.5\\), \\(B\\) with \\(p=0.3\\), and \\(C\\) with \\(p=0.2\\). First, generate a random number \\(x\\) with RAND().  If \\(x\\leq 0.5\\), the outcome is \\(A\\). If \\(0.5 \u0026lt; x \\leq (0.5+0.3)=0.8\\), the outcome is \\(B\\) Otherwise, the outcome is \\(C\\).    Macros and VBA #  What is a Macro? #  What is VBA? #  Next steps #   All of Katz (2023) is relevant, but take it as a cook book for the assignment. You can go as far as you wish. Chapters 16, 17, 18, and 19 of Slager and Slager (2020) are out of scope. However, macros and VBA (which is Chapter 19) are essential components of Excel  I strongly encourage you to get started. Start by recording a macro, then play around with the code. VBA allows more efficient calculations via compiled code, and is a powerful addition to Excel.   Fun fact: the 2021 Excel World Champion is an actuary: Andrew Ngai, now Director at Taylor Fry.  References #  Katz, A. I. 2023. Up up and Array! Dynamic Array Formulas for Excel 365 and Beyond. Apress.\n Slager, D., and A. Slager. 2020. Essential Excel 2019. 2nd ed. Apress.\n  "},{"id":13,"href":"/docs/0-study-plan/","title":"Weekly Study Plan","section":"Docs","content":"Weekly plans\n\nThis section will provide weekly guidance about the subject activities, and links to relevant sections of the website, and documents (such as slides, for instance).\nTo seek help:\n ask your question in Ed, or ask your tutor at your next tutorial, or ask the course Professor at the next consultation hours, or ask your friends in the course (on Ed or elsewhere!),  BUT do not e-mail tutors or the course Professor, unless your query is truly personal in nature, or has absolutely no chance of being of any relevance to any other student. Chances are other students have the same question, and so having the question being asked and answered publicly helps everyone.\n"},{"id":14,"href":"/docs/0-prerequisite-knowledge/","title":"Prerequisite Knowledge","section":"Docs","content":"Summary of some of the prerequisite knowledge required for this subject: Excel #  To view the eligibility and requirements, including prerequisites, corequisites, recommended background knowledge and core participation requirements for this subject, please see the University Handbook: https://handbook.unimelb.edu.au/2024/subjects/actl20004/eligibility-and-requirements\nExcel assumed knowledge Because everyone has used Excel before, a challenge in this subject is to start at the right level of assumed knowledge. We had to strike a balance between (i) not assuming so much that it is too much work to catch up if you have little experience, and (ii) not assuming enough that no-one draws benefits from the subject, and is bored to death. Below, you will find reference to those topics that we will assume are prior knowledge. Numbers correspond to Chapters of Slager and Slager (2020) (see ‚ÄúSubject Resources‚Äù below).\nIt is strongly recommended that you review those topics, including, but not limited to, those ‚Äúsemi-beginner‚Äù topics that are singled out with bullet points.\nCh. 1: Becoming Acquainted with Excel\nCh. 2: Navigating and Working with Worksheets\nCh. 3: Best Ways to Enter and Edit Data\n Autofill: P. 105-116, and p.¬†298-301  Ch. 4: Formatting and Aligning Data\n Format Painter: 178-181  Ch. 5 Different Ways of Viewing and Printing Your Workbook\nCh. 6 Understanding Backstage\n Freezing rows and columns: 221  Ch. 7 Creating and Using Formulas\n  Named ranges and constants: 312-332\n  Absolute and mixed cell references ($): 332-342\n  Ch. 8: Excel‚Äôs Pre-existing Functions\n All important New Excel 2019 functions (IFS, MAXIFS, MINIFS): p.¬†381-398  Ch. 9: Auditing, Validating, and Protecting Your Data\n Error Value Messages: p.¬†432 Formula Auditing: p.¬†436-439  Ch. 10: Using Hyperlinks, Combining Text, and Working with the Status Bar\n Concatenation: p.¬†475-485  Ch. 11: Transferring and Duplicating Data to Other Locations\n Paste special (including with ‚ÄúTranspose‚Äù): p.¬†518-530  Ch. 12: Working with Tables\n Conditional formatting: p.569-581  Ch. 13: Working with Charts\n Sparklines: p.¬†656-664  Ch. 14: Importing Data\n_\nReference:\nSlager, D., and A. Slager. 2020. Essential Excel 2019. 2nd ed. Apress.\n  "},{"id":15,"href":"/docs/1-utility/","title":"Utility theory (CM2 1.2)","section":"Docs","content":"Risk and Insurance\n\nThis section covers contents in the Section 1.2 in the CM2 syllabus ; see also the learning outcomes mappings in the Subject Guide.\nThe main references for Module 1 is: [Joshi] Joshi, M., Paterson, J. M. (2013), Introduction to Mathematical Portfolio Theory, Cambridge University Press, ISBN 9781107042315  Chapter 2 of following reference is also relevant (as additional reading): [Dickson] Dickson, D. C. M. (2016) Insurance Risk and Ruin, 2nd Edition, Cambridge University Press, https://doi.org/10.1017/9781316650776  "},{"id":16,"href":"/docs/2-reserving/","title":"Reserving (CM2 5.2)","section":"Docs","content":"Reserving\n\nThis section covers contents in the Section 5.2 in the CM2 syllabus ; see also the learning outcomes mappings in the Subject Guide.\nThe main references for Modules 2-5 is: [Taylor] Taylor, G. (2000) Loss Reserving: An Actuarial Perspective Huebner International Series on Risk, Insurance and Economic Security (HSRI volume 21) https://link.springer.com/book/10.1007/978-1-4615-4583-5  "},{"id":17,"href":"/docs/3-excel/","title":"Excel (CM2 Exam B)","section":"Docs","content":"Excel\n\nThis section covers contents in the CM2 syllabus related to Excel ; see also the learning outcomes mappings in the Subject Guide.\nThe main references for Module 6 is: [EE19] For Excel Slager, D., Slager, A. (2020) Essential Excel 2019 Apress, 2nd Edition, ISBN 978-1-4842-6208-5, https://doi.org/10.1007/978-1-4842-6209-2\n[AE23] For Excel Katz, A. I. (2023) Up Up and Array! Dynamic Array Formulas for Excel 365 and Beyond, Apress, ISBN 978-1-4842-8965-5, https://doi.org/10.1007/978-1-4842-8966-2\n "},{"id":18,"href":"/docs/4-stochastic-interest/","title":"Stochastic interest (CM2 3)","section":"Docs","content":"Stochastic interest\n\nThis section covers contents in the Section 3 in the CM2 syllabus ; see also the learning outcomes mappings in the Subject Guide.\nThe main references for Modules 7-9 is TBC.\n\u0026ndash;\u0026gt;\n-- Slides for week 7 are available here\nSlides for week 8 are available here\n"},{"id":19,"href":"/docs/5-ruin-theory/","title":"Ruin Theory (CM2 5.1 5.3)","section":"Docs","content":"Ruin theory\n\nThis section covers contents in the Section 5.1 and 5.3 in the CM2 syllabus ; see also the learning outcomes mappings in the Subject Guide.\nThe main reference for Modules 10-12 is: [Dickson] Insurance Risk and Ruin, 2nd Edition, Cambridge University Press, https://doi.org/10.1017/9781316650776  "}]