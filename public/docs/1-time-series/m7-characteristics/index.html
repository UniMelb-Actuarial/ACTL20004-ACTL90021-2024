<!DOCTYPE html>
<html lang="en" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="–&gt;
Introduction (TS 1.0) #  Definition #   Consider data of the same nature that have been observed at different points in time The mere fact that they are of the same nature means that they are likely related in one way or another - let’s call those `correlations’
(an acceptable term in this context as we focus on this measure, at least in this course) This is in contrast with the usual ``i.">
<meta name="theme-color" content="#FFFFFF"><meta property="og:title" content="M7 Characteristics of Time Series" />
<meta property="og:description" content="–&gt;
Introduction (TS 1.0) #  Definition #   Consider data of the same nature that have been observed at different points in time The mere fact that they are of the same nature means that they are likely related in one way or another - let’s call those `correlations’
(an acceptable term in this context as we focus on this measure, at least in this course) This is in contrast with the usual ``i." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://gim-am3.netlify.app/docs/1-time-series/m7-characteristics/" /><meta property="article:section" content="docs" />

<meta property="article:modified_time" content="2022-02-18T10:58:38+11:00" />

<title>M7 Characteristics of Time Series | General Insurance Modelling - AM3</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="/book.min.a08e958a0c32c71fa52257f56602fc9e6f4a8586733f7d90b6b544b160364dce.css" integrity="sha256-oI6Vigwyxx&#43;lIlf1ZgL8nm9KhYZzP32QtrVEsWA2Tc4=" crossorigin="anonymous">
  <script defer src="/flexsearch.min.js"></script>
  <script defer src="/en.search.min.878ab88e3d4527cf91ebf122d3c2168ec5f07897eee262b8596684862c6a9b5e.js" integrity="sha256-h4q4jj1FJ8&#43;R6/Ei08IWjsXweJfu4mK4WWaEhixqm14=" crossorigin="anonymous"></script>

  <script defer src="/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js" integrity="sha256-b2&#43;Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC&#43;NdcPIvZhzk=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  <script>
MathJax = {
  tex: {
      macros: { 
          angl: ["\\enclose{actuarial}{#1}", 1],
          alert:["\\textcolor{red}{#1}", 1],
          termins: ["A^{1}_{#1 : \\enclose{actuarial}{#2}}", 2],
          insend: ["\\bar{A}_{#1 : \\enclose{actuarial}{#2}}", 2],
          pureend: ["A^{\\,\\,\\,\\, 1}_{#1 : \\enclose{actuarial}{#2}}", 2],
          adv: ["\\maltese"]
      }
  }
};
</script>
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a href="/"><img src="/img/PRIMARY_A_Vertical_Housed_RGB.png" alt="Logo" /><span style="color: black;">General Insurance Modelling - AM3</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>











  



  
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-0c375332c765cd598c6ce4ee8867890f" class="toggle"  />
    <label for="section-0c375332c765cd598c6ce4ee8867890f" class="flex justify-between">
      <a href="https://gim-am3.netlify.app/docs/0-subject-guide/" class="">Subject Guide</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://gim-am3.netlify.app/docs/0-subject-guide/Eligibility/" class="">Eligibility and Requirements</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://gim-am3.netlify.app/docs/0-subject-guide/SILO/" class="">Learning Outcomes</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://gim-am3.netlify.app/docs/0-subject-guide/Activities/" class="">Activities</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://gim-am3.netlify.app/docs/0-subject-guide/Resources/" class="">Resources</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://gim-am3.netlify.app/docs/0-subject-guide/Assessment/" class="">Assessment</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-5b29660809eb857420b6473840d34e82" class="toggle"  />
    <label for="section-5b29660809eb857420b6473840d34e82" class="flex justify-between">
      <a href="https://gim-am3.netlify.app/docs/0-study-plan/" class="">Weekly Study Plan</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://gim-am3.netlify.app/docs/0-study-plan/week-1/" class="">Week 1 Study Plan</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://gim-am3.netlify.app/docs/0-prerequisite-knowledge/" class="">Prerequisite Knowledge</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://gim-am3.netlify.app/docs/m1-introduction/" class="">M1 Introduction</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://gim-am3.netlify.app/docs/1-claims-modelling/" class="">Claims Modelling (CS2 Section 1)</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="https://gim-am3.netlify.app/docs/1-claims-modelling/m2-collective-risk-modelling/" class="">M2 Collective Risk Modelling</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://gim-am3.netlify.app/docs/1-claims-modelling/m3-individual-claim-size-modelling/" class="">M3 Individual Claim Size Modelling</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://gim-am3.netlify.app/docs/1-claims-modelling/m4-compound-approx/" class="">M4 Approximations for Compound Distributions</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://gim-am3.netlify.app/docs/1-claims-modelling/m5-copulas/" class="">M5 Copulas</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="https://gim-am3.netlify.app/docs/1-claims-modelling/m6-extreme-value-theory/" class="">M6 Extreme Value Theory</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>











  
<ul>
  
  <li>
    <a href="https://canvas.lms.unimelb.edu.au/courses/145406/external_tools/4436?display=borderless" target="_blank" rel="noopener">
        Ed discussion forum
      </a>
  </li>
  
  <li>
    <a href="https://canvas.lms.unimelb.edu.au/courses/123529" target="_blank" rel="noopener">
        ACTL30007 Canvas
      </a>
  </li>
  
  <li>
    <a href="https://canvas.lms.unimelb.edu.au/courses/123552" target="_blank" rel="noopener">
        ACTL90020 Canvas
      </a>
  </li>
  
  <li>
    <a href="https://canvas.lms.unimelb.edu.au/courses/145406" target="_blank" rel="noopener">
        Community Canvas
      </a>
  </li>
  
  <li>
    <a href="https://canvas.lms.unimelb.edu.au/courses/145406/modules/items/3630303" target="_blank" rel="noopener">
        Pre-tutorial exercises
      </a>
  </li>
  
  <li>
    <a href="https://canvas.lms.unimelb.edu.au/courses/145406/modules/items/3630304" target="_blank" rel="noopener">
        Tutorial exercises
      </a>
  </li>
  
  <li>
    <a href="https://canvas.lms.unimelb.edu.au/courses/145406/modules/items/3630305" target="_blank" rel="noopener">
        Additional and Out-of-Scope exercises
      </a>
  </li>
  
  <li>
    <a href="" target="_blank" rel="noopener">
        __________________________
      </a>
  </li>
  
  <li>
    <a href="https://bavanzicv.netlify.app/" target="_blank" rel="noopener">
        Benjamin Avanzi
      </a>
  </li>
  
  <li>
    <a href="https://actl10001.netlify.app/" target="_blank" rel="noopener">
        Introduction to Actuarial Studies
      </a>
  </li>
  
  <li>
    <a href="https://communicate-data-with-r.netlify.app/" target="_blank" rel="noopener">
        Communicate Data with R
      </a>
  </li>
  
  <li>
    <a href="https://actuaries.asn.au/becoming-an-actuary/becoming-a-university-subscriber" target="_blank" rel="noopener">
        Actuaries Institute info
      </a>
  </li>
  
</ul>





<script src="//yihui.org/js/math-code.js"></script>
<script async
src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>
</nav>




  <script>(function(){var a=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>M7 Characteristics of Time Series</strong>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#introduction-ts-10">Introduction (TS 1.0)</a>
      <ul>
        <li><a href="#definition">Definition</a></li>
        <li><a href="#applications">Applications</a></li>
        <li><a href="#process-for-time-series-analysis">Process for time series analysis</a></li>
      </ul>
    </li>
    <li><a href="#examples-ts-11">Examples (TS 1.1)</a>
      <ul>
        <li><a href="#johnson--johnson-quarterly-earnings-per-share">Johnson &amp; Johnson quarterly earnings per share</a></li>
        <li><a href="#global-mean-land-ocean-temperature-index">Global mean land-ocean temperature index</a></li>
        <li><a href="#dow-jones-industrial-average">Dow Jones Industrial Average</a></li>
        <li><a href="#analysis-of-two-series-together-el-niño--fish-population">Analysis of two series together: El Niño &amp; fish population</a></li>
      </ul>
    </li>
    <li><a href="#basic-models-ts-12">Basic models (TS 1.2)</a>
      <ul>
        <li><a href="#preliminaries">Preliminaries</a></li>
        <li><a href="#filtering-and-moving-average">Filtering (and moving average)</a></li>
      </ul>
    </li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
<script src="//yihui.org/js/math-code.js"></script>
<script async
src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>


  </aside>
  
 
      </header>

      
      
  <article class="markdown"><!-- <!-- To include images: -->
<p>–&gt;</p>
<!-- ```{r fig.align='center',out.width="100%", echo=F} -->
<!-- include_graphics(sprintf("%s/XXX.png", graphics_path), error = F)  -->
<!-- ``` -->
<h1 id="introduction-ts-10">
  Introduction (TS 1.0)
  <a class="anchor" href="#introduction-ts-10">#</a>
</h1>
<h2 id="definition">
  Definition
  <a class="anchor" href="#definition">#</a>
</h2>
<ul>
<li>Consider data of the same nature that have been observed at different points in time</li>
<li>The mere fact that they are of the same nature means that they are likely related in one way or another - let’s call those `correlations’<br>
(an acceptable term in this context as we focus on this measure, at least in this course)</li>
<li>This is in contrast with the usual ``i.i.d.’’ assumptions associated with a sample of outcomes of a random variable</li>
<li>This invalidates some of the techniques we know, and brings additional difficulties, but also opportunities! (such as forecasting)</li>
</ul>
<p>Definition: “The systematic approach by which one goes about answering the mathematical and statistical questions posed by these time correlations is commonly referred to as <strong>time series analysis</strong>.”</p>
<h2 id="applications">
  Applications
  <a class="anchor" href="#applications">#</a>
</h2>
<p>The applications of time series are many, and crucial in many cases:</p>
<ul>
<li>Economics: unemployment, GDP, CPI, etc <code>\(\ldots\)</code></li>
<li>Finance: share prices, indices, etc <code>\(\ldots\)</code></li>
<li>Medicine: COVID-19 cases and fatalities, biometric data for a patient (blood pressure, iron levels, <code>\(\dots\)</code> ), etc <code>\(\ldots\)</code></li>
<li>Global warming: ocean temperatures, <code>\(\text{CO}_2\)</code> levels, particule levels in the atmosphere, sea levels, all in relation with another, and with many others</li>
<li><strong>Actuarial studies</strong>: frequency and severity of claims in a LoB, mortality (at different ages, in different locations, <code>\(\ldots\)</code>), superimposed inflation, IBNR claims, etc <code>\(\ldots\)</code></li>
</ul>
<h2 id="process-for-time-series-analysis">
  Process for time series analysis
  <a class="anchor" href="#process-for-time-series-analysis">#</a>
</h2>
<p>Sketch of process:</p>
<ul>
<li>Careful examination of data plotted over time (Module 7)</li>
<li>Compute major statistical indicators (Modules 7 and 8)</li>
<li>Guess an appropriate method for analysing the data (Modules 8 and 9)</li>
<li>Fit and assess your model (Module 9)</li>
<li>Use your model to perform forecasts if relevant (Module 9)</li>
</ul>
<p>We distinguish two types of approaches:</p>
<ul>
<li><em>Time domain approach</em>: investigate lagged relationships (impact of today on tomorrow)</li>
<li><em>Frequency domain approach</em>: investigate cycles (understand regular variations)</li>
</ul>
<p>In actuarial studies, both are relevant.</p>
<h1 id="examples-ts-11">
  Examples (TS 1.1)
  <a class="anchor" href="#examples-ts-11">#</a>
</h1>
<h2 id="johnson--johnson-quarterly-earnings-per-share">
  Johnson &amp; Johnson quarterly earnings per share
  <a class="anchor" href="#johnson--johnson-quarterly-earnings-per-share">#</a>
</h2>
<p><embed src="/img/week-X/TS1-JJ.pdf" width="100%" style="display: block; margin: auto;" type="application/pdf" /></p>
<p>What is the primary pattern?
Can you see any cyclical pattern as well?
How does volatility change over time (if at all)?</p>
<h2 id="global-mean-land-ocean-temperature-index">
  Global mean land-ocean temperature index
  <a class="anchor" href="#global-mean-land-ocean-temperature-index">#</a>
</h2>
<p><embed src="/img/week-X/TS1-GW.pdf" width="100%" style="display: block; margin: auto;" type="application/pdf" /></p>
<p>Can you see a trend? Are there periods of continuous increase?
What would be the main focus for global warming: trend or cycles?
How does this graph support the global warming thesis?</p>
<h2 id="dow-jones-industrial-average">
  Dow Jones Industrial Average
  <a class="anchor" href="#dow-jones-industrial-average">#</a>
</h2>
<p><embed src="/img/week-X/TS1-DJ.pdf" width="100%" style="display: block; margin: auto;" type="application/pdf" /></p>
<p>How is this time series special?
What qualities would a good forecast model need to have?</p>
<h2 id="analysis-of-two-series-together-el-niño--fish-population">
  Analysis of two series together: El Niño &amp; fish population
  <a class="anchor" href="#analysis-of-two-series-together-el-ni%c3%b1o--fish-population">#</a>
</h2>
<p><embed src="/img/week-X/TS1-ElNino.pdf" width="100%" style="display: block; margin: auto;" type="application/pdf" /></p>
<p>How many cycles can you spot?
Is there a relationship between both series?</p>
<h1 id="basic-models-ts-12">
  Basic models (TS 1.2)
  <a class="anchor" href="#basic-models-ts-12">#</a>
</h1>
<h2 id="preliminaries">
  Preliminaries
  <a class="anchor" href="#preliminaries">#</a>
</h2>
<ul>
<li>Our primary objective is to develop mathematical models that provide plausible descriptions for sample data.</li>
<li>A time series is a sequence of rv’s <code>\(x_1\)</code>, <code>\(x_2\)</code>, <code>\(x_3\)</code>, <code>\(\ldots\)</code> , <strong>denoted <code>\(\{x_t\}\)</code></strong></li>
<li>In this course, <code>\(t\)</code> will typically be discrete and be <code>\(\in \mathbb{N}\)</code> (or subset)</li>
<li>One set of observed values of <code>\(\{x_t\}\)</code> is referred to as a <em>realisation</em></li>
<li>Time series are usually plotted with time in the <code>\(x\)</code>-axis, with observations connected at adjacent periods</li>
<li>Sampling rate must be sufficient, lest appearance of the data is changed completely (<em>aliasing</em>)</li>
<li>Smoothness of the time series suggests some level of correlation between adjacent points, or in other words that <code>\(x_t\)</code> depends in some way on the past values <code>\(x_{t-1}\)</code>, <code>\(x_{t-2}\)</code>, <code>\(\ldots\)</code>. <code>\(\rightarrow\)</code> This is a good starting point for imagining appropriate theoretical models!</li>
</ul>
<!-- ## White noise - 3 scales :-) -->
<!-- Let's define `\(w_t\)` as \underline{uncorrelated} random variables `\(w_t\)` with mean 0 and finite variance `\(\sigma_w^2\)`. This is denoted -->
<!-- `$$w_t \sim \text{wn}(0,\sigma_w^2),$$` -->
<!-- and is called a **white noise**. Two special cases: -->
<!-- \begin{description} -->
<!-- \item[White independent noise:] (or iid noise) additional assumption of \underline{iid}, denoted -->
<!-- `$$w_t \sim \text{iid}(0,\sigma_w^2).$$` -->
<!-- \item[Gaussian white noise:] further additional assumption of \underline{normal distribution}, denoted -->
<!-- `$$w_t \sim \text{iid\,N}(0,\sigma_w^2).$$` -->
<!-- \end{description} -->
<!-- Usually, time series are smoother than that (see bottom graph on the next slide). Ways of introducing \emph{serial correlation} and more \emph{smoothness}   -->
<!-- into time series include \alert{filtering} and \alert{autoregression}. -->
<!-- ## Gaussian white noise series and its 3-point moving average -->
<!-- ```{r basicmodels, include=TRUE,echo=3:6,results="hide"} -->
<!-- pdf("TS1-WN.pdf",width=14,height=7.5) -->
<!-- par(mfrow=c(2,1),mar=c(3,3,1.5,.5),mgp=c(1.6,.6,0)) -->
<!-- w = rnorm(500,0,1) # 500 N(0,1) variates  -->
<!-- plot.ts(w, ylim=c(-3,3),  -->
<!--         main="white noise") -->
<!-- v = filter(w, sides=2, filter=rep(1/3,3)) # moving average  -->
<!-- plot.ts(v, ylim=c(-3,3),  -->
<!--         main="moving average") -->
<!-- dev.off() -->
<!-- ``` -->
<!-- \includegraphics[width=0.9\textwidth]{TS1-WN.pdf}  -->
<h2 id="filtering-and-moving-average">
  Filtering (and moving average)
  <a class="anchor" href="#filtering-and-moving-average">#</a>
</h2>
<p>A series <code>\(v_t\)</code> which is a <strong>linear combination</strong> of values of a more fundamental time series <code>\(w_t\)</code> is called a .</p>
<!-- - Example: 3-point moving average (see bottom of previous slide for graph): -->
<!-- `$$v_t=\frac{1}{3}(w_{t-1}+w_t+w_{t+1}).$$` -->
<!-- - In R, moving averages are implemented through the function -->
<!--   ``` -->
<!--   filter(x, filter, method = c("convolution"),sides = 2) -->
<!--   ``` -->
<!--   where ```x``` is the original series, ```filter``` is a vector of weights (in reverse time order), ```method = c("convolution")``` is the default (alternative is `recursive`), and where ```sides``` is 1 for past values only, and 2 if weights are centered around lag 0 (requires uneven number of weights).  -->
<!-- - Moving averages will be denoted \alert{MA($q$)} (details in Module 8). -->
<!-- ## Autoregressions -->
<!-- A series `\(x_t\)` that depends on some of its past values, as well as a noise `\(w_t\)` is called an \alert{autoregression}, because the formula looks like a regression---not of independent variables, but of its own past values---hence \underline{auto}regression. -->
<!-- - Example: An autoregression of the white noise: -->
<!-- `$$x_t=x_{t-1}-0.9x_{t-2}+w_t.$$` -->
<!-- - If the autoregression goes back `\(k\)` periods, one needs `\(k\)` initial conditions (`filter` will use 0's otherwise). -->
<!-- - In R, autoregressions are implemented through the function -->
<!--   ``` -->
<!--   filter(x, filter, method = c("recursive"),init) -->
<!--   ``` -->
<!-- where ```x``` is the original series, ```filter``` is a vector of weights (reverse time order) and  ```init``` a vector of initial values (reverse time order). -->
<!-- - Autoregressions will be denoted \alert{AR($p$)} (details in Module 8). -->
<!-- ## Autoregression examples -->
<!-- ``` {r,echo=3:7,include=TRUE,results="hide",tidy.opts=list(blank=FALSE,indent=2,arrow=TRUE,width.cutoff = 30)} -->
<!-- pdf("TS1-ARex.pdf",width=14,height=11) -->
<!-- par(mfrow=c(2,1),mar=c(2.5,2.5,1.5,.5),mgp=c(1.6,.6,0)) -->
<!-- w = rnorm(550,0,1) # 50 extra to avoid startup problems  -->
<!-- x = filter(w, filter=c(1,-.9), method="recursive")[-(1:50)] # remove first 50  -->
<!-- plot.ts(x, ylab="autogression", -->
<!--         main="Autoregressive series generated from model x_t=x_{t-1}-0.9x_{t-2}+w_t") -->
<!-- y = filter(w, filter=c(1,-.3), method="recursive")[-(1:50)] # remove first 50  -->
<!-- plot.ts(y, ylab="autogression", -->
<!--         main="Autoregressive series generated from model x_t=x_{t-1}-0.3x_{t-2}+w_t") -->
<!-- dev.off() -->
<!-- ``` -->
<!-- --- -->
<!-- \includegraphics[width=0.93\textwidth]{TS1-ARex.pdf} -->
<!-- ## Random walk with drift -->
<!-- - The autoregressions introduced above are all centered around 0 for all `\(t\)` (in the expected sense). -->
<!-- - Assume now that the series increases linearly by `\(\delta\)` (called \alert{drift}) every time unit. -->
<!-- - The \alert{random walk with drift} looks back only one time unit: -->
<!-- `$$x_t=\delta+x_{t-1}+w_t = \delta t + \sum_{j=1}^t w_j \text{ for }t=1,2,\ldots$$` -->
<!-- with initial condition `\(x_0=0\)` and with `\(w_t\)` a white noise. -->
<!-- - If `\(\delta=0\)` this is simply called a \alert{random walk}. -->
<!-- - The term can be explained by visualising each increment from `\(t\)` to `\(t+1\)` as a purely random step from wherever the process is at `\(x_t\)`, ignoring what happened before. -->
<!-- ### Random walk with drift `\(\delta=0.2\)` and `\(\sigma_w=1\)` -->
<!-- \includegraphics[width=\textwidth]{TS1-RW.pdf} -->
<!-- --- -->
<!-- Code used to generate the plot: -->
<!-- ```{r,echo=3:10,include=TRUE,results="hide"} -->
<!-- pdf("TS1-RW.pdf",width=14,height=7) -->
<!-- par(mfrow=c(1,1),mar=c(3,2,1.5,.5),mgp=c(1.6,.6,0)) -->
<!-- set.seed(155) # so you can reproduce the results -->
<!-- w = rnorm(200); x = cumsum(w)  -->
<!-- wd = w +.2; xd = cumsum(wd) -->
<!-- plot.ts(xd, ylim=c(-5,55), main="random walk", ylab='') -->
<!-- lines(x, col=4); abline(h=0, col=4, lty=2); abline(a=0, b=.2, lty=2) -->
<!-- dev.off() -->
<!-- ``` -->
<!-- # Describing the behaviour of basic models (TS 1.3) -->
<!-- ## Motivation -->
<!-- - In this section we would like to develop \alert{theoretical measures} to help describe how times series behave. -->
<!-- \vfill -->
<!-- - We are particularly interested in describing the \alert{relationships} between observations at different points in time. -->
<!-- ## Full specification -->
<!-- - A full specification of a time series of size `\(n\)` at times `\(t_1,t_2,\ldots,t_n\)` for any `\(n\)` would require the full joint distribution function -->
<!-- `$$F_{t_1,t_2,\ldots,t_n}(c_1,c_2,\ldots,c_n)=\Pr[x_{t_1}\le c_1,x_{t_2}\le c_2,\ldots,x_{t_n}\le c_n].$$` -->
<!-- This is a quite unwieldy tool for analysis. -->
<!-- - Examination of the margins `\(F_t(x)=\Pr[x_t\le x]\)` and corresponding pdf `\(f_t(x)\)`, \emph{when they exist}, can be informative. -->
<!-- - These are very theoretical. In practice, one often have only \underline{one} realisation for each `\(x_t\)` so that inferring full distributions (let alone their dependence structure) is simply impractical without tricks, manipulations, and assumptions (some of which we will learn). -->
<!-- ## Mean function -->
<!-- The \alert{mean function} is defined as -->
<!-- `$$\mu_{xt}=E[x_t]=\int_{-\infty}^\infty x f_t(x) dx.$$` -->
<!-- Examples: -->
<!-- \begin{description} -->
<!-- \item[Moving Average Series:] we have -->
<!-- `$$\mu_{vt}=E[v_t]=\frac{1}{3}\left(E[w_{t-1}]+E[w_t]+E[w_{t+1}]\right)=0.$$` -->
<!-- Smoothing does not change the mean. -->
<!-- \item[Random walk with drift:] we have -->
<!-- `$$\mu_{xt}=E[x_t]=\delta t + \sum_{j=1}^t E[w_j]=\delta t.$$` -->
<!-- \end{description} -->
<!-- ## Autocovariance function -->
<!-- The \alert{autocovariance function} is defined as the second moment product -->
<!-- `$$\gamma_x(s,t)=Cov(x_s,x_t)=E\left[(x_s-\mu_{xs})(x_t-\mu_{xt})\right]$$` -->
<!-- for all `\(s\)` and `\(t\)`. Note: -->
<!-- - We will write `\(\gamma_x(s,t)=\gamma(s,t)\)` if no confusion is possible. -->
<!-- - This is a measure of \underline{linear} dependence. -->
<!-- - Smooth series `\(\rightarrow\)` large `\(\gamma\)` even for `\(t\)` and `\(s\)` far apart -->
<!-- - Choppy series `\(\rightarrow\)` `\(\gamma\)` is nearly zero for large separations -->
<!-- - [$\gamma_x(s,t)=0$ `\(\Longrightarrow\)` independence] `\(\Longleftarrow\)` all variables are normal -->
<!-- For two series `\(x_t\)` and `\(y_t\)` this becomes -->
<!-- `$$\gamma_{xy}(s,t)=Cov(x_s,y_t)=E\left[(x_s-\mu_{xs})(y_t-\mu_{yt})\right],$$` -->
<!-- called \alert{cross-covariance function}. -->
<!-- ### Examples of autocovariance functions -->
<!-- \begin{description} -->
<!-- \item[White noise:] The white noise series `\(w_t\)` has `\(E[w_t]=0\)` and -->
<!-- $$\gamma_w(s,t)=Cov(w_s,w_t)=\left\{ \begin{array}{lc} -->
<!-- \sigma_w^2 & s=t \\ -->
<!-- 0 & s\neq t -->
<!-- \end{array}\right.$$ -->
<!-- \end{description} -->
<!-- --- -->
<!-- Remember that if  -->
<!-- `$$U=\sum_{j=1}^m a_j X_j$$`  -->
<!-- and  -->
<!-- `$$V=\sum_{k=1}^r b_k Y_k$$`  -->
<!-- then -->
<!-- `$$Cov(U,V) = \sum_{j=1}^m\sum_{k=1}^ra_j b_kCov(X_j,Y_k).$$` -->
<!-- This will be useful for computing `\(\gamma\)` of filtered series. -->
<!-- --- -->
<!-- \begin{description} -->
<!-- \item[Moving average:] A 3-point moving average `\(v_t\)` to the white noise series `\(w_t\)` has -->
<!-- $$\gamma_v(s,t)=Cov(v_s,v_t)=\left\{ \begin{array}{lc} -->
<!-- \frac{3}{9} \sigma_w^2 & s=t \\ -->
<!-- \frac{2}{9} \sigma_w^2 & |s-t|=1 \\ -->
<!-- \frac{1}{9} \sigma_w^2 & |s-t|=2 \\ -->
<!-- 0 & |s-t|>2 -->
<!-- \end{array}\right.$$ -->
<!-- This only depends on the time separation lag \underline{only}, and not on the absolute location along the series. -->
<!-- This is related to the concept of \emph{weak stationarity} which will introduce later. -->
<!-- \end{description} -->
<!-- --- -->
<!-- \begin{description} -->
<!-- \item[Random walk:] For the random walk `\(x_t=\sum_{j=1}^t w_j\)` we have -->
<!-- \begin{eqnarray*} -->
<!-- \gamma_x(s,t)&=&Cov(x_s,x_t) \\ -->
<!-- &=& Cov \left(\sum_{j=1}^s w_j,\sum_{k=1}^t w_k \right) \\ -->
<!-- &=& \min\{s,t\}\sigma_w^2. -->
<!-- \end{eqnarray*} -->
<!-- Contrary to the previous examples, this depends on the absolute location rather than the lag. -->
<!-- Also `\(Var(x_t)=t\sigma_w^2\)` increases without bound as `\(t\)` increases. -->
<!-- \end{description} -->
<!-- ## The autocorrelation function (ACF) -->
<!-- The \alert{autocorrelation function (ACF)} is defined as -->
<!-- `$$-1 \le \rho(s,t)=\frac{\gamma(s,t)}{\sqrt{\gamma(s,s)\gamma(t,t)}} \le 1.$$` -->
<!-- - The ACF measures the \emph{linear} predictability of the series at time `\(t\)`, say `\(x_t\)`, using \emph{only} the value `\(x_s\)`. -->
<!-- - If we could do that perfectly, then `\(\rho(s,t) \pm 1\)` and -->
<!-- `$$x_t=\beta_0+\beta_1 x_s$$` -->
<!-- with `\(\beta_1\)` of same sign as `\(\rho(s,t)\)`. -->
<!-- In the case of two series this becomes -->
<!-- `$$-1 \le \rho_{xy}(s,t)=\frac{\gamma_{xy}(s,t)}{\sqrt{\gamma_x(s,s)\gamma_y(t,t)}} \le 1,$$` -->
<!-- called \alert{cross-correlation function (CCF)}. -->
<!-- # Stationary time series (TS 1.4)  -->
<!-- ## Strict stationarity -->
<!-- A \alert{strictly stationary} times series is one for which the probabilistic behaviour of every collection of values `\(\{x_{t_1},x_{t_2},\ldots,x_{t_k}\}\)` -->
<!-- is identical to that of the time shifted set (for any `\(h\)`) `\(\{x_{t_1+h},x_{t_2+h},\ldots,x_{t_k+h}\}.\)` That is, -->
<!-- `$$\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\Pr[x_{t_1}\le c_1,x_{t_2}\le c_2,\ldots,x_{t_k}\le c_k]$$` -->
<!-- `$$\;\;= \Pr[x_{t_1+h}\le c_1,x_{t_2+h}\le c_2,\ldots,x_{t_k+h}\le c_k]$$` -->
<!-- for all `\(k=1,2,\ldots\)`, all the time points `\(t_1, t_2,\ldots,t_k\)`, all numbers `\(c_1,c_2,\ldots,c_k\)` and all time shifts `\(h=0,\pm 1,\pm 2,\ldots\)`. -->
<!-- This implies -->
<!-- - identical marginals of dimensions `\(<k\)` for any shift `\(h\)` -->
<!-- - constant mean: `\(\mu_{xs}=\mu_{xt}\equiv\mu\)` -->
<!-- - for `\(k=2\)`, an autocovariance function that depends only on `\(t-s\)`: `\(\gamma(s,t)=\gamma(s+h,t+h)\)` -->
<!-- We need something less constraining, that still allows us to infer   -->
<!-- properties from a single series. -->
<!-- ## Weak stationarity -->
<!-- A \alert{weakly stationary time series}, `\(x_t\)`, is a finite variance process such that -->
<!-- 1. the mean value function, `\(\mu_{xt}\)`   -->
<!--   is \underline{constant} and does not depend on time `\(t\)`, and -->
<!-- 1. the autocovariance function, `\(\gamma(s,t)\)` \underline{depends on `\(s\)` and `\(t\)` only through their difference `\(|s-t|\)`}. -->
<!-- Note: -->
<!-- - We dropped full distributional requirements. This imposes conditions on the first two moments of the series only. -->
<!-- - Since those completely define a normal distribution, a (weak) stationary Gaussian time series is also strictly stationary. -->
<!-- - We will use the term stationary to mean weakly stationary; if a process is stationary in the strict sense, we will use the term strictly stationary.   -->
<!-- \alert{Stationarity means we can estimate those two quantities}   -->
<!-- \alert{ by averaging of a \emph{single} series. This is what we needed.} -->
<!-- ## Properties of stationary series -->
<!-- - Because of condition 1, -->
<!-- `$$\mu_t=\mu.$$` -->
<!-- - Because of condition 2, -->
<!-- `$$\gamma(t+h,t)=Cov(x_{t+h},x_t)\alert{=}Cov(x_h,x_0)=\gamma(h,0)\alert{\equiv}\gamma(h)$$` -->
<!-- and the autocovariance of a stationary time series is then -->
<!-- `$$\gamma(h)=Cov(x_{t+h},x_t)=E[(x_{t+h}-\mu)(x_t-\mu)].$$` -->
<!-- The quantity `\(h\)` is the \alert{} -->
<!-- - `\(\gamma(h)\)` is \emph{non-negative definite}, which means that the variance of linear combinations of variates `\(x_t\)` will never be negative, that is, -->
<!-- `$$0\le Var(a_1x_1+\cdots + a_nx_n)=\sum_{j=1}^n\sum_{k=1}^n a_j a_k \gamma(j-k).$$` -->
<!-- --- -->
<!-- - Furthermore, -->
<!-- `$$|\gamma(h)|\le \gamma(0) \text{ (the variance of the time series)}$$` -->
<!-- and -->
<!-- `$$\gamma(h)=\gamma(-h).$$` -->
<!-- - The autocorrelation function (ACF) of a stationary time series becomes -->
<!-- `$$-1\le \alert{\rho(h)}=\frac{\gamma(t+h,t)}{\sqrt{\gamma(t+h,t+h)\gamma(t,t)}}\alert{=\frac{\gamma(h)}{\gamma(0)}} \le 1.$$` -->
<!-- ## Examples of (non)-stationarity -->
<!-- \begin{description} -->
<!-- \item[White noise:] We have -->
<!-- `$$\mu_{wt}=0$$` -->
<!-- and -->
<!-- $$\gamma_w(h)=Cov(w_{t+h},w_t)=\left\{ \begin{array}{lc} -->
<!-- \sigma_w^2 & h=0, \\ -->
<!-- 0 & h\neq 0, -->
<!-- \end{array}\right.,$$ -->
<!-- which are both independent of time. Hence, the white noise satisfies both conditions and is (weakly) stationary. Furthermore, -->
<!-- $$\rho_w(h)=\left\{ \begin{array}{lc} -->
<!-- 1 & h=0, \\ -->
<!-- 0 & h\neq 0. -->
<!-- \end{array}\right.$$ -->
<!-- If in addition `\(w_t\sim \text{iid N}(0,\sigma_w^2)\)`, then it is also strictly stationary. -->
<!-- \end{description} -->
<!-- --- -->
<!-- \begin{description} -->
<!-- \item[Moving average:] For the 3-point MA we have -->
<!-- $$\mu_{vt}=0\quad \text{ and } \quad \gamma_v(h)=\left\{ \begin{array}{lc} -->
<!-- \frac{3}{9} \sigma_w^2 & h=0, \\ -->
<!-- \frac{2}{9} \sigma_w^2 & h\pm1, \\ -->
<!-- \frac{1}{9} \sigma_w^2 & h\pm2, \\ -->
<!-- 0 & |h|>2, -->
<!-- \end{array}\right.$$ -->
<!-- which are both independent of time. Hence, the 3-point MA satisfies both conditions and is stationary. Furthermore, -->
<!-- $$\rho_v(h)=\left\{ \begin{array}{lc} -->
<!-- 1  & h=0, \\ -->
<!-- \frac{2}{3}  & h\pm1, \\ -->
<!-- \frac{1}{3}  & h\pm2, \\ -->
<!-- 0 & |h|>2, -->
<!-- \end{array}\right.$$ -->
<!-- which is symmetric around lag 0. -->
<!-- \end{description} -->
<!-- --- -->
<!-- \begin{description} -->
<!-- \item[Random walk:] For the random walk model `\(x_t=\sum_{j=1}^t w_j\)` we have -->
<!-- `$$\mu_{xt}=\delta t,$$` -->
<!-- which is a function of time `\(t\)`, and -->
<!-- `$$\gamma(s,t)=\min\{s,t\}\sigma_w^2,$$` -->
<!-- which depends on `\(s\)` and `\(t\)` (not just their difference), so the random walk is \underline{not} stationary. -->
<!-- Furthermore, remember -->
<!-- `$$Var(x_t)=\gamma_x(t,t)=t\sigma_w^2$$` -->
<!-- which increases without bound as `\(t\rightarrow \infty\)`. -->
<!-- \end{description} -->
<!-- ## Trend stationarity -->
<!-- - If only the second condition (on the ACF) is satisfied, but not the first condition (on the mean value function), we have \alert{trend stationarity} -->
<!-- - This means that the model has a stationary \emph{behaviour} \underline{around its trend}. -->
<!-- - Example: if -->
<!-- `$$x_t=\alpha + \beta t + y_t \quad \text{where }y_t\text{ is stationary,}$$` -->
<!--   then the mean function is -->
<!--   $$\mu_{x,t}=E[x_t]=\alpha + \beta t + \mu_y, $$ -->
<!--   which is \emph{not} independent of time. The autocovariance function, -->
<!--   \begin{eqnarray*} -->
<!--   \gamma_x(h)=Cov(x_{t+h},x_t)&=&E[(x_{t+h}-\mu_{x,t+h})(x_t-\mu_{x,t})]\\ -->
<!--   &=&E[(y_{t+h}-\mu_y)(y_t-\mu_y)]=\gamma_y(h), -->
<!--   \end{eqnarray*} -->
<!--   however, is independent of time. -->
<!-- ## Joint stationarity -->
<!-- Two time series, say, `\(x_t\)` and `\(y_t\)`, are said to be \alert{jointly stationary} if they are each stationary, and the cross-covariance function -->
<!-- `$$\gamma_{xy}(h) = Cov(x_{t+h}, y_t) = E[(x_{t+h} - \mu_x)(y_t - \mu_y)]$$` -->
<!-- is a function only of lag `\(h\)`. -->
<!-- The corresponding \alert{cross-correlation function (CCF)} is -->
<!-- `$$1\le \rho_{xy}(h)=\frac{\gamma_{xy}(h)}{\sqrt{\gamma_x(0)\gamma_y(0)}}\le 1.$$` -->
<!-- Note: -->
<!-- - Because `\(Cov(x_2,y_1)\)` and `\(Cov(x_1,y_2)\)` (for example) need not be the same, it follows that typically -->
<!--   `$$\rho_{xy}(h) \neq \rho_{xy}(-h),$$` -->
<!--   that is, the CCF is \underline{not generally symmetric about zero}. However, we have -->
<!--   `$$\rho_{xy}(h) = \rho_{\alert{yx}}(-h).$$` -->
<!-- ## Example of joint stationarity -->
<!-- Consider -->
<!-- `$$x_t=w_t+w_{t-1} \quad\text{and}\quad y_t=w_t-w_{t-1},$$` -->
<!-- where `\(w_t\)` are independent with mean 0 and variance `\(\sigma_w^2\)`. We have then -->
<!-- \begin{eqnarray*} -->
<!-- \gamma_x(0)&=&\gamma_y(0)=2\sigma_w^2 \\ -->
<!-- \gamma_x(-1)&=&\gamma_x(1)=\sigma_w^2 \\ -->
<!-- \gamma_y(-1)&=&\gamma_y(1)=-\sigma_w^2  -->
<!-- \end{eqnarray*} -->
<!-- and -->
<!-- `$$\gamma_{xy}(-1) = -\sigma_w^2 ,\quad \gamma_{xy}(0) = 0, \text{ and }\;\;\gamma_{xy}(1) = \sigma_w^2,$$` -->
<!-- so that -->
<!-- $$\rho_{xy}(h)=\left\{ \begin{array}{cl} -->
<!-- 0 & h=0, \\ -->
<!-- 1/2 & h=1, \\ -->
<!-- -1/2 & h=-1, \\ -->
<!-- 0 & |h|\ge 2, -->
<!-- \end{array}\right.$$ -->
<!-- which depends only on the lag `\(h\)`, so both series are jointly stationary. -->
<!-- ### Prediction using cross-correlation -->
<!-- \begin{description} -->
<!-- \item [Prediction using cross-correlation] A lagging relation between two series `\(x_t\)` and `\(y_t\)` may be exploited for predictions. For instance, if -->
<!-- `$$y_t=Ax_{t-\ell} + w_t,$$` -->
<!-- `\(x_t\)` is said to \emph{lead} `\(y_t\)` for `\(\ell>0\)`, and is said to \emph{lag} `\(y_t\)` for `\(\ell<0\)`. -->
<!-- \end{description} -->
<!-- If the relation above holds true, then the lag `\(\ell\)` can be inferred from the shape of the autocovariance of the input series `\(x_t\)`: -->
<!-- - If `\(w_t\)` is uncorrelated with `\(x_t\)` then -->
<!--   \begin{eqnarray*} -->
<!--   \gamma_{yx}(h)&=& Cov(y_{t+h},x_t) =Cov (Ax_{t+h-\ell} + w_{t+h},x_t) \\ -->
<!--   &=&Cov (Ax_{t+h-\ell} ,x_t)=A\gamma_x(h-\ell) -->
<!--   \end{eqnarray*} -->
<!-- - Since -->
<!--   `$$\gamma_x(h-\ell) \le \gamma_x(0),$$` -->
<!--   the peak of `\(\gamma_{yx}(h)\)` should be at `\(h=\ell\)`, and   -->
<!--   `\(h\)` will be positive if `\(x_t\)` leads `\(y_t\)`, negative if `\(x_t\)` lags `\(y_t\)`. -->
<!-- ``` {r} -->
<!-- pdf("TS1-leads.pdf",width=14,height=7) -->
<!-- par(mfrow=c(1,1),mar=c(3,3,3,.5),mgp=c(1.6,.6,0)) -->
<!-- x = rnorm(100) -->
<!-- y = lag(x, -5) + rnorm(100) -->
<!-- ccf(y, x, ylab='CCovF', type='covariance', -->
<!--     main="Prediction using cross-correlation") -->
<!-- dev.off() -->
<!-- ``` -->
<!-- --- -->
<!-- - Here `\(\ell=5\)` and `\(x_t\)` leads `\(y_t\)`: -->
<!-- \includegraphics[width=\textwidth]{TS1-leads.pdf} -->
<!-- Note this example was simulated and uses the R functions ```lag``` and ```ccf```: -->
<!-- ``` {r,eval=FALSE,echo=TRUE,include=TRUE} -->
<!-- x = rnorm(100) -->
<!-- y = lag(x, -5) + rnorm(100) -->
<!-- ccf(y, x, ylab='CCovF', type='covariance') -->
<!-- ``` -->
<!-- ## Linear process -->
<!-- A \alert{linear process}, `\(x_t\)` , is defined to be a linear combination of white noise variates `\(w_t\)`, and is given by -->
<!-- `$$x_t =\mu +\sum_{j=-\infty}^\infty \varPsi_j w_{t-j},\quad \sum_{j=-\infty}^\infty |\varPsi_j| <\infty$$` -->
<!-- This is an important class of models because it encompasses moving averages, autoregressions, and also the combination of both, called \emph{autoregressive moving average (ARMA) processes} which we will introduce later. -->
<!-- Example: -->
<!-- \begin{description} -->
<!-- \item[Moving average] The 3-point moving average has -->
<!-- `$$\varPsi_0=\varPsi_{-1}=\varPsi_1=1/3$$` -->
<!-- and is hence a linear process. -->
<!-- \end{description} -->
<!-- ### Properties of linear processes -->
<!-- - The autocovariance function of a linear process is given by -->
<!-- `$$\gamma_x(h)=\sigma_w^2 \sum_{j=-\infty}^\infty \varPsi_{j+h}\varPsi_j \quad \text{for }h\ge 0.$$` -->
<!-- - It has finite variance if `\(\sum_{j=-\infty}^\infty \varPsi_j^2 <\infty\)`. -->
<!-- - In its most general form `\(x_t\)` depends on the future ($j<0$ components), the present ($j=0$) and the past ($j>0$).   -->
<!-- For forecasting, a model dependent on the future is useless. We will focus on processes that do not depend on the future. Such processes are called \alert{causal}, that is, -->
<!-- $$ x_t \text{ is causal} \Longleftrightarrow \varPsi_j=0\quad\text{ for } \quad j<0,$$ -->
<!-- which we will assume unless stated otherwise. -->
<!-- <!-- - The \alert{Wold decomposition} states -->
<p>–&gt;</p>
<!-- <!-- $$\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\text{if a times series is stationary} \Longrightarrow $$ --> –>
<!-- <!-- `$$\text{it is a causal linear process }\left(\text{with }\sum_{j=-\infty}^\infty \varPsi_j^2 <\infty\right)$$` --> –>
<!-- <!-- This is a crucial result, as it justifies the use of causal linear processes to study stationary time series. --> –>
<!-- # Estimation of correlation (TS 1.5) -->
<!-- ## Background -->
<!-- - One can very rarely hypothetise (specify) time series. In practice, most analyses are performed using sample data. -->
<!-- - Furthermore, \alert{one often has only one realisation of the time series}. -->
<!-- - This means that we don't have `\(n\)` realisations of the time series to estimate its covariance and correlation functions. -->
<!-- - This is why the \alert{assumption of stationarity is essential}: in this case, the assumed `homogeneity' of the data means we can estimate those functions on one realisation only. -->
<!-- - This also means that one needs to manipulate / de-trend series such that they are arguably stationary before we can fit parameters to them and use them for projections. -->
<!-- ## Sample mean -->
<!-- If a time series is stationary the mean function `\(\mu_t = \mu\)` is constant so that we can estimate it by the sample mean, -->
<!-- $$ \overline{x}=\frac{1}{n}\sum_{t=1}^n x_t.$$ -->
<!-- This estimator is unbiased, -->
<!-- `$$E[\overline{x}]=\mu,$$` -->
<!-- and has standard error the square root of -->
<!-- `$$Var(\overline{x}) = \frac{1}{n^2}Cov\left(\sum_{t=1}^n x_t,\sum_{s=1}^n x_s\right)=\frac{1}{n}\sum_{h=-n}^n\left(1-\frac{|h|}{n}\right)\gamma_x(h).$$` -->
<!-- ## Sample autocovariance function -->
<!-- The \alert{sample autocovariance function} is defined as -->
<!-- `$$\hat{\gamma}(h)=\frac{1}{n}\sum_{t=1}^{n-h}(x_{t+h}-\overline{x})(x_t-\overline{x}) \quad\text{with }\hat{\gamma}(-h)=\hat{\gamma}(h)\text{ for }h=0,1,\ldots,n-1.$$` -->
<!-- Note: -->
<!-- - The estimator is biased. -->
<!-- - The sum runs over a restricted range ($n-h$) because `\(x_{t+h}\)` is not available for `\(t+h>n\)`. -->
<!-- - One could wonder why the factor of the sum is not `\(1/(n-h)\)` (the number of elements in the sum), but factor `\(1/n\)` is not a mistake. It ensures that the estimate of the variances of linear combinations, -->
<!-- `$$\widehat{Var}(a_1x_1+\cdots+a_nx_n)= \sum_{j=1}^n\sum_{k=1}^n a_j a_k \hat{\gamma}(j-k),$$` -->
<!-- is non-negative. -->
<!-- ## Sample autocorrelation function -->
<!-- The \alert{sample autocorrelation function (SACF)} is -->
<!-- `$$\hat{\rho}(h)=\frac{\hat{\gamma}(h)}{\hat{\gamma}(0)}.$$` -->
<!-- Under some conditions (see book for details), if `\(x_t\)` is a white noise, then for large `\(n\)`, the SACF `\(\hat{\rho}(h)\)` is approximately normally distributed with zero mean and standard deviation given by -->
<!--   `$$\sigma_{\hat{\rho}(h)}=\frac{1}{\sqrt{n}}.$$` -->
<!-- ## Testing for significance of autocorrelation -->
<!-- The asymptotic result for the variance of the SACF means we can test whether lagged observations are uncorrelated (which is a requirement for white noise): -->
<!-- - test for significance of the `\(\hat{\rho}\)`'s at different lags: check how many `\(\hat{\rho}\)`'s lie outside the interval `\(\pm 2/\sqrt{n}\)` (a 95\% confidence interval) -->
<!-- - One should expect approximately 1 out of 20 to lie outside the interval if the sequence is a white noise. Many more than that would invalidate the whiteness assumption. -->
<!-- - This allows for a recursive approach for manipulating / de-trending series until they are white noise, called \alert{whitening}. -->
<!-- - The R function ```acf``` automatically displays those bounds with dashed blue lines. -->
<!-- ### SOI autocorrelation -->
<!-- ``` {r,echo=3,include=TRUE,results="hide"} -->
<!-- pdf("TS1-SOIacf.pdf",width=14,height=5.5) -->
<!-- par(mfrow=c(1,1),mar=c(3,3,3,.5),mgp=c(1.6,.6,0)) -->
<!-- acf(soi,main="Sample autocorrelation function (SACF) of SOI") -->
<!-- dev.off() -->
<!-- ``` -->
<!-- \includegraphics[width=\textwidth]{TS1-SOIacf.pdf} -->
<!-- ``` {r,echo=1,include=TRUE,results="markup",output.lines=1} -->
<!-- r <- round(acf(soi, 6, plot=FALSE)$acf[-1], 3) # first 6 sample acf values -->
<!-- r -->
<!-- ``` -->
<!-- The SOI series is clearly not a white noise. -->
<!-- --- -->
<!-- ``` {r,echo=4:7,include=TRUE,results="hide"} -->
<!-- #r = round(acf(soi, 6, plot=FALSE)$acf[-1], 3) # first 6 sample acf values blab blab bab -->
<!-- pdf("TS1-soi-corrplots.pdf",width=14,height=7) -->
<!-- par(mfrow=c(1,2),mar=c(3,3,3,.5),mgp=c(1.6,.6,0)) -->
<!-- plot(lag(soi,-1), soi,main="SOI pairs of values 1 month apart") -->
<!-- legend('topleft', legend=r[1])  -->
<!-- plot(lag(soi,-6), soi,main="SOI pairs of values 6 months apart")  -->
<!-- legend('topleft', legend=r[6]) -->
<!-- dev.off() -->
<!-- ``` -->
<!-- \includegraphics[width=\textwidth]{TS1-soi-corrplots.pdf} -->
<!-- Scatterplots allow to have a visual representation of the dependence   -->
<!-- (which may not necessarily be linear). -->
<!-- ## Sample cross-covariances and cross-correlations -->
<!-- The \alert{sample cross-covariance function} is -->
<!-- `$$\hat{\gamma}_{xy}(h)=\frac{1}{n} \sum_{t=1}^{n-h} (x_{t+h}-\overline{x})(y_t-\overline{y}),$$` -->
<!-- where `\(\hat{\gamma}_{xy}(-h)=\hat{\gamma}_{yx}(h)\)` determines the function for \emph{negative} lags. -->
<!-- The \alert{sample cross-correlation function} is -->
<!-- `$$-1\le \hat{\rho}_{xy}(h)=\frac{\hat{\gamma}_{xy}(h)}{\sqrt{\hat{\gamma}_x(0)\hat{\gamma}_y(0)}}\le 1.$$` -->
<!-- Note: -->
<!-- - Graphical examinations of `\(\hat{\rho}_{xy}(h)\)` provide information about the leading or lagging relations in the data. -->
<!-- ## Testing for independent cross-whiteness -->
<!-- If `\(x_t\)` and `\(y_t\)` are \emph{independent} linear processes then the large sample distribution of `\(\hat{\rho}_{xy}(h)\)` has mean 0 and -->
<!-- `$$\sigma_{\hat{\rho}_{xy}}=\frac{1}{\sqrt{n}}$$` -->
<!-- if at least one of the processes is independent white noise. -->
<!-- This is very useful, and adds to the toolbox: -->
<!-- - This provides feedback about the quality of our explanation of the relationship between both time series: if we have explained the trends and relationships between both processes, then their residuals should be independent white noise. -->
<!-- - After each improvement of our model, significance of the `\(\hat{\rho}_{xy}\)`'s of the residuals can be tested: if we have independent cross-whiteness then we have a good model. If the `\(\hat{\rho}_{xy}\)`'s are still significant (outside   -->
<!-- the boundaries) then we still have things to explain (to add). -->
<!-- ### SOI and recruitment correlation analysis -->
<!-- ``` {r,echo=3:5,include=TRUE,results="hide"} -->
<!-- pdf("TS1-soi-recruitment.pdf",width=14,height=6) -->
<!-- par(mfrow=c(3,1),mar=c(3,3,3,.5),mgp=c(1.6,.6,0)) -->
<!-- acf(soi, 48, main="Southern Oscillation Index") -->
<!-- acf(rec, 48, main="Recruitment") -->
<!-- ccf(soi, rec, 48, main="SOI vs Recruitment", ylab="CCF") -->
<!-- dev.off() -->
<!-- ``` -->
<!-- \includegraphics[width=\textwidth]{TS1-soi-recruitment.pdf} -->
<!-- The SCCF (bottom) has a different cycle, and peak at `\(h=-6\)`   -->
<!-- suggests SOI leads Recruitment by 6 months (negatively). -->
<!-- ### Idea of prewhitening -->
<!-- - to use the text of cross-whiteness one needs to "prewhiten" at least one of the series -->
<!-- - for the SOI vs recruitment example, there is strong seasonality which, upon removal, may whiten the series -->
<!-- - we look at an example here that looks like the SOI vs recruitment example, and show how this seasonality could be removed with the help of `\(\sin\)` and `\(\cos\)` functions -->
<!-- Example: -->
<!-- - Let us generate two series `\(x_t\)` and `\(y_t\)`, for `\(t=1,\ldots,120\)`, independently as -->
<!-- $$ x_t=2\cos\left(2\pi t\frac{1}{12}\right)+w_{t`}\quad\text{and}\quad y_t=2 \cos\left(2\pi[t+5])\frac{1}{12}\right)+w_{t2},$$ -->
<!-- where `\(\{w_{t1},w_{t2}; t=1,\ldots,120\}\)` are all independent standard   -->
<!-- normals. -->
<!-- --- -->
<!-- - this generates the data and plots it: -->
<!-- ``` {r,eval=TRUE,echo=TRUE,include=TRUE,fig.height=3} -->
<!-- set.seed(1492) -->
<!-- num=120; t=1:num -->
<!-- X = ts(2*cos(2*pi*t/12) + rnorm(num), freq=12) -->
<!-- Y = ts(2*cos(2*pi*(t+5)/12) + rnorm(num), freq=12) -->
<!-- par(mfrow=c(1,2), mgp=c(1.6,.6,0), mar=c(3,3,1,1) ) -->
<!-- plot(X) -->
<!-- plot(Y) -->
<!-- ``` -->
<!-- --- -->
<!-- - looking at the ACFs one can see seasonality -->
<!-- ``` {r,eval=TRUE,echo=TRUE,include=TRUE,fig.height=6} -->
<!-- par(mfrow=c(3,2), mgp=c(1.6,.6,0), mar=c(3,3,1,1) ) -->
<!-- acf(X,48, ylab='ACF(X)') -->
<!-- acf(Y,48, ylab='ACF(Y)') -->
<!-- ccf(X,Y,24, ylab='CCF(X,Y)') -->
<!-- ``` -->
<!-- - furthermore the CCF suggests cross-correlation   -->
<!-- even though the series are independent -->
<!-- --- -->
<!-- - what we do now is to "prewhiten" `\(y_t\)` by removing the sginal from the data by running a regression of `\(y_t\)` on `\(\cos (2\pi t)\)` and `\(\sin(2\pi t)\)` and then putting  -->
<!-- `$$\tilde{y}=y_t-\hat{y}_t,$$` -->
<!-- where `\(\hat{y}_t\)` are the predicted values from the regression. -->
<!-- - in the R code below, `Yw` is `\(\tilde{y}\)` -->
<!-- ``` {r,eval=TRUE,echo=TRUE,include=TRUE,fig.height=2.9} -->
<!-- par(mgp=c(1.6,.6,0), mar=c(3,3,1,1) ) -->
<!-- Yw = resid( lm(Y~ cos(2*pi*t/12) + sin(2*pi*t/12), na.action=NULL) )  -->
<!-- ccf(X,Yw,24, ylab='CCF(X,Yw)', ylim=c(-.3,.3)) -->
<!-- ``` -->
<ul>
<li>the updated CCF now suggests cross-independence as it should</li>
</ul>
<h1 id="references">
  References
  <a class="anchor" href="#references">#</a>
</h1>
</article>
 
      <script src="//yihui.org/js/math-code.js"></script>
<script async
src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">



  <div><a class="flex align-center" href="https://github.com/UniMelb-Actuarial/commit/05bf3c0b5cfbeb01a4adbc3c5a5ad71560c2835c" title='Last modified by Benjamin Avanzi | 18 February 2022' target="_blank" rel="noopener">
      <img src="/svg/calendar.svg" class="book-icon" alt="Calendar" />
      <span>18 February 2022</span>
    </a>
  </div>



</div>



  <script>(function(){function a(c){const a=window.getSelection(),b=document.createRange();b.selectNodeContents(c),a.removeAllRanges(),a.addRange(b)}document.querySelectorAll("pre code").forEach(b=>{b.addEventListener("click",function(c){a(b.parentElement),navigator.clipboard&&navigator.clipboard.writeText(b.parentElement.textContent)})})})()</script>


 
        <script src="//yihui.org/js/math-code.js"></script>
<script async
src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#introduction-ts-10">Introduction (TS 1.0)</a>
      <ul>
        <li><a href="#definition">Definition</a></li>
        <li><a href="#applications">Applications</a></li>
        <li><a href="#process-for-time-series-analysis">Process for time series analysis</a></li>
      </ul>
    </li>
    <li><a href="#examples-ts-11">Examples (TS 1.1)</a>
      <ul>
        <li><a href="#johnson--johnson-quarterly-earnings-per-share">Johnson &amp; Johnson quarterly earnings per share</a></li>
        <li><a href="#global-mean-land-ocean-temperature-index">Global mean land-ocean temperature index</a></li>
        <li><a href="#dow-jones-industrial-average">Dow Jones Industrial Average</a></li>
        <li><a href="#analysis-of-two-series-together-el-niño--fish-population">Analysis of two series together: El Niño &amp; fish population</a></li>
      </ul>
    </li>
    <li><a href="#basic-models-ts-12">Basic models (TS 1.2)</a>
      <ul>
        <li><a href="#preliminaries">Preliminaries</a></li>
        <li><a href="#filtering-and-moving-average">Filtering (and moving average)</a></li>
      </ul>
    </li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
<script src="//yihui.org/js/math-code.js"></script>
<script async
src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












