[{"id":0,"href":"/docs/0-subject-guide/Eligibility/","title":"Eligibility and Requirements","section":"Subject Guide","content":"To view the eligibility and requirements, including prerequisites, corequisites, recommended background knowledge and core participation requirements for this subject, please see the University Handbook: https://handbook.unimelb.edu.au/2022/subjects/actl30007/eligibility-and-requirements https://handbook.unimelb.edu.au/2022/subjects/actl90020/eligibility-and-requirements\nPrerequisites #  Relevant knowledge from ACTL20003 Stochastic Techniques in Insurance (or MAST20004 Probability) includes in particular:\n Distributions of random variables Expectations and conditional expectations Moment and probability generating functions Law of Large Numbers, and the Central Limit Theorem  Relevant knowledge from MAST20005 Statistics includes in particular:\n Fitting of statistical models to data (Descriptive statistics, maximum likelihood) Parameter estimation (point and interval estimation, confidence intervals) Hypotheses tests, including goodness-of-fit Linear regression and ANOVA Correlation Software R The website includes some basic mathematics and probability concept for your review.  Probability and Statistics #  As described above under \u0026ldquo;Prerequisites\u0026rdquo;, a good prior knowledge of probability and statistics is essential for this course. Some of the required knowledge is reviewed in the Module 1 tutorial set. I strongly recommend you review those materials before the semester starts.\nUse of the R software #  R is required prior knowledge for this course, and part of the prerequisite course MAST20005 (Statistics).\nThis is because R is a required software for the actuarial professional exams CS1 and CS2 (see https://www.actuaries.org.uk/studying/curriculum/actuarial-statistics and also https://www.actuaries.org.uk/studying/curriculum/frequently-asked-questions-curriculum ). It is used very widely by actuaries in industry (see, for instance, http://insightriskconsulting.co.uk/blog/r-for-actuaries/ or https://www.actuaries.digital/2019/09/26/my-top-10-r-packages-for-data-analytics/ ). Some companies also use R to produce presentations and documentation for generally.\nIn order to help you with R I have put together a website that summarises all the things I think you should know before starting your first grad role: https://communicate-data-with-r.netlify.app At the very least, you need to know what is under \u0026ldquo;Base R\u0026rdquo;. Learning the \u0026ldquo;tidyverse\u0026rdquo; will be most useful, as well as \u0026ldquo;ggplot2\u0026rdquo; for better visualisations. \u0026ldquo;htmlwidgets\u0026rdquo; is more advanced, and not required for the course. You may want to create your assignment with \u0026ldquo;R Markdown\u0026rdquo; (under \u0026ldquo;Communicate Data\u0026rdquo;), although this is not required either.\nThe main reference for Base R is the book http://biostatisticien.eu/springeR/index-en.html, which is also available in other languages (including mandarin). The English version can be downloaded for free from the Unimelb library: https://go.openathens.net/redirector/unimelb.edu.au?url=http%3A%2F%2Fdx.doi.org%2F10.1007%2F978-1-4614-9020-3\nI strongly recommend you review the R materials mentioned above before the semester starts.\nSee also the Actuaries Institute Analytics Cookbook: https://www.actuaries.digital/2021/11/30/the-actuaries-analytics-cookbook-recipes-for-data-success/ https://actuariesinstitute.github.io/cookbook/docs/index.html\n"},{"id":1,"href":"/docs/0-study-plan/week-1/","title":"Week 1 Study Plan","section":"Weekly Study Plan","content":"Week 1 Overview #  This week, we will cover the following topics:\n Introduction to the course (Module 1): what are the broader questions we will answer in this subject, and how are they connected? Collective Risk Modelling (Module 2): how do you work out the distribution of the sum of claims if you can make assumptions about their frequency and severity, and what are typical assumptions for claims frequency?   -- If you wish to watch the embedded videos from Lecture Capture, you need to have logged in and entered Lecture Capture via Canvas once for each session. This is to restrict access to students enrolled at the University of Melbourne only. -- See also detailed learning outcomes 1.2.1-1.2.4 of the CS2 syllabus here.\nMain references and lectures #  Prerequisite knowledge #  Please review the Prerequisite knowledge page for this course. This is not exhaustive by any means, but provides minimum knowledge you should be comfortable with. Note that Tutorials in Week 1 are your opportunity to ask about those topics.\nModule 1: Introduction #  Read: Module 1: Introduction\nAnnotate: slides\nWatch: refer to your lecture recording under ‚ÄúLecture Capture‚Äù ( UG/ PG). This is where annotated slides will be made available, too.\nModule 2: Collecture Risk Modelling #  Read: Module 2: Collecture Risk Modelling\nAnnotate: slides\nWatch: refer to your lecture recording under ‚ÄúLecture Capture‚Äù ( UG/ PG) This is where annotated slides will be made available, too.\nAdditional preparation and resources #  Mandatory #   Read the Subject Guide. As outlined above, review the mathematical background and revisions as well as associated tutorial questions. Chapter 1 of Wuthrich (2020) (for Module 1 and revisions) Chapter 2.1 and 2.2 of Wuthrich (2020) (for Module 2)  Optional #   Chapter 12 of Bowers et al. (1997) (free download available from Readings Online)  Tutorials #  Pre-Tutorial work #  Please study those questions before the tutorial.\nPre-Tutorial exercises are available in the Pre-Tutorial book, which already includes solutions. It is recommended to attempt the questions before looking at the solutions\nTutorial materials #  Some questions have been especially selected for the tutorials. Students should review and attempt those questions prior to their scheduled tutorial, after they complete the pre-tutorial work.\nThe Tutorial book includes all questions for the whole semester already, but solutions will only be added sequentially at the end of each week, as we work our way through the set.\nNote that solutions will be gradually added to that same document. Hence it is not recommended to print it, as it will regularly change (typos will also dynamically be corrected).\nThis week #  The whole of Module 1 is in scope for the week 1 tutorials. These cover a broad range of revisions.\nA recording will be made available for each cohort on their respective Canvas.\n -- If you wish to watch the embedded videos from Lecture Capture, you need to have logged in and entered Lecture Capture via Canvas once for each session. This is to restrict access to students enrolled at the University of Melbourne only. -- Next week (week 2) #  Next week, we will discuss the whole of Module 2.\nAdditional questions #  The ‚Äúadditional questions‚Äù are here for reinforcement or revision, but are not the main focus of the tutorials. Solutions for those exercises are already available.\nPreparation for assessment #  Mid-semester (15%) and final (60%) exams #   Nothing to do for now - just don‚Äôt fall behind üòÑ.  Assignment (25%) #   Nothing to do for now.  References #  Bowers, Newton L. Jr, Hans U. Gerber, James C. Hickman, Donald A. Jones, and Cecil J. Nesbitt. 1997. Actuarial Mathematics. Second. Schaumburg, Illinois: The Society of Actuaries.\n Wuthrich, Mario V. 2020. ‚ÄúNon-Life Insurance: Mathematics \u0026amp; Statistics.‚Äù Lecture notes. RiskLab, ETH Zurich; Swiss Finance Institute.\n  "},{"id":2,"href":"/docs/0-subject-guide/SILO/","title":"Learning Outcomes","section":"Subject Guide","content":"Subject Intended Learning Outcomes (SILO) #  To view the subject objectives and the generic skills you will develop through successful completion of this subject, please see the University Handbook.\nACTL30007 SILO #  The ACTL30007 subject intended learning outcomes (SILO) are as follows:\n Apply relevant pre-requisite knowledge of mathematics, probability theory and statistics in the solution of a range of practical problems. Derive and calculate probabilities for, and moments of, loss distributions both with and without simple reinsurance arrangements. Estimate the parameters of a loss distribution when data is complete or incomplete. Fit a statistical distribution to a dataset and perform goodness-of-fit tests. Construct risk models appropriate for short term insurance contracts and derive both moments and moment generating functions for aggregate claim amounts under these models with and without simple forms of proportional and excess of loss reinsurance. Derive recursion formulae and apply approximation methods to calculate aggregate claims distributions. Describe and apply copulas to model dependent risks. Apply extreme value theory in modelling the distribution of severity of loss. Describe and apply the main concepts and properties underlying the analysis of several time series models.  ACTL90020 SILO #  The ACTL90020 subject intended learning outcomes (SILO) are as follows:\n Apply relevant pre-requisite knowledge of mathematics, probability theory and statistics in the solution of a range of practical problems. Derive and calculate probabilities for, and moments of, loss distributions both with and without simple reinsurance arrangements. Estimate the parameters of a loss distribution when data is complete or incomplete. Fit a statistical distribution to a dataset and perform goodness-of-fit tests. Construct risk models appropriate for short term insurance contracts and derive both moments and moment generating functions for aggregate claim amounts under these models with and without simple forms of proportional and excess of loss reinsurance. Derive recursion formulae and apply approximation methods to calculate aggregate claims distributions. Describe and apply copulas to model dependent risks. Introduce extreme value theory and its applications in modelling the distribution of severity of loss. Explain and concepts and general properties of several time series models.  Mapping to the IFoA Actuarial Statistics 2 (CS2) subject #  This subject is one of three subjects covering the contents of the ‚ÄúCS2 ‚Äì Risk Modelling and Survival Analysis Core Principles‚Äù subject of the Institute and Faculty of Actuaries. This counts towards the ‚ÄúFoundation Program‚Äù of the Australian Actuaries Institute professional curriculum and syllabus.\nIn particular, ACTL30007 is meant to cover Items 1 and 2 of the CS2 syllabus. This is detailed in the following table, with mapping towards the course modules (and major reference in parentheses; see ‚ÄúSubject Resources‚Äù below), as well as subject intended learning outcomes (SLO).\nItem 1 ‚Äì Random variables and distributions for risk modelling #     CS2 Subject module (main reference) SILO          1.1 Loss distributions, with and without risk sharing. Module 3 Individual Claim Size Modelling (MW 3), and 1, 2, 3, 4    Module 4* Algorithms and Approximations for compound distributions (MW 4) 6*   1.1.1 Describe the properties of the statistical distributions that are suitable for modelling individual and aggregate losses.     1.1.2 Explain the concepts of excesses (deductibles) and retention limits.     1.1.3 Describe the operation of simple forms of proportional and excess of loss reinsurance.     1.1.4 Derive the distribution and corresponding moments of the claim amounts paid by the insurer and the reinsurer in the presence of excesses (deductibles) and reinsurance.     1.1.5 Estimate the parameters of a failure time or loss distribution when the data is complete, or when it is incomplete, using maximum likelihood and the method of moments.     1.1.6 Fit a statistical distribution to a data set and calculate appropriate goodness-of-fit measures.          1.2 Compound distributions and their applications in risk modelling. Module 2 Collective Risk Modelling (MW 2) 1, 2, 5    Module 4* Algorithms and Approximations for compound distributions (MW 4) 6*   1.2.1 Construct models appropriate for short-term insurance contracts in terms of the numbers of claims and the amounts of individual claims.     1.2.2 Describe the major simplifying assumptions underlying the models in 1.2.1.     1.2.3 Define a compound Poisson distribution and show that the sum of independent random variables, each having a compound Poisson distribution, also has a compound Poisson distribution.     1.2.4 Derive the mean, variance and coefficient of skewness for compound binomial, compound Poisson and compound negative binomial random variables.     1.2.5 Repeat 1.2.4 for both the insurer and the reinsurer after the operation of simple forms of proportional and excess of loss reinsurance.          1.3 Introduction to copulas. Module 5 Copulas (CS2, Unit 3) 1, 7   1.3.1 Describe how a copula can be characterised as a multivariate distribution function that is a function of the marginal distribution functions of its variates, and explain how this allows the marginal distributions to be investigated separately from the dependency between them.     1.3.2 Explain the meaning of the terms ‚Äòdependence or concordance‚Äô, ‚Äòupper and lower tail dependence‚Äô, and state in general terms how tail dependence can be used to help select a copula suitable for modelling particular types of risk.     1.3.3 Describe the form and characteristics of the Gaussian copula and the Archimedean family of copulas.          1.4 Introduction to extreme value theory. Module 6 Extreme Value Theory (CS2, Unit 4) 1, 8   1.4.1 Recognise extreme value distributions, suitable for modelling the distribution of severity of loss and their relationships.     1.4.2 Calculate various measures of tail weight and interpret the results to compare the tail weights.      \\(*\\) Module 4 (SILO 6) covers associated recursive methods and approximations. While materials are provided as part of this subject, they are neither taught nor assessed for lack of time. Note these are not part of the syllabus.\nItem 2 ‚Äì Time series #     CS2 Subject module SILO     2.1 Concepts underlying time series models. Module 7 Characteristics of Time Series (TS 1.1-1.5) 1, 9    Module 8 Time Series Regression and Exploratory Data Analysis (TS 2)     Module 9 Time Series Models (TS 3.1, 3.3, 3.6, 3.9, 5.6)    2.1.1 Explain the concept and general properties of stationary, I(0), and integrated, I(1), univariate time series.     2.1.2 Explain the concept of a stationary random series.     2.1.3 Explain the concept of a filter applied to a stationary random series.     2.1.4 Know the notation for backwards shift operator, backwards difference operator and the concept of roots of the characteristic equation of time series.     2.1.5 Explain the concepts and basic properties of Autoregressive (AR), Moving Average (MA), Autoregressive Moving Average (ARMA) and Autoregressive Integrated Moving Average (ARIMA) time series.     2.1.6 Explain the concept and properties of discrete random walks and random walks with normally distributed increments, both with and without drift.     2.1.7 Explain the basic concept of a multivariate autoregressive model.     2.1.8 Explain the concept of cointegrated time series.     2.1.9 Show that certain univariate time series models have the Markov property and describe how to rearrange a univariate time series model as a multivariate Markov model.     2.2 Applications of time series models. Module 10 Estimation and Forecasting (TS 3.3, 3.5, 3.7) 1, 9   2.2.1 Outline the processes of identification, estimation and diagnosis of a time series, the criteria for choosing between models and the diagnostic tests that may be applied to the residuals of a time series after estimation.     2.2.2 Describe briefly other non-stationary, non-linear time series models.     2.2.3 Describe simple applications of a time series model, including random walk, autoregressive and cointegrated models, as applied to security prices and other economic variables.     2.2.4 Develop deterministic forecasts from time series data, using simple extrapolation and moving-average models, applying smoothing techniques and seasonal adjustment when appropriate.      "},{"id":3,"href":"/docs/0-subject-guide/Activities/","title":"Activities","section":"Subject Guide","content":"Lectures #  Lecture Times #  Lectures will be delivered live, in dual mode (both on Zoom and in person) in the following venue:\n PAR-The Spot-3008, Mondays, from 2.15pm to 4.15pm Recordings will be made available on the Canvas website https://canvas.lms.unimelb.edu.au/courses/123529  Notes:\n The tutorials on Friday 15 April 2022 won\u0026rsquo;t run (Good Friday holiday). Students with a tutorial on 13 April 2022 will review the mid-semester exam questions. A recording will be made available. Details about readings are available below under ‚ÄúSubject Resources‚Äù. They can be downloaded from the Canvas ‚ÄúReadings Online‚Äù.  Note this subject is one-quarter of a standard load. That means, including attendance at classes, you should be putting the equivalent of one quarter of a week‚Äôs work into this subject, i.e. 9 to 10 hours.\nLecture Slides and Materials #  All materials on the Canvas Websites https://canvas.lms.unimelb.edu.au/courses/145406 [that is the community website] or https://canvas.lms.unimelb.edu.au/courses/123529 [that is the ACTL30007 website] or https://canvas.lms.unimelb.edu.au/courses/123552 [that is the ACTL90020 website]\nThat being said, the best way to access files is from this website. The most salient exception are lecture recordings, with annotated slides, which are cohort specific, and will be available from \u0026ldquo;Lecture Recordings\u0026rdquo;.\nMaterials tagged with a `\\(\\maltese\\)` sign are out-of-scope. They are included for your interest, but they won't be assessed. Exercises are also provided for students who are interested in learning those materials. They are however, out-of-scope for this subject in 2022. Recorded Lectures #  Audio and video recordings of lectures delivered in this subject will be made available for review. These recordings allow you to revise lectures during the semester, or to review them in preparation for the end of semester exam.\nYou can access recorded lectures by clicking on the Lecture Recordings (or similar) menu item on the LMS page for this subject.\nPlease note that for live classes, recordings are not a substitute for attendance; rather they are designed for revision. On rare occasions the recordings can fail to take place due to technical reasons. In such cases, a substitute recording will be made available.\nTutorials #  Tutorial Times #  Tutorials start in week 1. Please check your timetable and the University timetable https://sws.unimelb.edu.au/2022/ for details. You must attend the tutorials you are enrolled in.\nFor UG students, tutorial ACTL30007/U/1/SM1/T01/05 on Wednesday at 5:15pm will be recorded and made available to the whole class at the end of the week.\nFor PG students, the Friday tutorial will be recorded and made available as usual.\nTutorial Details and Materials #  The tutorial schedule is provided in the plan above.\nStudents are requested to complete the \u0026ldquo;pre-tutorial questions\u0026rdquo; prior to their tutorial (note solutions are readily available). While tutors might answer questions about those at the tutorial in Q\u0026amp;A fashion, the contents to be covered during tutorials are the ‚Äútutorial questions‚Äù, which are more advanced and/or better suited for discussions. Solutions to the tutorial questions are not released prior to the tutorials.\nPrivate Tutoring Services #  The Faculty has become increasingly concerned about the existence of a number of private tutoring services operating in Melbourne that heavily target University of Melbourne students enrolled in FBE subjects.\nStudents are urged to show caution and exercise their judgement if they are considering using any of these services, and to please take note of the following:\n Any claim by any of these businesses that they have a \u0026ldquo;special\u0026rdquo; or \u0026ldquo;collaborative\u0026rdquo; or \u0026ldquo;partnership\u0026rdquo; style relationship with the University or Faculty is false and misleading. Any claim by a private tutoring service that they are in possession of, or can supply you with, forthcoming University exam or assignment questions or \u0026ldquo;insider\u0026rdquo; or \u0026ldquo;exclusive\u0026rdquo; information is also false and misleading. The University has no relationship whatsoever with any of these services and takes these claims very seriously as they threaten to damage the University‚Äôs reputation and undermine its independence.  It is also not appropriate for students to provide course materials (including University curricula, reading materials, exam and assignment questions and answers) to operators of these businesses for the purposes of allowing them to conduct commercial tutoring activities. Doing so may amount to misconduct and will be taken seriously. Those materials contain intellectual property owned or controlled by the University.\nWe encourage you to bring to the attention of Faculty staff any behaviour or activity that is not aligned with University expectations or policy as outlined above.\n"},{"id":4,"href":"/docs/0-subject-guide/Resources/","title":"Resources","section":"Subject Guide","content":"Website #  The current website contains (or links) to all the resources you need for the course (compare the following list with left hand side menu):\n  Subject guide (you are reading it!)  Study Plan: this will outline our recommended study plan for the course, week by week. This will include a link to the video lectures / recordings, too.  Claims modelling: the contents covering Section 1 of the CS2 syllabus.  Time series: the contents covering Section 2 of the CS2 syllabus.  Ed discussion forum: link to the main interaction fora  Pre-Tutorial exercises: to do before tutorials  Tutorial exercises: to be discussed in tutorials. Solutions will be posted at the end of the relevant week.  Tutorial exercises: additional and out-of-scope exercises.  Details about assessment will be available under Subject Guide / Assessment\nPrescribed readings #  Prescribed references are:\n [MW] For the first component of the subject, selected parts of: W√ºthrich, Mario V., Non-Life Insurance: Mathematics \u0026amp; Statistics (December 17, 2020). Available at SSRN: https://ssrn.com/abstract=2319328 [TS] For second component of the subject, selected parts of: Shumway, Robert H., Stoffer, David S. (2017) Time Series Analysis and Its Applications With R Examples, Springer, ISBN 978-3-319-52452-8 [CS2] Institute and Faculty of Actuaries, CS2 Core Reading, Unit 3 Copulas and Unit 4 Extreme Value Theory  All those references can be downloaded from the Readings Online section of the Canvas community website.\nYou do not need to purchase any textbook for this course; all resources are available from the library without additional charge.\n"},{"id":5,"href":"/docs/0-subject-guide/Assessment/","title":"Assessment","section":"Subject Guide","content":"Assessment Overview #  Assessment structure #  Your assessment for this subject comprises the following:\n  a 50-in-75-minute mid-semester exam (15%)  individual assignment (25%)  a 2-in-3 hour final exam (60%)  Hurdle requirement: To pass this subject students must pass the end of semester examination.\nIn the following sections, details are provided (and will be populated throughout semester) about those assessments.\nExaminations:\nCanvas Quiz will automatically submit the exam when the time limit (comprised of reading time \u0026amp; writing time) has been reached (for example, after 60 minutes for an exam with 30 minutes of writing time and 30 minutes of reading time). Students must click ‚Äòsubmit‚Äô at the completion time if they did not start the exam at the scheduled commencement time to not incur any late penalty.\nFor example, a 60-minute exam scheduled at 3:00pm, but commenced at 3:15pm, will automatically submit at 4:15pm and the student will incur a late penalty. Students must click \u0026lsquo;submit\u0026rsquo; at 4:00pm to not incur a late penalty. Students who are prevented from submitting on time or at all due to technical difficulties will need to apply for technical consideration with supporting documentation.\nMid-semester exam #  Date #  The mid-semester exam will take place online on Monday 11 April 2022 between 2.30pm and 3.45pm.\nDuration #  Important online mid-semester examination information: This mid-semester examination is the equivalent of a 50-minute assessment; however, students are provided with 75-minutes to accommodate the time that will be required for typesetting of mathematical expressions, loading, processing and modelling of data with the statistical software R, and the uploading of documents / answers.\nScope #  The mid-semester exam will assess contents from Modules 2 and 3 (lectures of weeks 1-3) and associated tutorial exercises, readings, R codes, and course contents. This focuses on the claims modelling component of the subject.\nExaminable materials include associated tutorial questions, revision questions, additional questions, technical and practice lectures with slides and associated mandatory readings.\nPractice #  There will be two quizzes on Canvas to help you practice:\nAdditional details #  See Exam Format and General Advice for further details.\nIndividual assignment #  Assignment instructions #  The assignment will be a data analysis assignment which will need to be performed in R, and you will need to submit a 5 minute oral presentation, slides, and R code.\nThe assignment will assess skills from Modules 7 to 9 (lectures of weeks 4-6) and associated tutorial exercises, readings, R codes, and course contents. This focuses on the time series component of the subject. Communication skills will also be assessed\nAdditional details and instructions will be available later (around mid-semester exam time).\nAssignment submission #  Assignment submission is via the LMS Assignment Submission link for all written assignments. Please refer to the Turnitin section of the LMS website via for detailed submission instructions if needed.\nPlease note that you are required to keep a copy of your assignment after it has been submitted as you must be able to produce a copy of your assignment at the request of teaching staff at any time after the submission due date.\nPenalties for Late Submission #  In order to ensure equality for all students, assignments and examinations (where relevant) must be submitted by specified deadlines. Late assignments, where approval for late submission has not been given, will be penalised at the rate of 20% of the total mark per day, for up to 5 days, at which time a mark of zero will be given.\nStudents with a genuine and acceptable reason for not completing an assignment (or other assessment task), such as illness, can apply for special consideration. Special Consideration assists students who have been significantly affected by illness or other serious circumstances during the semester. The following website contains detailed information relating to who can apply for Special Consideration and the process for making an application: http://students.unimelb.edu.au/admin/special\nFinal exam #  Date #  The date of the final exam is published on the University of Melbourne Exams Timetable website.\nAt your scheduled date, the final exam will be available on the Canvas website under \u0026quot;Assignments\u0026quot;.\nDuration #  Important online examination information: This examination is the equivalent of a 2-hour assessment; however, students are provided with 3-hours to accommodate the time that will be required for typesetting of mathematical expressions, loading, processing and modelling of data with the statistical software R, and the uploading of documents / answers.\nCanvas Quiz will automatically submit the exam when the time limit (comprised of reading time \u0026amp; writing time) has been reached (for example, after 60 minutes for an exam with 30 minutes of writing time and 30 minutes of reading time). Students must click ‚Äòsubmit‚Äô at the completion time if they did not start the exam at the scheduled commencement time to not incur any late penalty.\nFor example, a 60-minute exam scheduled at 3:00pm, but commenced at 3:15pm, will automatically submit at 4:15pm and the student will incur a late penalty. Students must click \u0026lsquo;submit\u0026rsquo; at 4:00pm to not incur a late penalty. Students who are prevented from submitting on time or at all due to technical difficulties will need to apply for technical consideration with supporting documentation.\nScope #  The final exam will assess all contents of the course, with particular focus on the materials taught beyond week 6. This focuses on both the claims modelling and time series components of the subject.\nExaminable materials include associated tutorial questions, revision questions, additional questions, technical and practice lectures with slides and associated mandatory readings, as well as the assignment task and the mid-semester exam.\nMarks and allocation to topics #  Will be available later.\nExam consultation #  Will be announced later.\nQuestions and issues during the exam #  All exams will be moderated. Please log your concern via the Canvas chat board in the first instance using the Big Blue Button.\nAlternatively, you can call the following numbers for assistance during the exam if you are experiencing technical difficulties. Inside Australia: 13MELB (13 6352) (select Option 1 for current students then select Option 1 again for exam enquiries). Outside Australia: +61 3 9035 5511 (select Option 1 for current students and then select Option 1 again for exam enquiries).\nAdditional details #  See Exam Format and General Advice for further details.\nSee also the University of Melbourne exam webpage for students.\nExam format #  The mid-semester exam and the final exams will be Canvas quizzes. Questions will be randomised, such that there are millions of possible combinations of questions, even before answers are shuffled (and they will be when relevant).\nThese are Open Book exams, which means that you will not be prevented from using notes or texts during the exam. However, interaction with other students is not permitted, and is a breach of the University‚Äôs Academic Integrity Policy. Any evidence of collusion will be investigated as potential academic misconduct. The Academic Integrity policy can be found at: https://academicintegrity.unimelb.edu.au/\nIt is recommended that you use Google Chrome to access your exam on Canvas. Firefox has also been shown to be reliable.\nExam conditions #  It is important for students to be in a private, quiet and well-lit room. Please let us emphasize that you should not discuss exam matters with others and that you should only submit work of your own.\nThe Academic Integrity policy can be found at: https://academicintegrity.unimelb.edu.au/\nTypes of quiz questions #  There will be three types of questions:\n Multiple Choice Question (MCQ) with exactly one correct answer. These are similar to those used in the mid-semester exam. Students get full mark for a correct answer, and 0 marks for an incorrect answer.   Example: What is the result of 2 times 4?  4 6 8 10   In this example there is only one correct answer ‚Äì 3.  Multiple Answers Questions (MAQ) In this case, there are multiple choices, but the number of correct answers can be different from 1 (it can be anything from 0 to the number of possible choices). Here is the explanation of Canvas about how marks are awarded: ‚ÄúTo calculate scores for Multiple Answers quiz questions, Canvas divides the total points possible by the amount of correct answers for that question. This amount is awarded for every correct answer selected and deducted for every incorrect answer selected. No points are awarded or deducted for correct or incorrect answers that are not selected. For example, an instructor may create a Multiple Answer quiz question with 9 points possible that includes three correct choices and two incorrect choices. If a student selects two correct answers and one incorrect answer, they would be awarded 3 total points for that question. This would be calculated by awarding 3 points (9 total points divided by 3 correct answers) for each correct answer and subtracting 3 points for the incorrect answer.‚Äù Furthermore, there is a minimum score of 0 (you can‚Äôt get a negative score).   Example: Which of the following are negative?  4 -6 -8 10   In this example there are two correct answers: 2. and 3.. If there are 4 marks allocated to this question, marks would be awarded as follows:   and 3.: 2+2=4 marks    only: 2 marks    and 4.: 2-2=0 marks     and 2. and 4.: -2+2-2=-2 but there is a minimum score of 0, so 0 marks.     With this type of question you must carefully assess every single answer independently of the others.  Essay Questions In this case, you must answer with text, and you get a text field to compose your answer. Note that formulas can be composed too. You may also upload a handwritten answer if you wish, although we recommend not to do this unless formulas are required extensively. These questions will be marked by someone as they cannot be automatically graded.  Precision of numerical answers #  Students often ask about the required significant figures / decimal places.\nIf it is not specified:\n In an open question we will look at the reasoning so you should not be too worried about it (within reason). Use, say, 5 significant figures/digits \\(*\\) Otherwise the precision needs to match that of the possible answers (if we have 5 significant figures ( \\(*\\) ) in the answers then work with 6, say).  \\(*\\) Please move away from talking about decimal places - this does not take the scale of your problem into account. The correct way to think about this is in terms of significant figures/digits. For more details, see e.g. Wikipedia or this video from Khan academy.  Exam preparation #  To prepare for this exam we suggest you review the tutorial exercises and past exam questions. You should also make sure you read all prescribed readings and have understood the main arguments. Remember that this is an open book exam, so there is nothing to memorise. You must demonstrate understanding instead, and being able to \u0026ldquo;connect the dots\u0026rdquo;. If you have done the work and know where the things are, you should be able to easily find the details you need to answer the questions.\nMore specifically:\n Finalise your own summaries and formula sheets in preparation for the exam. I recommend you review carefully the Detailed Learning Outcomes pages:  For Actuarial Practice: are you able to do all these? (describe, list, explain, \u0026hellip;); if you are on top of those you should be well prepared for the final exam; For Actuarial Techniques: do you understand all those formulas? Do you have formulas to add? Is there anything to add to your own summaries?   Of course, review also all tutorial exercises. For each question:  What skill(s) is this exercise testing? Where are the required concepts in your summaries? in your formula sheets? What did you find challenging in this exercise, and how comfortable are you with this now?    General Advice #  Tips #  Remember there are different ways of answering a multiple choice/answer question (especially if it involves numbers):\n Work out the answer as if you were in a standard pen and paper invigilated exam, then see if the answer is in the list. Work out the answer with tools that would not otherwise be available, such as Excel spreadsheet, R, or even the internet. (but be careful not to collude or otherwise be guilty of academic misconduct ‚Äì check https://academicintegrity.unimelb.edu.au/ for further details) In some circumstances, it may be easier to simply ‚Äúplug back‚Äù the different answers to see which one works. Don‚Äôt forget these three options exist for multiple choice/answer open book exams! Hence make sure you have Excel and R available and working before the exam starts.  Furthermore, some more advice / exam technique / tips:\n Read attentively the exam questions at least twice before attempting the question. Work out what skills you are meant to demonstrate. Make sure you understand what is being asked in the question ‚Äì consider key words indicating what you have to do. When you have finished, re-read the question to make sure you have not missed anything. Key is to demonstrate understanding. Attempt all questions where possible. If you run out of time, explain how to you would approach the task (where possible). If this is right this should give you marks. Do questions you are most confident with first. Prioritise. Observe the number of marks allocated. Manage time accordingly. Reassess your available time regularly (say, half way through for a 1 hour exam, and perhaps 2 times for a 2 hour exam). For instance, if the exam is one hour and there are 100 marks, then you have 36 seconds per mark.  Pay close attention to the wording of the question - this is not chosen randomly. Pay particular attention to the verb (for instance: \u0026ldquo;State\u0026rdquo; and \u0026ldquo;Show\u0026rdquo; do not mean the same thing. With \u0026ldquo;State\u0026rdquo; you just need to give the answer, with \u0026ldquo;Show\u0026rdquo; you need to actually prove the answer) and adverb if any (for instance: \u0026ldquo;briefly\u0026rdquo; is not the same as \u0026ldquo;in detail\u0026rdquo;). Examples are:\n ‚Äò\u0026hellip;Justify your answer‚Äô ‚ÄòFind‚Äô ‚ÄòExplain in words‚Äô ‚ÄòDerive‚Äô ‚ÄòShow that‚Äô ‚ÄòBriefly list‚Äô ‚ÄòPresent in detail\u0026hellip;‚Äô ‚ÄòBriefly explain‚Äô ‚ÄòBriefly describe‚Äô ‚ÄòState (without justification) 4 examples of \u0026hellip;‚Äô  "},{"id":6,"href":"/docs/0-subject-guide/","title":"Subject Guide","section":"Docs","content":"Subject guide information\n\nIn this section we outline the main elements of the full subject guide UG, PG, which you should read as it includes additional relevant information (such as for special consideration) which is not included here but might be relevant to you, and which we will assume you are aware of.\nNote that consultation hours with Professor Avanzi will be:\n [PG] during semester on Fridays, 12:00-13:00 [UG] during semester on Fridays, 13:15-14:15  Consultation will not be recorded. Note that there won‚Äôt be consultation during the non-teaching week, and exam consultation will be advertised later.\nStudents should ask all course content questions in Ed, during tutorials, or during consultation hours. E-mail queries should be restricted to those of an administrative and personal nature. Any query that could be of relevance to other students will only be answered during face-to-face tutorial and consultation time, or in Ed.\n"},{"id":7,"href":"/docs/1-claims-modelling/m2-collective-risk-modelling/","title":"M2 Collective Risk Modelling","section":"Claims Modelling (CS2 Section 1)","content":"Introduction: Models for aggregate losses #  A portfolio of contracts or a contract will potentially experience a sequence of losses: $$Y_1, Y_2, Y_3, \\ldots$$ We are interested in the aggregate sum \\(S\\) of these losses over a certain period of time.\n How many losses will occur?  if deterministic \\((n)\\) \\(\\longrightarrow\\) individual risk model if random \\((N)\\) \\(\\longrightarrow\\) collective risk model   How do they relate to each other?  usual assumption: iid   When do these losses occur?  usual assumption: no time value of money\n\\(\\longrightarrow\\) short term models   How big are these losses?  The Individual Risk Model #  Definition #  The Individual Risk Model #  In the Individual Risk Model $$S=Y_{1}+\\cdots +Y_{n}=\\sum_{i=1}^{n}Y_{i},$$ where \\(Y_{i}\\), \\(i=1,2,...,n\\), are iid claims. There are several methods to get probabilities about \\(S\\):\n get the whole distribution of \\(S\\) (if possible)  Convolutions Generating functions   \\(\\adv\\) approximate with the help of the moments of \\(S\\) (Module 4)  Convolutions of random variables #  In probability, the operation of determining the distribution of the sum of two random variables is called a convolution. It is denoted by $$F_{X+Y}=F_X*F_Y.$$ The result can then be convolved with the distribution of another random variable. For instance, $$F_{X+Y+Z}=F_Z*F_{X+Y}.$$ This can be done for both discrete and continuous random variables. It is also possible for mixed rv‚Äôs, but it is more complicated.\nFormulas #  In short\n Discrete case: df: \\(F_{X+Y}\\left( s\\right) = \\sum_{x}F_{Y}\\left( s-x\\right)f_{X}\\left( x\\right)\\) pmf: \\(f_{X+Y}\\left( s\\right) = \\sum_{x}f_{Y}\\left(s-x\\right) f_{X}\\left( x\\right)\\) Continuous case: cdf: \\(F_{X+Y}\\left( s\\right) = \\int_{-\\infty }^{s}F_{Y}\\left( s-x\\right) f_{X}\\left( x\\right) dx\\) pdf: \\(f_{X+Y}\\left( s\\right) = \\int_{-\\infty }^{s}f_{Y}\\left( s-x\\right) f_{X}\\left( x\\right) dx\\)  Examples:\n discrete case: Bowers et al. (1997) Example 2.3.1 on page 35 continuous case: Bowers et al. (1997) Example 2.3.2 on page 36  Numerical example #  Consider 3 discrete r.v.‚Äôs with probability mass functions\n$$\\begin{array}{rcll} f_{1} \\left( y \\right) \u0026amp; = \u0026amp; \\frac{1}{4}, \\frac{1}{2},\\frac{1}{4} \u0026amp; \\text{ for } y=0,1,2 \\\\ f_{2} \\left( y \\right) \u0026amp;=\u0026amp; \\frac{1}{2}, \\frac{1}{2} \u0026amp; \\text{ for } y=0,2 \\\\ f_{3} \\left( y \\right) \u0026amp;=\u0026amp; \\frac{1}{4}, \\frac{1}{2},\\frac{1}{4} \u0026amp; \\text{ for } y=0,2,4 \\end{array}$$\nCalculate the pmf \\(f_{1+2+3}\\) and the df \\(F_{1+2+3}\\) of the sum of the three random variables.\nSolution #     \\(y\\) \\(f_{1}\\left( y\\right)\\) \\(f_{2}\\left( y\\right)\\) \\(f_{1+2}\\left(y\\right)\\) \\(f_{3}\\left( y\\right)\\) \\(f_{1+2+3}\\left( y\\right)\\) \\(F_{1+2+3}\\left( y\\right)\\)     \\(0\\) \\(1/4\\) \\(1/2\\) \\(1/8\\) \\(1/4\\) \\(1/32\\) \\(1/32\\)   \\(1\\) \\(1/2\\) \\(0\\) \\(2/8\\) \\(0\\) \\(2/32\\) \\(3/32\\)   \\(2\\) \\(1/4\\) \\(1/2\\) \\(2/8\\) \\(1/2\\) \\(4/32\\) \\(7/32\\)   \\(3\\) \\(0\\) \\(0\\) \\(2/8\\) \\(0\\) \\(6/32\\) \\(13/32\\)   \\(4\\) \\(0\\) \\(0\\) \\(1/8\\) \\(1/4\\) \\(6/32\\) \\(19/32\\)   \\(5\\) \\(0\\) \\(0\\) \\(0\\) \\(0\\) \\(6/32\\) \\(25/32\\)   \\(6\\) \\(0\\) \\(0\\) \\(0\\) \\(0\\) \\(4/32\\) \\(29/32\\)   \\(7\\) \\(0\\) \\(0\\) \\(0\\) \\(0\\) \\(2/32\\) \\(31/32\\)   \\(8\\) \\(0\\) \\(0\\) \\(0\\) \\(0\\) \\(1/32\\) \\(32/32\\)    $$\\begin{array}{rcl} f_{1+2}(2) \u0026amp;=\u0026amp; 1/4 \\cdot 1/2+1/2 \\cdot 0+1/4 \\cdot 1/2 \\\\ f_{1+2+3}(4) \u0026amp;=\u0026amp; 1/8 \\cdot 1/4+2/8 \\cdot 0+2/8 \\cdot 1/2+2/8 \\cdot 0+1/8 \\cdot 1/4 \\end{array}$$\nUsing generating functions #  There is a 1-1 relation between a distribution and its mgf or pgf.\nBecause $$M_S(t)=E\\left[e^{tS}\\right]=E\\left[e^{t(Y_1+\\ldots+Y_n)}\\right]=E\\left[e^{tY_1}\\cdots e^{tY_n}\\right]$$ and if losses are independent then we have $$M_S(t)=E\\left[e^{tS}\\right]=E\\left[e^{tY_1}\\right]\\cdots E\\left[e^{tY_n}\\right]=M_{Y_1}(t)\\cdots M_{Y_n}(t).$$ The same argument holds for the pgf‚Äôs.\n Sometimes, \\(M_S(t)\\) or \\(p_S(t)\\) can be recognised: this is the case for infinitely divisible distributions (Normal, Poisson, Inverse Gaussian, ) and certain other distributions (Binomial, Negative binomial). Otherwise, \\(M_S(t)\\) or \\(p_S(t)\\) can be expanded numerically to get moments and/or probabilities.  Example #  Consider a portfolio of 10 contracts. The losses \\(Y_i\\)‚Äôs for these contracts are iid rv‚Äôs with mean 100 and variance 100. Determine the distribution, the expected value and the variance of \\(S\\) if these losses are\n Normal; Gamma; Poisson.  Using R #   Contrary to Excel, convolutions are extremely easy to implement in R using vectors.  f1 \u0026lt;- c(1/4, 1/2, 1/4, 0, 0) f2 \u0026lt;- c(1/2, 0, 1/2, 0, 0) f12 \u0026lt;- c(f1[1] * f2[1], sum(f1[1:2] * f2[2:1]), sum(f1[1:3] * f2[3:1]), sum(f1[1:4] * f2[4:1]), sum(f1[1:5] * f2[5:1])) f12 ## [1] 0.125 0.250 0.250 0.250 0.125   The example above is generalised in Exercise los9R8. A more advanced R function is convolve. It actually involves the Fast Fourier Transform (a method that is related to that of the mgf‚Äôs) for efficiency. We do not discuss this here, but it is used in the implementation of convolutions in the function aggregateDist of the package actuar (introduced later).  The Collective Risk Model (Compound distributions, MW 2.1) #  Definition #  Introduction #  Two models, depending on the assumption on the number of losses:\n deterministic - \\(n\\)  main focus on the claims of individual policies (whose number is a priori known) \\(\\longrightarrow\\) Individual Risk Model discussed in previous sections   random - \\(N\\)  main focus on claims of a whole portfolio (whose number is a priori unknown) \\(\\longrightarrow\\) Collective Risk Model this is another way of separating frequency and severity    In this section we focus on the Collective Risk Model.\nDefinition #  In the Collective Risk Model, aggregate losses become $$S=Y_{1}+\\ldots +Y_{N}=\\sum_{i=1}^{N}Y_{i}.$$ This is a random sum. We make the following assumptions:\n \\(N\\) is the number of claims \\(Y_i\\) is the amount of the \\(i\\)th claim the \\(Y_i\\)‚Äôs are iid with  (c)df \\(G(y)\\) p(d/m)f \\(g(y)\\)   the \\(Y_i\\)‚Äôs and \\(N\\) are mutually independent  Moments of \\(S\\) #  We have $$E[S]=E\\left[ E[S|N] \\right] = E\\left[ N E[Y] \\right]= E[N]E[Y],$$ and\n$$\\begin{array}{rcl} Var(S) \u0026amp; = \u0026amp; E\\left[ Var(S|N) \\right] + Var\\left( E[S|N] \\right) \\\\ \u0026amp; = \u0026amp; E\\left[ N Var(Y) \\right] + Var(E[Y] N) \\\\ \u0026amp; = \u0026amp; E[N] Var(Y) + E[Y]^2 Var(N) \\\\ \u0026amp; = \u0026amp; E[N] (E[Y^2] - E[Y]^2)+ E[Y]^2 Var(N) \\\\ \u0026amp; = \u0026amp; E[N]E[Y^2] + E[Y]^2 \\left( Var(N)-E[N] \\right). \\end{array}$$\nMoment generating function of \\(S\\) #  It is possible to get \\(M_S(t)\\) as a function of \\(M_Y(t)\\) and \\(M_N(t)\\):\n$$\\begin{array}{rcl} M_S(t) \u0026amp;=\u0026amp; E\\left[e^{tS}\\right]= E\\left[ E\\left[ \\left.e^{t(Y_1+Y_2+\\ldots+Y_N)}\\right|N \\right]\\right] \\\\ \u0026amp;=\u0026amp; E\\left[ M_Y(t)^N\\right]=E\\left[ e^{N \\ln M_Y(t)} \\right] \\\\ \u0026amp;=\u0026amp; M_N\\left(\\ln M_Y(t)\\right) \\end{array}$$\nExample (Bowers et al. (1997), 12.2.1) #  Assume that \\(N\\) is geometric with probability of success \\(p\\): $$\\Pr[N=n]=pq^n, \\quad n=0,1,\\ldots,$$ where \\(0\u0026lt;q\u0026lt;1\\) and \\(p=1-q\\). We have then $$M_N(t)=E[e^{tN}]=\\sum_{n=0}^\\infty pq^ne^{tn}=\\frac{p}{1-qe^t},$$ and thus $$M_S(t)=M_N\\left(\\ln M_Y(t)\\right)=\\frac{p}{1-qe^{\\ln M_Y(t)}}=\\frac{p}{1-qM_Y(t)}.$$\nDistribution of \\(S\\) #  It is possible to get a fairly general expression for the df of \\(S\\) by conditioning on the number of claims:\nwhere \\(G^{*n}(y)\\) is the \\(n\\)-th convolution of \\(G\\).\nNote that\n \\(N\\) will always be discrete, so this works for any type of rv \\(Y\\). (continuous, discrete or mixed) However, the type of \\(S\\) will depend on the type of \\(Y\\).  Distribution of \\(S\\) if \\(X\\) is continuous #  If \\(X\\) is continuous, \\(S\\) will generally be mixed:\n with a mass at 0 because of \\(\\Pr[N=0]\\) (if positive) continuous elsewhere, but with a density integrating to \\(1-\\Pr[N=0]\\)  Example, continued (Bowers et al. (1997), 12.2.3) #  Assume now that $$G(y)=1-e^{-y}\\;\\text{ and hence }\\;M_Y(t)=\\frac1{1-t} \\text{ for } t\u0026lt;1.$$ Now, we have that (remember \\(\\Pr[N=0]=p\\)) $$ M_S(t)=\\frac{p}{1-qm_Y(t)}.$$ It follows that $$ M_S(t)=\\frac{p}{1-q\\frac1{1-t}}=p+q\\frac{p}{p-t}=pE\\left[e^{t\\cdot 0}\\right]+(1-p)E\\left[e^{t Z}\\right],$$ where \\(Z\\) is an exponential rv with parameter \\(p\\). Therefore, $$f_S(s)=\\begin{cases} p=\\Pr[N=0] \\mbox{ (probability mass)}\u0026amp;s=0;\\\\ (1-p)(pe^{-ps}) \\mbox{ (probability density)}\u0026amp;s\u0026gt;0. \\end{cases}$$\nDistribution of \\(S\\) if \\(Y\\) is mixed #  If \\(Y\\) is mixed, \\(S\\) will generally be mixed:\n with a mass at 0 because of \\(\\Pr[N=0]\\) and \\(\\Pr[Y=0]\\) (if positive) mixed (if \\(Y\\) is not continuous for \\(x\u0026gt;0\\)) or continuous elsewhere with a density integrating to something \\(\\le 1-\\Pr[N=0]\\)  Distribution of \\(S\\) if \\(Y\\) is discrete #  For discrete \\(Y\\)‚Äôs we can get a similar expression to for the pmf of \\(S\\):\nwhere \\(g^{*0}(0)=1\\) (and thus 0 anywhere else).\n This can be implemented in a table and/or in a program. However, if the range of \\(N\\) goes really to infinity, calculating \\(f_S(s)\\) may require an infinity of convolutions of \\(Y\\). This formula is more efficient if the number of possible outcomes for \\(N\\) is small. \\(\\adv\\) The pmf \\(g^{*n}(s)\\) can be calculated using de Pril‚Äôs algorithm. (see Module 4)  Example with tabular approach #  From Bowers et al. (1997), 12.2.2:\n The convolutions are in done the usual way. The number of columns depends on the range of \\(N\\). The \\(f_S(x)\\) are the sumproduct of the row \\(x\\) and row \\(\\Pr[N=n]\\):  $$f_S(3)=0 \\cdot 0.1 + 0.1\\cdot0.3 + 0.4 \\cdot 0.4 + 0.125 \\cdot 0.2.$$\nUsing R #  We will make extensive use of the function aggregateDist from the package actuar (Dutang, Goulet, and Pigeon 2008):\n This function allows for several different aggregate distribution approaches, which will be introduced here ($$\\maltese$$ and in Module 4 as the associated theory is presented). Here, we show how the function can be used to implement formulas and (using the function convolve in the background). This corresponds to the method=\u0026quot;convolution\u0026quot; approach.   actuar::aggregateDist(method=\u0026quot;convolution\u0026quot;):\n A discrete distribution is required. Note that discretisation methods are discussed in Module 4. This is input as a vector of claim amount probability masses after the argument model.sev=. The first element must be \\(\\Pr[Y=0]\\). There is no restriction on the shape of the frequency distribution, but it must have a finite range. This is input as a vector of claim number probability masses after the argument model.freq=. The first element must be \\(\\Pr[N=0]\\). The outcome of the function is . Additional outputs:  plot: to get a pretty plot of the df summary: to get summary statistics mean: to get the mean diff: to get the pmf   Additional options are:  x.scale: value of an amount 1 in the severity model (this allows calculations on multiples of $1)     # Bowers 12.2.2 fy \u0026lt;- c(0, 0.5, 0.4, 0.1) fn \u0026lt;- c(0.1, 0.3, 0.4, 0.2) Fs \u0026lt;- aggregateDist(\u0026#34;convolution\u0026#34;, model.freq = fn, model.sev = fy) mean(Fs) ## [1] 2.72 pmf \u0026lt;- c(Fs(0), diff(Fs(0:9))) cbind(s = c(0:9), fs = pmf, Fs = Fs(0:9)) ## s fs Fs ## [1,] 0 0.1000 0.1000 ## [2,] 1 0.1500 0.2500 ## [3,] 2 0.2200 0.4700 ## [4,] 3 0.2150 0.6850 ## [5,] 4 0.1640 0.8490 ## [6,] 5 0.0950 0.9440 ## [7,] 6 0.0408 0.9848 ## [8,] 7 0.0126 0.9974 ## [9,] 8 0.0024 0.9998 ## [10,] 9 0.0002 1.0000  summary(Fs) ## Aggregate Claim Amount Empirical CDF: ## Min. 1st Qu. Median Mean 3rd Qu. Max.  ## 0.00 2.00 3.00 2.72 4.00 9.00 plot(Fs) Explicit claims count distributions (MW 2.2) #  Introduction #  Exposure #   It makes no sense to talk about frequency in an insurance portfolio without considering exposure. Chapter 4 of Werner and Modlin (2010) defines exposure as ‚Äúthe basic unit that measures a policy‚Äôs exposure to loss.‚Äù One primary criterion for choosing an exposure base is that it ‚Äúshould be directly proportional to expected loss.‚Äù Here we are focussing on frequency, so exposure should be something directly proportional to the expected frequency. Wuthrich (2020) calls exposure ‚Äúvolume,‚Äù denoted \\(v\\), and defines the claims frequency as $$ \\frac{N}{v}.$$  Basic models for claims frequency #   In our case, we will assume that it directly affects the likelihood of a claim to occur - the frequency - such that \\(N/v\\) is normalised MW defines $$p_k = \\Pr[N=k], \\quad \\text{for }k \\in \\mathcal{A} \\subset \\mathbb{N}_0.$$ There are three main assumptions for \\(p_k\\):  binomial (with variance less than mean) Poisson (with variance equal to the mean) negative-binomial (a Poisson with random mean, so that variance is more than the mean)   A summary table of those distributions is also given in Bowers et al. (1997), see Table 12.3.1 on page 376. \\(\\adv\\) These all belong to a class of distributions called \\((a,b)\\)  Var(N)\\)` -- Var(N)=mp(1-p)\\)` -- Binomial distribution #   fixed volume \\(v \\in \\mathbb{N}\\) fixed default probability \\(p \\in (0, 1)\\) (expected claims frequency) pmf of \\(N\\sim\\text{Binom}(v, p)\\) is $$p_k =\\Pr[N=k]={v \\choose k}p^k(1-p)^{v-k}, \\quad \\text{for all } k \\in \\{0,\\ldots,v\\} = \\mathcal{A}.$$ same as a sum of Bernoulli (which is the case \\(v=1\\)) makes sense for homogenous portfolio with unique possible events, such as credit defaults, or deaths in a life insurance model In R: dbinom, pbinom, qbinom, rbinom, where size is \\(v\\), and where prob is \\(p\\) Note that \\({\\binom v k}\\) can be computed with the R function choose.  Compound binomial model #  The total claim amount \\(S\\) has a compound binomial distribution $$S \\sim \\text{CompBinom}(v,p,G)$$ if \\(S\\) has a compound distribution with \\(N \\sim \\text{Binom}(v,p)\\) for given \\(v\\in \\mathbb{N}\\) and \\(p\\in (0,1)\\) and individual claim size distribution \\(G\\).\nCorollary 2.7: Assume \\(S_1,\\ldots,S_n\\) are independent with \\(S_j \\sim \\text{CompBinom}(v_j,p,G)\\) for all \\(j=1,\\ldots,n\\). The aggregated claim has a compound binomial distribution with $$S=\\sum_{j=1}^n S_j \\sim \\text{CompBinom}\\left( \\sum_{j=1}^n v_j, p,G \\right).$$ Exercise NLI3 considers the decomposition of \\(S\\) into small and large claims. It shows that \\(S_{\\text{lc}}\\)‚Äîthe sum of those claims exceeding a certain threshold \\(M\\) only‚Äîis compound binomial again.\nPoisson distribution #    fixed volume \\(v \u0026gt; 0\\)\n  expected claims frequency \\(\\lambda \u0026gt;0\\)\n  pmf of \\(N\\sim\\text{Poi}(\\lambda v)\\) is $$p_k =\\Pr[N=k]=e^{-\\lambda v}\\frac{(\\lambda v)^k}{k!}\\quad \\text{for all }k\\in \\mathcal{A}=\\mathbb{N}_0.$$\n  Lemma 2.9: increase volume while keeping \\(E[N]\\) fixed in a binomial model leads to a Poisson distribution (more so for small \\(p\\) compared to \\(v\\)).\n  In R: dpois, ppois, qpois, rpois, where lambda is \\(\\lambda v\\)\n  Compound Poisson model #  The total claim amount \\(S\\) has a compound Poisson distribution $$S \\sim \\text{CompPoi}(\\lambda v,G)$$ if \\(S\\) has a compound distribution with \\(N \\sim \\text{Poi}(\\lambda v)\\) for given \\(\\lambda,v\u0026gt;0\\) and individual claim size distribution \\(G\\).\n The compound Poisson distribution has nice properties such as:  The aggregation property \\(\\uparrow\\) The disjoint decomposition property \\(\\downarrow\\)   These are reviewed in the next section, along with related new techniques for computing the distribution of \\(S\\).  Mixed Poisson distribution #  Inhomogeneous portfolio #   So far we have seen distributions with variance less (binomial) or exactly equal (Poisson) to the mean. In reality, actuarial data is often overdispersed, that is, variance is larger than mean. This could be due to frequency or severity, but it makes sense that some of this extra variability would come from frequency. If we believe in a Poisson frequency for known frequency parameter, then additional uncertainty such as heterogeneity of risks in a portfolio, uncertain conditions (weather, for instance) could be modelled with a random Poisson parameter, and could explain the extra variability. This is the idea of a mixed Poisson.  The mixed Poisson distribution #   Assume \\(\\Lambda \\sim H\\) with \\(H(0)=0\\), \\(E[\\Lambda]=\\lambda\\), and \\(Var(\\Lambda)\u0026gt;0\\). Conditionally, given \\(\\Lambda\\), \\(N\\sim \\text{Poi}(\\Lambda v)\\) for fixed volume \\(v\u0026gt;0\\).  We have then\n$$\\begin{array}{rcl} \\Pr[N=n]\u0026amp;=\u0026amp;\\int_0^\\infty \\Pr[N=n|\\Lambda=\\lambda] d H(\\lambda)=\\int_0^\\infty \\frac{e^{-\\lambda v}(\\lambda v)^n}{n!} d H(\\lambda); \\\\ E[N] \u0026amp;=\u0026amp; E\\left[E[N|\\Lambda]\\right] = E[\\Lambda]v=\\lambda v; \\\\ Var(N) \u0026amp;=\u0026amp; E\\left[Var(N|\\Lambda)\\right] + Var\\left(E[N|\\Lambda]\\right)= \\lambda v+v^2 Var(\\Lambda) \u0026gt; \\lambda v; \\\\ M_N(t) \u0026amp;=\u0026amp;E\\left[e^{tN}\\right] = E\\left[E\\left[e^{tN}|\\Lambda\\right]\\right]=E\\left[e^{\\Lambda v (e^t-1)}\\right]=M_\\Lambda(v[e^t-1]). \\end{array}$$\nExample #  If \\(\\Lambda \\sim \\text{inverse Gaussian}(\\alpha,\\beta)\\) (Example 12.3.2):\n \\(N\\) is Poisson Inverse Gaussian. \\(\\adv\\) This distribution is the pig distribution in actuar, so that you can use dpig, ppig, etc‚Ä¶); see Section 5 of the vignette ‚Äúdistribution‚Äù of actuar. \\(\\longrightarrow\\) \\(S\\) will be compound inverse Gaussian.  Another example, which is very famous, is \\(\\Lambda \\sim \\Gamma\\), which leads to the negative-binomial distribution.\nNegative-binomial distribution #   Define \\(\\Lambda = \\lambda \\Theta\\). Now, \\(\\Theta \\sim \\Gamma(\\gamma,\\gamma)\\) such that $$ E[\\Theta]=1 \\quad \\text{and} \\quad Var(\\Theta)=\\frac{1}{\\gamma}$$ and $$ E[\\Lambda]=\\lambda \\quad \\text{and} \\quad Var(\\Lambda)=\\frac{\\lambda^2}{\\gamma}.$$ If conditionally, given \\(\\Theta\\), \\(N\\sim \\text{Poi}(\\Theta \\lambda v)\\), then $$N \\sim \\text{NegBin}(\\lambda v, \\gamma)$$ with volume \\(v\u0026gt;0\\), expected claims frequency \\(\\lambda\u0026gt;0\\), and dispersion parameter \\(\\gamma \u0026gt;0\\).   Proof:\n$$\\begin{array}{rcl} M_N(t) \u0026amp;=\u0026amp; E[e^{tN}]=E\\left[ E[e^{tN} | \\Theta] \\right] = E\\left[ e^{\\Theta \\lambda v (e^t-1)} \\right] \\\\\\\\ \u0026amp;=\u0026amp; \\left( \\frac{\\gamma}{\\gamma-\\lambda v (e^t-1)} \\right)^\\gamma = \\left( \\frac{\\gamma}{\\gamma +\\lambda v -\\lambda v e^t} \\right)^\\gamma = \\left( \\frac{\\frac{\\gamma}{\\lambda v +\\gamma}}{1 -\\frac{\\lambda v}{\\lambda v +\\gamma} e^t} \\right)^\\gamma, \\end{array}$$\nwhich can be recognised as a negative-binomial with probability of ‚Äúfailure‚Äù $$p = \\frac{\\lambda v}{\\lambda v +\\gamma}$$ so that $$p_k =\\Pr[N=k]={k+\\gamma -1 \\choose k}p^k(1-p)^{\\gamma}$$ In R, use dnbinom, pnbinom, qnbinom, rnbinom, where size is \\(\\gamma\\) and prob is probability of success \\(1-p\\) (note volume is hidden in \\(p\\) and will affect the scale of the distribution).\nInterpretation #   \\(\\Theta\\) reflects the uncertainty about the `true‚Äô parameter of the Poisson distribution. In the end we have  $$\\begin{array}{rcl} E[N] \u0026amp;=\u0026amp; \\lambda v, \\\\ Var(N) \u0026amp;=\u0026amp; \\lambda v \\left(1+\\frac{\\lambda v}{\\gamma}\\right) \u0026gt; \\lambda v,\\\\ \\text{Vco}\\left( \\frac{N}{v} \\right) \u0026amp;=\u0026amp; \\sqrt{(\\lambda v)^{-1} + \\gamma^{-1}}. \\end{array}$$\n This additional uncertainty is not diversifiable (remains even for large \\(v\\)): $$\\text{Vco}\\left( \\frac{N}{v} \\right) = \\sqrt{(\\lambda v)^{-1} + \\gamma^{-1}} \\rightarrow \\gamma^{-1/2} \u0026gt; 0\\quad \\text{ for } v \\rightarrow \\infty.$$  Compound negative-binomial model #  The total claim amount \\(S\\) has a compound negative-binomial distribution $$S \\sim \\text{CompNB}(\\lambda v,\\gamma,G)$$ if \\(S\\) has a compound distribution with \\(N \\sim \\text{NegBin}(\\lambda v,\\gamma)\\) for given \\(\\lambda,v,\\gamma\u0026gt;0\\) and individual claim size distribution \\(G\\).\nAdditional properties and applications of Poisson frequencies #  Theorem 2.12: Aggregation property \\(\\uparrow\\) #  Assume \\(S_1,\\ldots,S_n\\) are independent with \\(S_j \\sim \\text{CompPoi}(\\lambda_j v_j,G_j)\\) for all \\(j=1,\\ldots,n\\). The aggregated claim has a compound Poisson distribution $$S = \\sum_{j=1}^n S_j \\sim \\text{CompPoi}(\\lambda v,G),\\text{ with}$$ $$v= \\sum_{j=1}^n v_j,\\quad \\lambda = \\sum_{j=1}^n \\frac{v_j}{v} \\lambda_j,\\quad G = \\sum_{j=1}^n \\frac{\\lambda_j v_j}{\\lambda v} G_j.$$ So what?\n Independent \\(n\\) portfolios of losses can be easily aggregated. Alternatively (or in addition), total claims paid over \\(n\\) years are compound Poisson, even if the severity and frequency of losses vary across years. ‚ÄúBottom-up‚Äù modelling In Bowers et al. (1997), this is Theorem 12.4.1.  Proof #  Let \\(m_{X_i}(t)\\) as the mgf of \\(X_i\\) (with df \\(P_i(t)\\)). Then $$m_{S_i}(t)=\\exp\\{\\lambda_i(m_{X_i}(t)-1)\\}$$ and\n$$\\begin{array}{rcl} m_S(t)\u0026amp;=\u0026amp;\\prod_{i=1}^m m_{S_i}(t)=\\exp\\left( \\sum_{i=1}^m\\lambda_i(m_{X_i}(t)-1)\\right) \\\\ \u0026amp;=\u0026amp;\\exp\\left(\\lambda\\left(\\sum_{i=1}^m \\frac{\\lambda_i}{\\lambda}m_{X_i}(t)-1\\right)\\right) \\end{array}$$\nThus, \\(S\\) has the mgf of a compound Poisson rv with intensity \\(\\lambda\\), iff \\(\\sum_{i=1}^m\\frac{\\lambda_i}{\\lambda}m_{X_i}(t)\\) is a mgf. Indeed, it is the mgf of $$P(x)=\\sum_{i=1}^m\\frac{\\lambda_i}{\\lambda}P_i(x)$$\nExample 12.4.1 of Bowers et al. (1997) #  Suppose that \\(N_1, N_2,\\cdots, N_m\\) are independent random variables. Further, suppose that \\(N_i\\) follows Poisson($\\lambda_i$). Let \\(x_1,x_2,\\cdots, x_m\\) be deterministic numbers. What is the distribution of $$x_1N_1+\\cdots+x_mN_m?$$\nTheorem 2.14: Disjoint decomposition property \\(\\downarrow\\) #  Preliminary 1: Add LoBs in the CompPoi formulation #  Let us introduce Lines of Business (‚ÄúLoB‚Äù) in the notation:\n Let \\(\\left(p_j^+\\right)_{j=1,\\ldots,m}\\) be a discrete probability distribution on the finite set \\(\\{1,\\ldots,m\\}\\). We can interpret the set \\(\\{1,\\ldots,m\\}\\) as different sub-portfolios, or different lines of business (‚ÄúLoB‚Äù thereafter). For instance, we could have \\(j\\in\\{1,2,3\\}\\) for car \\((j=1)\\), building \\((j=2)\\) and liability \\((j=3)\\) LoBs. We assume $$ p_j^+ \u0026gt;0 \\text{ for all }j,$$ that is, the probability of having claims in any of the \\(m\\) LoBs is strictly positive. We further assume that \\(G_j\\) is the claim size distribution of LoB \\(j\\), with \\(G_j(0)=0\\).    Finally, we define the mixture distribution by $$G(y)= \\sum_{j=1}^m p_j^+ G_j(y)\\quad \\text{ for }y \\in \\mathbb{R}.$$ Note that this matches the formulation in the aggregation property Theorem 2.12 with $$p_j^+ = \\frac{\\lambda_j v_j}{\\lambda v}.$$ Now, define a discrete random variable \\(I\\) which indicates which sub-portfolio a randomly selected claim \\(Y\\) belongs to: $$\\Pr [I=j]= p_j^+ \\quad \\text{ for all }j\\in \\{1,\\ldots,m\\}.$$   We are now ready to define the following extended compound Poisson model:\n The total claims \\(S = \\sum_{i=1}^N Y_i\\) has a compound Poisson distribution as defined earlier. In addition, we assume that $$ (Y_i,I_i)_{i\\ge 1}$$ are  i.i.d. and independent of \\(N\\), with \\(Y_i\\) having marginal distribution function \\(G\\) with \\(G(0)=0\\), and \\(I_i\\) having marginal distribution function given by \\(\\Pr [I=j]= p_j^+ \\quad \\text{ for all }j\\in \\{1,\\ldots,n\\}\\).    Preliminary 2: Partition #   The random vector \\((Y_1,I_1)\\) takes values in \\(\\mathbb{R}_+ \\times \\{1,\\ldots,m\\}\\). On this set we choose a finite sequence of sets $$A_1, \\ldots, A_n$$ such that  $$\\begin{array}{rcl} A_k \\cap A_l \u0026amp;=\u0026amp; \\emptyset \\text{ for all }k\\neq l\\quad \\text{(no overlap)}; \\\\ \\cup_{k=1}^n A_k \u0026amp;=\u0026amp; \\mathbb{R}_+ \\times \\{1,\\ldots,m\\} \\quad \\text{(all-inclusive)}. \\end{array}$$\n Such a sequence is called a ‚Äúmeasurable disjoint decomposition‚Äù or ‚Äúpartition‚Äù of \\(\\mathbb{R}_+ \\times \\{1,\\ldots,m\\}\\). This partition is called ‚Äúadmissible‚Äù for \\((Y_1,I_1)\\) if for all \\(k=1,\\ldots,n\\) $$p^{(k)} = \\Pr[(Y_1,I_1)\\in A_k]\u0026gt;0.$$ Note that \\(\\sum_{k=1}^n p^{(k)}=1\\) due to the properties of the partition above (no overlap and all-inclusive).  Theorem 2.14: Disjoint decomposition \\(\\downarrow\\) #   Assume that \\(S\\) fulfills the extended compound Poisson model assumptions above (Preliminary 1). We choose an admissible partition \\(A_1, \\ldots,A_n\\) for \\((Y_1,I_1)\\) (Preliminary 2).  Then the random variable (sum of claims for partition \\(k\\)): $$S_k=\\sum_{i=1}^N Y_i 1_{\\{(Y_i,I_i)\\in A_k\\}} \\sim \\text{CompPoi}(\\lambda_k v_k,G_k),$$ for \\(k=1,\\ldots,n\\), with $$\\lambda_k v_k = \\lambda v p^{(k)} \u0026gt; 0,\\quad G_k(y) = \\Pr[Y_1 \\le y | (Y_1,I_1) \\in A_k].$$ Furthermore, the \\(S_k\\)‚Äôs are independent (over \\(k\\)).\n\\(\\adv\\) Thinning of the Poisson process #   Assume that \\(m=1\\) (only one LoB) The disjoint decomposition theorem implies that $$ Y_i = Y_i 1_{{Y_i\\in A_1}} + \\ldots + Y_i 1_{{Y_i\\in A_n}}.$$ For for each partition \\(A_k\\) (defined on the claims) a natural choice is  \\(v_k=v\\) \\(\\lambda_k = \\lambda p^{(k)}\\)   This means that the volume remains constant in each partition, but the expected claims frequencies \\(\\lambda_k\\) change proportionally to the probabilities of falling in partition \\(A_k\\), \\(k=1,\\ldots,n\\). This is called thinning of the Poisson process.  \\(\\adv\\) Sparse vector algorithm #  If \\(S\\sim\\text{compound Poisson}(\\lambda,g(y_i)=\\pi_i)\\), \\(i=1,\\ldots,m\\) then $$S=y_1N_1+\\ldots+y_mN_m,$$ where the \\(N_i\\)‚Äôs\n represent the number of claims of amount \\(y_i\\); are mutually independent; are Poi \\((\\lambda_i=\\lambda \\pi_i).\\)  Proof: see tutorial exercise los18. Note also that this is a special case of Theorem 2.14, and is Theorem 12.4.2 of Bowers et al. (1997).\nSo what?\n Sparse vector algorithm: allows to develop an alternative method for tabulating the distribution of \\(S\\) that is more efficient as \\(m\\) is small. \\(S\\) can be used to approximate the Individual Risk Model if \\(X=Ib\\) (see Module 3).  \\(\\adv\\) The sparse vector algorithm #  (Bowers et al. 1997, Example 12.4.2) Suppose \\(S\\) has a compound Poisson distribution with \\(\\lambda =0.8\\) and individual claim amount distribution\n   $y_i$  \\(\\Pr\\left[ Y=y_i\\right]\\)     1  0.250   2  0.375   3  0.375    Compute \\(f_{S}\\left( s\\right) =\\Pr\\left[ S=s\\right]\\) for \\(s=0,1,...,6\\).\nThis can be done in two ways:\n Basic method (seen earlier in the lecture): requires to calculate up to the 6th convolution of \\(Y\\). Sparse vector algorithm: requires no convolution of \\(Y\\).   Solution - Basic Method\n   \\(x\\) \\(g^{*0}\\left(x \\right)\\) \\(g\\left( x\\right)\\) \\(g^{* 2}\\left(x\\right)\\) \\(g^{* 3}\\left( x\\right)\\) \\(g^{* 4}\\left( x\\right)\\) \\(g^{* 5}\\left( x\\right)\\) \\(g^{* 6}\\left( x\\right)\\) \\(f_{S}\\left( x\\right)\\)     0 1 - - - - - - 0.4493   1 - 0.250 - - - - - 0.0899   2 - 0.375 0.0625 - - - - 0.1438   3 - 0.375 0.1875 0.0156 - - - 0.1624   4 - - 0.3281 0.0703 0.0039 - - 0.0499   5 - - 0.2813 0.1758 0.0234 0.0010 - 0.0474   6 - - 0.1406 0.2637 0.0762 0.0073 0.0002 0.0309   \\(n\\) 0 1 2 3 4 5 6    \\(\\Pr[N=n]=e^{-0.8}\\frac{\\left( 0.8\\right) ^{n}}{n!}\\) 0.4493 0.3595 0.1438 0.0383 0.0077 0.0012 0.0002      The convolutions are done in the usual way. The \\(f_S(x)\\) are the sumproduct of the row \\(x\\) and row \\(\\Pr[N=n]\\). The number of convolutions (and thus of columns) will increase by 1 for each new value of \\(f_S(x)\\), without bound!   Solution - Sparse vector algorithm\nThanks to Theorem 2.12, we can write \\(S=N_1+2N_2+3N_3\\)\n   \\(x\\) \\(\\Pr\\left[ N_{1}=x\\right]\\) \\(\\Pr\\left[ 2N_{2}=x\\right]\\) \\(\\Pr\\left[ 3N_{3}=x\\right]\\) \\(\\Pr\\left[ N_{1}+2N_{2}=x\\right]\\) \\(f_{S}\\left( x\\right)\\)     0 0.818731 0.740818 0.740818 0.606531 0.449329   1 0.163746 0 0 0.121306 0.089866   2 0.016375 0.222245 0 0.194090 0.143785   3 0.001092 0 0.222245 0.037201 0.162358   4 0.000055 0.033337 0 0.030974 0.049906   5 0.000002 0 0 0.005703 0.047360   6 0.000000 0.003334 0.033337 0.003288 0.030923   \\(x_i\\) 1 2 3     \\(\\lambda _{i}=\\lambda \\pi_i\\) 0.2 0.3 0.3     \\(\\Pr[N_i=x/i]\\) \\(e^{-0.2}\\frac{\\left( 0.2\\right) ^{x}}{x!}\\) \\(e^{-0.3}\\frac{\\left(0.3\\right) ^{x/2}}{(x/2)!}\\) \\(e^{-0.3}\\frac{\\left( 0.3\\right) ^{x/3}}{(x/3)!}\\)      The \\(f_S(x)\\) are convolution, e.g.: $$(5)[3]=.818731\\cdot0+.163746\\cdot.222245+.016375\\cdot0+.001092\\cdot.740818$$ $$(6)[3]=.740818\\cdot.037201+0\\cdot.194090+0\\cdot.121306+.222245\\cdot.606531$$\nNote that only two convolutions are needed: columns (5) and (6).\n\\(\\adv\\) Example 2.16: Large claim separation #   This is a very important (and convenient) application of the Disjoint decomposition property (Theorem 2.14). Attritional and catastrophic claims often have very different distributions (different \\(G\\)‚Äôs). Idea here is to divide the claims into different layers with different distributions:  Small claims are modelled using a parametric distribution for which it is easy to obtain the distribution of the compound distribution, potentially even approximated with a normal distribution thanks to volume and light right tail; Large claims are typically modelled with a Pareto distribution with threshold \\(M\\) and tail parameter \\(\\alpha\u0026gt;1\\) (see Module 6 for a justification of this, and for the choice of an appropriate \\(M\\)).     \\(\\adv\\) Assuming two layers:\n We choose a large claims threshold \\(M\u0026gt;0\\) such that $$0 \u0026lt; G(M) \u0026lt; 1,$$ that is, there is probability mass on either size of \\(M\\). We define the partition $$A = A_1 = \\{Y_1 \\le M\\} \\quad \\text{and} \\quad A^c = A_2 = \\{Y_1\u0026gt;M\\}.$$ Assume that $$S \\sim \\text{CompPoi}(\\lambda v, G).$$ We now define the small and large claims layers as $$\\begin{array}{rcl} S_{\\text{sc}} \u0026amp;=\u0026amp; \\sum_{i=1}^N Y_i 1_{\\{Y_i\\le M\\}}, \\text{ and}\\\\ S_{\\text{lc}} \u0026amp;=\u0026amp; \\sum_{i=1}^N Y_i 1_{\\{Y_i \u0026gt; M\\}}, \\end{array}$$ respectively.    \\(\\adv\\) Theorem 2.14 implies that \\(S_{\\text{sc}}\\) and \\(S_{\\text{lc}}\\) are and compound Poisson distributed with  $$\\begin{array}{rcl} S_{\\text{sc}} \u0026amp;\\sim \u0026amp; \\text{CompPoi}(\\lambda_{\\text{sc}}v = \\lambda G(M)v, \\\\ \u0026amp;\u0026amp;\\quad\\quad\\quad\\quad G_{\\text{sc}}(y)=\\Pr[Y_1\\le y|Y_1\\le M]), \\text{ and} \\\\ S_{\\text{lc}} \u0026amp;\\sim \u0026amp; \\text{CompPoi}(\\lambda_{\\text{lc}}v = \\lambda (1-G(M))v, \\\\ \u0026amp;\u0026amp;\\quad\\quad\\quad\\quad G_{\\text{lc}}(y)=\\Pr[Y_1\\le y|Y_1\u0026gt; M]), \\end{array}$$\nrespectively.\n The distribution of $$S=S_{\\text{sc}} + S_{\\text{lc}}$$ can then be obtained by a simple convolution of distributions of \\(S_{\\text{sc}}\\) and \\(S_{\\text{lc}}\\) (thanks to independence); see Module 4 for examples.  \\(\\adv\\) Parameter estimation (MW 2.3) #  \\(\\adv\\) Introduction #  \\(\\adv\\) Estimation methods #  You should be familiar with the main estimation methods:\n Method of moments Maximum likelihood estimation  Here the problem is slightly complicated because our observations may not be directly comparable due to varying exposures \\(v\\)‚Äôs.\nAssume that \\((N_1,\\ldots,N_T)'\\) is the vector of observations.\n\\(\\adv\\) What to do with volumes? Lemma 2.26 #   The key idea here is to find the minimum variance method of moments estimator, when the volumes across the observations can vary. This is what is different from a straight method of moments estimator, and explains why we need to think it through: how to deal with those volumes? Assume there exist strictly positive volumes \\(v_1,\\ldots, v_T\\) such that the components of \\((N_1/v_1,\\ldots,N_T/v_T)\\) are independent with $$\\lambda = E\\left[\\frac{N_t}{v_t}\\right]\\text{ and } \\tau_t^2 = Var\\left(\\frac{N_t}{v_t}\\right) \\in (0,\\infty),$$ for all \\(t = 1,\\ldots,T\\).   Lemma 2.26 states that the unbiased, linear estimator for \\(\\lambda\\) with minimal variance is given by $$\\widehat{\\lambda}^{\\text{MV}}_T = \\left( \\sum_{t=1}^T \\frac{1}{\\tau_t^2}\\right)^{-1} \\sum_{t=1}^T \\frac{N_t/v_t}{\\tau_t^2},$$ with variance $$Var(\\widehat{\\lambda}^{\\text{MV}}_T) = \\left( \\sum_{t=1}^T \\frac{1}{\\tau_t^2} \\right)^{-1}.$$\nNote:\n We haven‚Äôt made any distributional assumption yet - this estimates \\(E\\left[\\frac{N_t}{v_t}\\right]\\) via method of moments, taking the \\(v_t\\)‚Äôs into account in an optimal way (in the sense that it minimises the variance of the estimator). The superscript ‚ÄúMV‚Äù stands for ‚Äúminimal variance.‚Äù  \\(\\adv\\) Method of moments #  \\(\\adv\\) Binomial and Poisson cases #  Unbiased, minimal variance estimators:\n binomial case for \\(p\\): $$\\widehat{p}_T^{\\text{MV}} = \\frac{1}{\\sum_{s=1}^T v_s} \\sum_{t=1}^T N_t = \\sum_{t=1}^T \\frac{v_t}{\\sum_{s=1}^T v_s} \\frac{N_t}{v_t} \\sim$$ Furthermore, \\(\\sum_{t=1}^T N_t\\sim\\text{Binom}(\\sum_{s=1}^T v_s,p)\\), which means we know the distribution of \\(\\widehat{p}_T^{\\text{MV}}\\). Poisson case for \\(\\lambda\\): $$\\widehat{\\lambda}_T^{\\text{MV}} = \\frac{1}{\\sum_{s=1}^T v_s} \\sum_{t=1}^T N_t = \\sum_{t=1}^T \\frac{v_t}{\\sum_{s=1}^T v_s} \\frac{N_t}{v_t}$$ Here, \\(\\sum_{t=1}^T N_t\\sim\\text{Poi}(\\lambda \\sum_{s=1}^T v_s)\\).  \\(\\adv\\) Negative binomial case #  More complicated, because: $$E\\left[\\frac{N_t}{v_t}\\right] = \\lambda \\text{ and } Var\\left(\\frac{N_t}{v_t}\\right) = \\lambda/v_t + \\lambda^2/\\gamma= \\tau_t^2,$$ Unbiased (but not guaranteed minimal variance): $$\\widehat{\\lambda}^{\\text{NB}}_T = \\frac{1}{\\sum_{s=1}^T v_s} \\sum_{t=1}^T N_t = \\sum_{t=1}^T \\frac{v_t}{\\sum_{s=1}^T v_s} \\frac{N_t}{ v_t}$$\n \\(\\adv\\) We need a sense of the dispersion for estimating the dispersion parameter \\(\\gamma\\).\nLet the weighted sample variance $$\\widehat{V}_T^2 = \\frac{1}{T-1} \\sum_{t=1}^T v_t \\left( \\frac{N_t}{v_t} - \\widehat{\\lambda}^{\\text{NB}}_T \\right) ^2.$$ Then we have $$\\widehat{\\gamma}^{\\text{NB}}_T = \\frac{(\\widehat{\\lambda}^{\\text{NB}}_T)^2}{\\widehat{V}_T^2 -\\widehat{\\lambda}^{\\text{NB}}_T}\\frac{1}{T-1} \\left( \\sum_{t=1}^T v_t - \\frac{ \\sum_{t=1}^T v_t^2}{ \\sum_{t=1}^T v_t} \\right),$$ ONLY if \\(\\widehat{V}_T^2 \u0026gt;\\widehat{\\lambda}^{\\text{NB}}_T\\). Otherwise use Poisson or binomial.\n\\(\\adv\\) Maximum likelihood estimators #  \\(\\adv\\) Binomial and Poisson cases #  Estimators are identical to method of moments estimators. Or conversely, the MLE estimators are actually unbiased.\n binomial case for \\(p\\): $$\\widehat{p}_T^{\\text{MLE}} = \\frac{1}{\\sum_{s=1}^T v_s} \\sum_{t=1}^T N_t = \\sum_{t=1}^T \\frac{v_t}{\\sum_{s=1}^T v_s} \\frac{N_t}{v_t}=\\widehat{p}_T^{\\text{MV}}$$ Poisson case for \\(\\lambda\\): $$\\widehat{\\lambda}_T^{\\text{MLE}} = \\frac{1}{\\sum_{s=1}^T v_s} \\sum_{t=1}^T N_t = \\sum_{t=1}^T \\frac{v_t}{\\sum_{s=1}^T v_s} \\frac{N_t}{v_t}= \\widehat{\\lambda}_T^{\\text{MV}}$$  \\(\\adv\\) Negative binomial case #  Assume \\(N_1,\\ldots, N_T\\) are independent and \\(\\text{NegBin}(\\lambda v_t,\\gamma)\\). The MLE \\((\\widehat{\\lambda}_T^{\\text{MLE}},\\widehat{\\gamma}^{\\text{MLE}}_T)\\) are the solution of $$\\frac{\\partial}{\\partial (\\lambda, \\gamma)} \\sum_{t=1}^T \\log {N_t +\\gamma -1 \\choose N_t} + \\gamma \\log (1-p_t) + N_t \\log p_t = 0,$$ with \\(p_t = \\lambda v_t / (\\gamma + \\lambda v_t) \\in (0,1)\\).\n\\(\\adv\\) The \\((a,b,0)\\) and \\((a,b,1)\\) classes of distributions #  \\(\\adv\\) The \\((a,b)\\) class of Panjer distributions (4.2.1) #  A class of distributions has the following property\n$$ \\Pr[N=n]=\\left(a+\\frac{b}{n}\\right)\\Pr[N=n-1], \\text{ or } \\frac{p_k}{p_{k-1}}=\\left(a+\\frac{b}{k}\\right). $$\nThis is the \\((a,b)\\) class of ‚ÄúPanjer distributions.‚Äù This means that \\(\\Pr[N=n]\\) can be obtained recursively with initial value \\(\\Pr[N=0]\\); see Wuthrich (2020), Definition 4.6.\nThe exhaustive list of its members (see Wuthrich 2020 Lemma 4.7) is\n   Distribution \\(a\\) \\(b\\) \\(\\Pr[N=0]\\)     Poisson \\(\\left( \\lambda \\right)\\) \\(0\\) \\(\\lambda\\) \\(e^{-\\lambda}\\)   Neg Bin \\(\\left( r,p\\right)\\) \\(1-p\\) \\((r-1)(1-p)\\) \\(p^r\\)   Binomial \\(\\left( m,p\\right)\\) \\(-p/(1-p)\\) \\((m+1)p/(1-p)\\) \\((1-p)^m\\)    Exercise: prove the results in the above table! (Note \\(p\\) is the probability of success for the Negative Binomial)\n\\(\\adv\\) First three cumulants of the \\((a,b)\\) family #     Distribution \\(E[N]\\) \\(Var(N)\\) \\(E\\left[(N-E[N])^3\\right]\\)     Poisson \\(\\left( \\lambda \\right)\\) \\(\\lambda\\) \\(\\lambda\\) \\(\\lambda\\)   Neg Bin \\(\\left( r,p\\right)\\) \\(\\displaystyle\\frac{rq}{p}\\) \\(\\displaystyle \\frac{rq}{p^2}\\) \\(\\displaystyle \\frac{rq(1+q)}{p^3}\\)   Binomial \\(\\left( m,p\\right)\\) \\(mp\\) \\(mpq\\) \\(mpq(q-p)\\)    Exercise:\n check these results using the cgf find the first 3 cumulants of \\(S\\), as well as \\(\\varsigma_S\\) for each member of the family  \\(\\adv\\) actuar and the \\((a,b,1)\\) class #   The package actuar extends the definition above to allow for zero-truncated and zero-modified distributions. The Poisson, binomial and negative-binomial (and special case geometric) are all well supported in Base R with the d, p, q and r functions. If one takes the Panjer equation for granted, then we can think of \\(p_0\\) as the mass that will make the pmf add up to one: $$\\text{given Panjer}: p_0 \\text{ is such that } \\sum_{k=0}^\\infty p_k = 1.$$ We introduce here the \\((a,b,1)\\) class which extends the idea above so that we have more freedom on the mass at 0. The reference for this section is Section 4 of the vignette ‚Äúdistribution‚Äù of actuar  \\(\\adv\\) The \\((a,b,1)\\) class of distributions #  A discrete random variable is a member of the ** \\((a,b,1)\\) class of distributions** if there exist constants \\(a\\) and \\(b\\) such that $$\\frac{p_k}{p_{k-1}}=a+\\frac{b}{k},\\quad **k=2,3,\\ldots**.$$ Note:\n The recursion starts at \\(k=2\\) for the \\((a,b,1)\\) class. The extra freedom allows the probability at zero to be set to any arbitrary number \\(0\\le p_0 \\le 1\\)  \\(\\adv\\) Zero-truncated distributions #   Setting \\(p_0=0\\) in the \\((a,b,1)\\) class defines the subclass of zero-truncated distributions Members are the zero-truncated Poisson (actuar::ztpois), zero-truncated binomial (actuar::ztbinom), zero-truncated negative-binomial (actuar::ztnbinom), and the zero-truncated geometric (actuar::ztgeom). Let \\(p_k^T\\) denote the probability mass at \\(k\\) for a zero-truncated distribution (‚Äú$T$‚Äù for truncated). We have $$p_k^T = \\left\\{ \\begin{array}{ll} 0, \u0026amp; k=0; \\\\ \\frac{p_k}{1-p_0}, \u0026amp; k=1,2,\\ldots. \\end{array}\\right.,$$ where \\(p_k\\) is the probability mass of the corresponding member of the \\((a,b,0)\\) ‚Äî that is, \\((a,b)\\) ‚Äî class. actuar provides the d, p, q, and r functions of the zero-truncated distributions mentioned above.  \\(\\adv\\) Zero-modified distributions #   Setting \\(p_0\\equiv p_0^M\\) \\((0\u0026lt;p_0^M\u0026lt;1)\\) in the \\((a,b,1)\\) class defines the subclass of zero-modified distributions (‚Äú$M$‚Äù for ‚Äúmodified‚Äù) These distributions are discrete mixtures between a degenerate distribution at zero, and the corresponding distribution from the \\((a,b,0)\\) class. Let \\(p_k^M\\) denote the probability mass at \\(k\\) for a zero-modified distribution. We have then $$p_k^M = \\left( 1-\\frac{1-p_0^M}{1-p_0}\\right)1_{\\{k=0\\}} + \\frac{1-p_0^M}{1-p_0}p_k.$$ Alternatively, $$p_k^M = \\left\\{ \\begin{array}{ll} p_0^M, \u0026amp; k=0; \\\\ \\frac{1-p_0^M}{1-p_0}p_k, \u0026amp; k=1,2,\\ldots. \\end{array}\\right.,$$ where \\(p_k\\) is the probability mass of the corresponding member of the \\((a,b,0)\\) class.    Quite obviously, zero-truncated distributions are zero-modified distributions with \\(p_0^M=0\\), and $$p_k^M = p_0^M 1_{\\{k=0\\}}+(1-p_0^M)p_k^T.$$ Members are the zero-modified Poisson (actuar::zmpois), zero-modified binomial (actuar::zmbinom), zero-modified negative-binomial (actuar::zmnbinom), and the zero-modified geometric (actuar::zmgeom). actuar provides the d, p, q, and r functions of the zero-truncated distributions mentioned above.   plot(dpois(0:7, 2.5), pch = 20, col = \u0026#34;red\u0026#34;, ylim = c(0, 0.3), cex = 1.5, type = \u0026#34;b\u0026#34;) points(dztpois(0:7, 2.5), pch = 20, col = \u0026#34;blue\u0026#34;, type = \u0026#34;b\u0026#34;) points(dzmpois(0:7, 2.5, 2 * dpois(0, 2.5)), pch = 20, col = \u0026#34;green\u0026#34;, type = \u0026#34;b\u0026#34;) References #  Bowers, Newton L. Jr, Hans U. Gerber, James C. Hickman, Donald A. Jones, and Cecil J. Nesbitt. 1997. Actuarial Mathematics. Second. Schaumburg, Illinois: The Society of Actuaries.\n Dutang, Christophe, Vincent Goulet, and Mathieu Pigeon. 2008. ‚ÄúActuar: An r Package for Actuarial Science.‚Äù Journal of Statistical Software 25 (7).\n Werner, Geoff, and Claudine Modlin. 2010. Basic Ratemaking. Casualty Actuarial Society.\n Wuthrich, Mario V. 2020. ‚ÄúNon-Life Insurance: Mathematics \u0026amp; Statistics.‚Äù Lecture notes. RiskLab, ETH Zurich; Swiss Finance Institute.\n  "},{"id":8,"href":"/docs/0-study-plan/","title":"Weekly Study Plan","section":"Docs","content":"Suggested weekly schedule\n\nTo summarise, in week \\(n\\) you should:\n on Monday:  [lec] have a first read of the lecture readings for the week (website) [lec] attend the lecture in person or online (as appropriate) [tut] review the solutions of the tutorials questions of week \\(n-1\\), and seek help if needed*   on Tuesday:  [lec] review the lecture notes from Monday, and seek help if needed* [tut] perform the pre-tutorial work and review solutions (before your Wednesday or Friday tutorial) [tut] have a first look at the tutorials questions for week \\(n\\) and see whether you can attempt some of them already (before your Wednesday or Friday tutorial)   on Wednesday  [tut] attend tutorials (if they are on Wednesday) [tut] continue your attempts of the tutorial questions for week \\(n\\)   on Thursday  [lec] finish your review of lecture notes, and seek help if needed* [tut] continue your attempts of the tutorial questions for week \\(n\\) [amt] consider the additional questions if you need more practice on week \\(n-1\\)   on Friday \u0026lt;- wrapping the week up!  [tut] attend tutorials (if they are on Friday) [amt] create draft 1 of your own summary of week \\(n\\), and seek help if some areas are unclear* [amt] upgrade your summary of week \\(n-1\\) into a \u0026ldquo;cheat sheet\u0026rdquo; for the mid-semester exam and the final exam. [lec] have a first read of the lecture readings for the next week (website)    Note [lec] refers to lecture activities (learning the theory), [tut] to tutorials (applying the theory), and [amt] to assessment preparation (prepare for exams).\n* To seek help:\n ask your question in Ed, or ask your tutor at your next tutorial, or ask the course Professor at the next consultation hours, or ask your friends in the course (on Ed!),  BUT do not e-mail tutors or the course Professor, unless your query is truly personal in nature, or has absolutely no chance of being of any relevance to any other student. Chances are other students have the same question, and so having the question being asked and answered publicly helps everyone.\n"},{"id":9,"href":"/docs/1-claims-modelling/m3-individual-claim-size-modelling/","title":"M3 Individual Claim Size Modelling","section":"Claims Modelling (CS2 Section 1)","content":"Introduction #   How to fit loss models to insurance data? Peculiar characteristics of insurance data:  complete vs incomplete set of observations left-truncated observations right-censored observations heavy tailed risks   Parametric distribution models  model parameter estimation judging quality of fit model selection criteria\n(graphical, score-based approaches, information criteria)   Concepts and R functions are demonstrated with the help of some data sets  Steps in fitting loss models to data #   explore and summarise data  graphical explorations empirical moments and quantiles   select a set of candidate distributions  Pareto, Log-Normal, Inverse Gaussian, Gamma, etc.   estimate the model parameters  method of moments maximum likelihood (MLE) maximum goodness (MGE)   evaluate the quality of a given model  graphical procedures (qq, pp plots, empirical cdf‚Äôs) score-based approaches (Kolmogorov-Smirnoff tests, AD tests, chi-square goodness-of-fit tests) likelihood based information criteria (AIC, BIC)   determine which model to choose based on needs  Insurance data #  Complete vs incomplete data #   complete, individual data  you observe the exact value of the loss   incomplete data  exact data may not be available in loss/claims data, these arise in the following situations:  observations may be grouped - observe only the range of values in which the data belongs presence of censoring and/or truncation due to typical insurance and reinsurance arrangements such as deductibles and limits      Left-truncation and right-censoring #   left-truncated observation (e.g.¬†excess / deductible)  observation is left-truncated at \\(c\\) if it is NOT recorded when it is below \\(c\\) and when it is above \\(c\\), it is recorded at its exact value.   right-censored observation (e.g.¬†policy limit)  observation is right-censored at \\(d\\) if when it is above \\(d\\) it is recorded as being equal to \\(d\\) but when it is below \\(d\\) it is recorded at its observed value.   similarly, we can define right-truncated, left-censored, \\(\\ldots\\) of course, observations can be both left-truncated and right-censored; this is often the case in actuarial data  Zero claims #   Significant proportions of zero claims are frequent, for a number of reasons:  Data is policy-based, not claims-based; Claim not exceeding deductible; Mandatory reporting of accidents; etc‚Ä¶   This complicates the fitting (parametric distributions often don‚Äôt have a flexible mass at 0, if at all) Several possible solutions  Adjust \\(X\\) by mixing 0 with a parametric distribution Adjust the frequency of claims accordingly (hence ignoring 0 claims), but this  may understate the volatility of claims (the proportion of 0‚Äôs may also be random) should be avoided if 0‚Äôs are claims of no cost (rather than absence of claim)      Heavy tailed risks #   Essentially, these are risks that can be very large. This is translated by thick tails, that is, a density that goes to zero slowly in the tails. Typically, this means that the expectation of the excess over a threshold increases with that threshold. We will encounter such losses here, but a full treatment is deferred to Module 6 (Extreme Value Theory).  Data set used for illustrations #  SUVA: bivariate data set of Swiss workers compensation medical and daily allowance costs:\n Example of real actuarial data; see Avanzi, Cassar, and Wong (2011) Data were provided by the SUVA (Swiss workers compensation insurer) Random sample of 5% of accident claims in construction sector with accident year 1999 (developped as of 2003) Claims are (joint) medical costs and daily allowance costs   SUVA \u0026lt;- read_excel(\u0026#34;SUVA.xls\u0026#34;) as_tibble(SUVA) ... ## # A tibble: 2,326 x 2 ## medcosts dailyallow ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 407 0 ## 2 12591 13742 ## 3 269 0 ## 4 142 0 ## 5 175 0 ## 6 298 839 ## 7 47 0 ... Data analysis and descriptive statistics (MW 3.1) #  It is essential, before any modelling is done, to make sure that one gets a good sense of what the data look like.\n For any type of data analysis, first thing to do is to summarise the data.  summary statistics: mean, median, standard deviation, coefficient of variation, skewness, quantiles, min, max, etc \\(\\ldots\\) gives a preliminary understanding of the data   Visualisating the data is often even more informative:  histogram, with associated kernel density empirical cdf, which can be compared to that of a normal cdf via Q-Q or P-P plot   When data are heavy tailed it often helps to perform the above on the log of the data (we can then compare the data to a lognormal) Data collection procedures and standards should be understood Any unusual feature (outliers, breaks, ‚Ä¶) should be investigated. If possible, ask the claims adjusters or data owners about them.  Raw data #  Visualise the SUVA raw data #  fitdistrplus::plotdist(SUVA$medcosts, histo = TRUE, demp = TRUE)  plotdist(SUVA$dailyallow, histo = TRUE, demp = TRUE)  plotdist(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0], histo = TRUE, demp = TRUE) A log transformation may help us see better what is happening.\nlog of raw SUVA data #  plotdist(log(SUVA$medcosts[SUVA$medcosts \u0026gt; 0]), histo = TRUE, demp = TRUE)  plotdist(log(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0]), histo = TRUE, demp = TRUE)   Medical costs are still skewed even after a log transformation, which suggests that a very heavy tailed distribution might be necessary. Daily allowance costs look symmetrical after the log transformation, which suggests a lognormal (or similar) distribution might be appropriate. Removing 0‚Äôs is especially important for the daily allowance claims (and is necessary for taking the log anyway), as more than half of the claims are 0.  Moments #  Moments of a distribution provide information:\n The mean provides its location. The second moment leads to the variance, and the coefficient of variation, which give an idea of the dispersion around the mean. Skewness is self-explanatory. Kurtosis provides an idea of how fat the tails are.   The following are also helpful:\n Loss size index function $$\\mathcal{I}(G(y)) = \\frac{\\int_0^y z dG(z)}{\\int_0^\\infty z dG(z)} \\quad \\text{and} \\quad \\widehat{\\mathcal{I}}_n(\\alpha)=\\frac{\\sum_{i=1}^{\\lfloor n \\alpha \\rfloor} Y_{(i)}}{\\sum_{i=1}^n Y_i}$$ for \\(\\alpha \\in [0,1]\\). Corresponds of the contribution of \\([0,y]\\) to the overall mean. (see also , even though MW quotes 75% for GI claims) Mean excess function $$e(u)=E[Y_i-u|Y_i\u0026gt;u] \\quad \\text{and} \\quad \\widehat{e}_n(u) = \\frac{\\sum_{i=1}^n (Y_i-u) 1_{\\{Y_i\u0026gt;u\\}}}{\\sum_{i=1}^n 1_{\\{Y_i\u0026gt;u\\}}}$$ This is useful for the analysis of large claims, and for the analysis of reinsurance. Increasing values of the mean excess function indicate a heavy tailed distribution (see also Module 6) [Note \\(u=0\\) leads to the mean only when all claims are strictly positive.]   In R:\n To get numbers:  the function actuar::emm provides empirical moments up to any order mean, stats:var and stats::sd provide mean, variance, and standard deviation (unbiased versions) codes for the Loss size index function are provided in the illustration codes for the Mean excess function are also provided, but the graph is most easily plotted with extRemes::mrlplot as will be demonstrated   The function fistdistrplus:descdist provides a graph that shows where the couple ‚Äúskewness/kurtosis‚Äù lies, in comparison with its theoretically possible locations for a certain number of distributions.  the parameter boot allows for nonparametric bootstrapping of that coordinate, which helps with the assessment of its potential variability (it is sensitive to outliers, that is, not ‚Äúrobust‚Äù) method can be \u0026quot;unbiased\u0026quot; or \u0026quot;sample\u0026quot; for the unbiased or sample versions of the moments    SUVA moments #  Medical costs:\nformat(actuar::emm(SUVA$medcosts, order = 1:3), scientific = FALSE) ## [1] \u0026quot; 1443.349\u0026quot; \u0026quot; 34268506.007\u0026quot; \u0026quot;1791560934502.502\u0026quot;  sd(SUVA$medcosts)/mean(SUVA$medcosts) ## [1] 3.93143  Daily allowance:\nformat(actuar::emm(SUVA$dailyallow, order = 1:3), scientific = FALSE) ## [1] \u0026quot; 3194.15\u0026quot; \u0026quot; 172677852.63\u0026quot; \u0026quot;20364647975482.07\u0026quot;  sd(SUVA$dailyallow)/mean(SUVA$dailyallow) ## [1] 3.991459   Medical costs:\nformat(actuar::emm(SUVA$medcosts[SUVA$medcosts \u0026gt; 0], order = 1:3), scientific = FALSE) ## [1] \u0026quot; 1492.765\u0026quot; \u0026quot; 35441771.887\u0026quot; \u0026quot;1852899392464.571\u0026quot;  sd(SUVA$medcosts[SUVA$medcosts \u0026gt; 0])/mean(SUVA$medcosts[SUVA$medcosts \u0026gt; 0]) ## [1] 3.861552  Daily allowance:\nformat(actuar::emm(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0], order = 1:3), scientific = FALSE) ## [1] \u0026quot; 6760.322\u0026quot; \u0026quot; 365467411.472\u0026quot; \u0026quot;43101156679682.695\u0026quot;  sd(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0])/mean(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0]) ## [1] 2.646343  SUVA Medical Costs skewness and kurtosis #  fitdistrplus::descdist(SUVA$medcosts, boot = 1000)  descdist(SUVA$medcosts[SUVA$medcosts \u0026gt; 0], boot = 1000)  descdist(log(SUVA$medcosts[SUVA$medcosts \u0026gt; 0]), boot = 1000)  descdist(log(log(SUVA$medcosts[SUVA$medcosts \u0026gt; 0])), boot = 1000) SUVA Daily Allowance skewness and kurtosis #  descdist(SUVA$dailyallow, boot = 1000)  descdist(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0], boot = 1000)  descdist(log(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0]), boot = 1000) ## summary statistics ## ------ ## min: 3.258097 max: 12.13806 ## median: 7.474772 ## mean: 7.63051 ## estimated sd: 1.441014 ## estimated skewness: 0.3378648 ## estimated kurtosis: 3.259708  Note descdist also gives stats as above! (not shown on the previous slides)\n Good candidates seem to be lognormal, gamma, and potentially Weibull.\nLoss index function #  SUVA.MC.lif \u0026lt;- cumsum(sort(SUVA$medcosts))/sum(SUVA$medcosts) plot(1:length(SUVA$medcosts)/length(SUVA$medcosts), SUVA.MC.lif, xlab = \u0026#34;number of claims (in 100%)\u0026#34;, ylab = \u0026#34;empirical loss size index function\u0026#34;) abline(h = 0.25, v = 0.8)  SUVA.DA.lif \u0026lt;- cumsum(sort(SUVA$dailyallow))/sum(SUVA$dailyallow) plot(1:length(SUVA$dailyallow)/length(SUVA$dailyallow), SUVA.DA.lif, xlab = \u0026#34;number of claims (in 100%)\u0026#34;, ylab = \u0026#34;empirical loss size index function\u0026#34;) abline(h = 0.25, v = 0.8) Mean excess function #  This function will return the mean excess function for an arbitrary vector of thresholds u (for instance, 0, 100, 1000 and 10000 here)\nmef \u0026lt;- function(x, u) { mefvector \u0026lt;- c() for (i in u) { mefvector \u0026lt;- c(mefvector, sum(pmax(sort(x) - i, 0))/length(x[x \u0026gt; i])) } return(mefvector) } mef(SUVA$medcosts, c(0, 100, 1000, 10000)) ## [1] 1492.765 1709.148 6233.296 18156.631 mean(SUVA$medcosts) ## [1] 1443.349 mean(SUVA$medcosts[SUVA$medcosts \u0026gt; 0]) ## [1] 1492.765  The graph is best done with extRemes::mrlplot\nmrlplot(SUVA$medcosts[SUVA$medcosts \u0026gt; 0]) Linear increases point towards a heavy tailed distribution. Here the graph is biased because dominated by a few very large claims.\n If we restrict the graph to range up to 20000 (which is roughly 99% of the data) we get:\nmrlplot(SUVA$medcosts[SUVA$medcosts \u0026gt; 0], xlim = c(250, 20000))  mrlplot(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0]) This is not as heavy tailed.\nQuantiles #  quantile(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0]) ## 0% 25% 50% 75% 100% ## 26.0 842.0 1763.0 4841.5 186850.0  One can also focus on particular quantiles:\nquantile(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0], probs = c(0.75, 0.95, 0.99)) ## 75% 95% 99% ## 4841.5 25140.5 93285.0  Note this corresponds to Values at Risk (‚ÄúVaR‚Äùs).\nBoxplots #  boxplot(list(`Medical costs` = SUVA$medcosts[SUVA$medcosts \u0026gt; 0], `Daily allowance` = SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0]))  boxplot(list(`Medical costs` = log(SUVA$medcosts[SUVA$medcosts \u0026gt; 0]), `Daily allowance` = log(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0]))) Log-log plots #  The log-log plot is defined as $$ y \\rightarrow \\left[ \\log y, \\log (1-G(y))\\right],$$ where \\(G\\) is simply replaced by \\(\\hat{G}\\) for the empirical version.\nJust as for the (empirical) mean-excess plots, a linear behaviour (now decreasing) in the (empirical) log-log plot suggests a heavy-tailed distribution. Typical log-log plots are as in Figure 3.19 of Wuthrich (2020).\nSUVA log-log plots #  emplot(SUVA$medcosts[SUVA$medcosts \u0026gt; 0], alog = \u0026#34;xy\u0026#34;, labels = TRUE)  emplot(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0], alog = \u0026#34;xy\u0026#34;, labels = TRUE) Again, medical costs are a good candidate for heavy tailed distributions (graph is more linear, except at the very end), whereas daily allowance is more reasonably behaved (graph is more concave).\nIn what follows we will focus on the fitting of daily allowance costs. Medical costs will be modelled in Module 6.\nSelected parametric claims size distributions (MW 3.2) #  Parametric models for severity \\(Y\\) #  Gamma #   We shall write \\(Y\\sim \\Gamma\\left( \\alpha ,\\beta \\right)\\) if density has the form $$g\\left( y\\right) =\\frac{\\beta ^{\\alpha }}{\\Gamma \\left( \\alpha \\right) }y^{\\alpha -1}e^{-\\beta y}\\text{, for }y\u0026gt;0\\text{; }\\alpha ,\\beta\u0026gt;0\\text{.}$$ Mean: \\(E\\left( Y\\right) =\\alpha /\\beta\\) Variance: \\(Var\\left( Y\\right) =\\alpha /\\beta ^{2}\\) Skewness: \\(\\varsigma_Y=2/\\sqrt{\\alpha }\\) [positively skewed distribution] Mgf: \\(M_{Y}\\left( t\\right) =\\left( \\dfrac{\\beta }{\\beta-t}\\right) ^{\\alpha }\\), provided \\(t \u0026lt; \\beta\\) . Higher moments: \\(E\\left( Y^{k}\\right) =\\dfrac{\\Gamma \\left(\\alpha +k\\right) }{\\Gamma \\left( \\alpha \\right) \\beta ^{k}}\\) Special case: When \\(\\alpha =1\\), we have $YExp() $  Inverse Gaussian #   We shall write $YIG( ,) $ if density has the form $$g\\left( y\\right) =\\frac{\\alpha y^{-3/2}}{\\sqrt{2\\pi \\beta }}\\exp \\left[ -\\frac{\\left( \\alpha -\\beta y\\right) ^{2}}{2\\beta y}\\right] \\text{, for }y\u0026gt;0\\text{; }\\alpha ,\\beta \u0026gt;0\\text{.}$$ Mean: \\(E\\left( Y\\right) =\\alpha / \\beta\\) Variance: \\(Var\\left( Y\\right) =\\alpha /\\beta ^{2}\\) Skewness: \\(\\varsigma_Y=3/\\sqrt{\\alpha }\\) [positively skewed distribution] Mgf: \\(M_{Y}\\left( t\\right) =e^{\\alpha \\left( 1-\\sqrt{1-2t/\\beta }\\right) }\\), provided \\(t \u0026lt; \\beta /2\\) . The term ‚ÄúInverse Gaussian‚Äù comes from the fact that there is an inverse relationship between its cgf and that of the Gaussian distribution, but NOT from the fact that the inverse is Gaussian!  Weibull #   We shall write \\(Y\\sim \\text{Weibull}\\left(\\tau,c \\right)\\) if density has the form $$g\\left( y\\right) = (c\\tau) (cy)^{\\tau-1} \\exp \\{-(cy)^\\tau\\} \\text{, for }y\\ge 0\\text{; }\\alpha ,\\beta\u0026gt;0\\text{.}$$ Note \\(G(u)=1-\\exp\\{-(cy)^\\tau\\}\\). Mean: \\(E\\left( Y\\right) =\\frac{\\gamma(1+1/\\tau)}{c}\\) Variance: \\(Var\\left( Y\\right) =\\frac{\\gamma(1+2/\\tau)}{c^2}-\\mu_Y^2\\) Skewness: \\(\\varsigma_Y= \\left[ \\frac{\\gamma(1+2/\\tau)}{c^2} - 3\\mu_Y\\sigma_Y^2-\\mu_Y^3\\right]/ \\sigma_Y^3\\) Mgf: does not exist for \\(\\tau\u0026lt;1\\) and \\(t\u0026gt;0\\). Higher moments: \\(E\\left( Y^{k}\\right) =\\frac{\\gamma(1+k/\\tau)}{c^k}\\) Note: if \\(Z\\sim\\text{expo}(1)\\) then \\(Z^{1/\\tau}/c\\sim \\text{Weibull}\\left(\\tau,c \\right)\\).  Lognormal #   We shall write \\(Y\\sim \\text{LN}\\left(\\mu, \\sigma^2 \\right)\\) and we have $$\\log Y \\sim \\mathcal{N}(\\mu,\\sigma^2).$$ Mean: \\(E\\left( Y\\right) =\\exp\\{\\mu+\\sigma^2/2\\}\\) Variance: \\(Var\\left( Y\\right) =\\exp\\{2\\mu+\\sigma^2\\}\\left(\\exp\\{\\sigma^2\\}-1\\right)\\) Skewness: \\(\\varsigma_Y= \\left(\\exp\\{\\sigma^2\\}+2\\right)\\left(\\exp\\{\\sigma^2\\}-1\\right)^{1/2}\\) Mgf: does not exist for \\(t\u0026gt;0\\).  Log-gamma #   We shall write \\(\\log Y\\sim \\Gamma\\left(\\gamma, c \\right)\\) and we have $$g\\left( y\\right) =\\frac{c ^{\\gamma }}{\\Gamma \\left( \\gamma \\right) }(\\log y)^{\\gamma -1}y^{-(c+1)}\\text{, for }y\u0026gt;0\\text{; }\\alpha ,\\beta\u0026gt;0\\text{.}$$ Mean: \\(E\\left( Y\\right) =\\left(\\frac{c}{c-1}\\right)^\\gamma\\) for \\(c\u0026gt;1\\) Variance: \\(Var\\left( Y\\right) =\\left(\\frac{c}{c-2}\\right)^\\gamma -\\mu_Y^2\\) for \\(c\u0026gt;2\\) Skewness: \\(\\varsigma_Y=\\left[ \\left(\\frac{c}{c-3}\\right)^\\gamma - 3\\mu_Y\\sigma_Y^2-\\mu_Y^3\\right]/ \\sigma_Y^3\\) Mgf: does not exist for \\(t\u0026gt;0\\). Higher moments: \\(E\\left( Y^{k}\\right) =\\left(\\frac{c}{c-k}\\right)^\\gamma\\) for \\(c\u0026gt;k\\) Special case: When \\(\\alpha =1\\), we have \\(Y\\sim Exp\\left(\\beta \\right)\\)  Pareto Distribution #   We shall write \\(Y\\sim Pareto\\left( \\theta, \\alpha \\right)\\) if density has the form $$g\\left( y\\right) = \\frac{\\alpha}{\\theta}\\left(\\frac{y}{\\theta}\\right)^{-(\\alpha+1)} \\quad\\text{for }\\alert{y\\ge \\theta}$$ Mean: \\(E\\left( Y\\right) =\\theta \\frac{\\alpha}{\\alpha-1}\\), \\(\\alpha\u0026gt;1\\) Variance: \\(Var\\left( Y\\right) =\\theta^2 \\frac{\\alpha}{(\\alpha-1)^2(\\alpha-2)}\\), \\(\\alpha\u0026gt;2\\) Skewness: \\(\\varsigma_Y=\\frac{2(1+\\alpha)}{\\alpha-3}\\left(\\frac{\\alpha-2}{\\alpha}\\right)^{1/2}\\), \\(\\alpha\u0026gt;3\\) Mgf: does not exist for \\(t\u0026gt;0\\) Translated Pareto: distribution of \\(Y=Y-\\beta\\)  Extreme Value Theory #  Note that the following topics (which appear briefly in MW 3) will be covered in Module 6 on Extreme Value Theory:\n regular variation at infinity and tail index (page 58-59+) Hill plot (page 75+) Generalised Pareto (‚ÄúGP‚Äù) and Generalised Extreme Value (‚ÄúGEV‚Äù) distributions  Fitting of distributions (MW3.2) #  Introduction #   By now, the modeller should have identified some candidate parametric distributions for their data set.  Example: for SUVA$dailyallow, based on data numerical and graphical explorations, we decided to try gamma, lognormal, and Weibull.   In order to be able to compare them, that is, assess their goodness of fit (which will be discussed in the next section), one must find numerical values for the parameters of the candidates. The parameter values we will choose will depend on some criterion, which the modeller can choose. In some cases, they may even want to follow different approaches and choose which is the one they think works best for their purposes.    Fitting criteria include:  moment matching: choose the \\(m\\) parameters such that the first \\(m\\) moments match maximum likelihood: choose the parameters such that the overall likelihood that the model generated the data is maximal \\(\\adv\\) maximum goodness: choose the parameters such that some goodness of fit criterion is maximised   We will discuss the first two here, and the third after the goodness of fit section (for obvious reasons).    There are other criteria which we will not discuss in details, such as:  quantile matching: choose parameters such that empirical quantiles match theoretical quantiles least squares: choose parameters such that the ‚Äúsum of squares‚Äù is minimal (this corresponds to maximum likelihood for Gaussian random variables) so-called ‚Äúminimum distance estimation,‚Äù which minimises distances between certain theoretical and empirical functions; see actuar::mde. For instance the actuar Cram√©r-von Mises method minimises the (weighted) distance between empirical and theoretical cdf‚Äôs score functions such as considered in probabilistic forecasting; see, e.g., package scoringRules in Jordan, Kr√ºger, and Lerch (2019)    R and technical notes #   The function MASS::fitdist is standard, and uses by default MLE via the optim function. The package fitdistrplus allows other fitting criteria (such as method of moments and maximum goodness), and also allows for the user to supply their own optimisation function. Distributions are coded in the following way. For distribution foo:  dfoo is the density (pdf) of foo pfoo is the distribution function (cdf) of foo qfoo is the quantile function of foo rfoo is the random number generator of foo    This link provides a very comprehensive list of available distributions in R. As a user you can define any distribution yourself, and this syntax will be recognised by some functions. The package actuar provides additional distributions. It also automates the transformation of such distribution functions in presence of left-truncation and right-censoring (more later).  Moment matching estimation (MME) #   This is quite straightforward; for a distribution of \\(m\\) parameters:  choose \\(m\\) moments you care about (usually the first \\(m\\) moments around the origin or the mean); build a system of \\(m\\) equations (for the \\(m\\) parameters) matching empirical moments of your choice; solve the system (might require a computer or some numerical techniques if the equations are not linear).   In R, set the argument method to mme in the call to fitdist  For distributions of 1 and 2 parameters, mean and variances are matched For a certain number of distributions a closed form formula is used. For the others, equations are solved numerically using optim and by minimising the sum of squared differences between theoretical and observed moments.    MME for SUVA #  fit.lnorm.mme \u0026lt;- fitdist(log(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0]), \u0026#34;lnorm\u0026#34;, method = \u0026#34;mme\u0026#34;, order = 1:2) fit.lnorm.mme$estimate ## meanlog sdlog ## 2.0146490 0.1871132  fit.lnorm.mme$loglik ## [1] -1959.06  fit.lnorm.mme2 \u0026lt;- fitdistrplus::mmedist(log(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0]), \u0026#34;lnorm\u0026#34;, order = 1:2) fit.lnorm.mme2$estimate ## meanlog sdlog ## 2.0146490 0.1871132  fit.lnorm.mme2$loglik ## [1] -1959.06   fit.gamma.mme \u0026lt;- fitdist(log(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0]), \u0026#34;gamma\u0026#34;, method = \u0026#34;mme\u0026#34;, order = 1:2) fit.gamma.mme$estimate ## shape rate ## 28.065081 3.678009  fit.gamma.mme$loglik ## [1] -1951.838  # function to calculate sample raw moment memp \u0026lt;- function(x, order) emm(x, order) fit.weibull.mme \u0026lt;- fitdist(log(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0]), \u0026#34;weibull\u0026#34;, method = \u0026#34;mme\u0026#34;, memp = memp, order = c(1, 2)) fit.weibull.mme$estimate ## shape scale ## 6.172910 8.212158  fit.weibull.mme$loglik ## [1] -2020.119  Maximum likelihood estimation (MLE) #   The likelihood for a statistical model gives an indication of how likely it is that this data set was generated, should the model be correct. If there is no censoring or truncation, we have Obviously, this is a function of the parameter (vector) \\(\\mathbf{\\theta }=\\left( \\theta _{1},...,\\theta_{m}\\right)\\), for a given set of observations, denoted \\(\\mathbf{x}=\\left(x_{1},...,x_{n}\\right)\\). The value \\(\\widehat{\\mathbf{\\theta }}\\) that maximises the likelihood is called the maximum likelihood estimator (MLE). Often, it is more convenient to maximise the log-likelihood function given by This avoids issues of underflow for \\(L\\) when \\(n\\) is large.   Note:\n The formulation of a likelihood in presence of left-tuncation and right-censoring will be covered in a later section. Sometimes, MLEs match MMEs (check book). MLEs have nice properties (such as asymptotic normality, which is one way to estimate standard errors; see later).  MLE for SUVA #  fit.lnorm.mle \u0026lt;- fitdist(log(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0]), \u0026#34;lnorm\u0026#34;, method = \u0026#34;mle\u0026#34;) fit.lnorm.mle$estimate ## meanlog sdlog ## 2.0140792 0.1918442  fit.lnorm.mle$loglik ## [1] -1958.359  fit.gamma.mle \u0026lt;- fitdist(log(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0]), \u0026#34;gamma\u0026#34;) fit.gamma.mle$estimate ## shape rate ## 27.825788 3.646718  fit.gamma.mle$loglik ## [1] -1951.818   fit.weibull.mle \u0026lt;- fitdist(log(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0]), \u0026#34;weibull\u0026#34;, method = \u0026#34;mle\u0026#34;) fit.weibull.mle$estimate ## shape scale ## 5.527347 8.231313  fit.weibull.mle$loglik ## [1] -2003.26  summary(fit.weibull.mle) ## Fitting of the distribution ' weibull ' by maximum likelihood ## Parameters : ## estimate Std. Error ## shape 5.527347 0.12127037 ## scale 8.231313 0.04760155 ## Loglikelihood: -2003.26 AIC: 4010.519 BIC: 4020.524 ## Correlation matrix: ## shape scale ## shape 1.0000000 0.3301744 ## scale 0.3301744 1.0000000  Parameter uncertainty #   The estimation of parameters is not perfect. The mere fact that different methods lead to different estimates makes that point obvious. This will always be true even if we are fitting the right distribution, just because we have only a finite sample of data How far can we be from the ‚Äútruth‚Äù ? There are different ways of answering that question, two of which we discuss here:  the Wald approximation: standard errors via the Hessian for MLEs bootstrap    Hessian matrix #   The score (or gradient) vector consists of first derivatives  $$\\begin{array} S\\left( \\mathbf{\\theta };\\mathbf{x}\\right) =\\left( \\frac{\\partial \\ell \\left( \\mathbf{\\theta };\\mathbf{x}\\right) }{\\partial \\theta_{1}},...,\\frac{\\partial \\ell \\left( \\mathbf{\\theta };\\mathbf{x}\\right) }{\\partial \\theta_{m}}\\right) ^{\\prime }, \\end{array}$$\nso that the MLE satisfies F.O.C. \\(S\\left(\\widehat{\\mathbf{\\theta}};\\mathbf{ x}\\right) =\\mathbf{0=}\\left(0,...,0\\right)^{\\prime}\\).\n The \\(m\\times m\\) Hessian matrix for \\(\\ell\\left(\\mathbf{\\theta};\\mathbf{x}\\right)\\) is defined by  $$\\begin{array} H\\left( \\mathbf{\\theta };\\mathbf{x}\\right) =\\frac{\\partial ^{2}\\ell \\left( \\mathbf{\\theta };\\mathbf{x}\\right) }{\\partial \\mathbf{\\theta}\\partial \\mathbf{\\theta }^{\\prime }} =\\begin{bmatrix} \\dfrac{\\partial ^{2}\\ell \\left( \\mathbf{\\theta };\\mathbf{x}\\right)}{\\partial \\theta _{1}^{2}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial ^{2}\\ell \\left(\\mathbf{\\theta };\\mathbf{x}\\right) }{\\partial \\theta _{1}\\partial\\theta _{m}} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\dfrac{\\partial ^{2}\\ell \\left( \\mathbf{\\theta };\\mathbf{x}\\right) }{\\partial \\theta _{m}\\partial \\theta _{1}} \u0026amp; \\cdots \u0026amp; \\dfrac{\\partial^{2}\\ell \\left( \\mathbf{\\theta };\\mathbf{x}\\right) }{\\partial \\theta_{m}^{2} } \\end{bmatrix}. \\end{array}$$\n This Hessian is used to estimate \\(Var\\left(\\widehat{\\mathbf{\\theta}}\\right)\\). Minus the expected value of this is called the (expected) Fisher information.  The Wald approximation #  It is well-known that a consistent estimator for the covariance matrix \\(Var\\left(\\widehat{\\mathbf{\\theta}}\\right)\\) is given by the inverse of the negative of this Hessian matrix:\n$$\\begin{array} \\text{Var}\\left(\\widehat{\\mathbf{\\theta}}\\right)\\ge \\widehat{\\text{Var}}\\left( \\widehat{\\mathbf{\\theta }}\\right) =\\left[-\\mathbf{E}[H\\left( \\widehat{\\mathbf{\\theta }};\\mathbf{x}\\right) ]\\right] ^{-1}. \\end{array}$$\nThe square root of the diagonal elements of this covariance estimate give the standard errors of the MLE estimates. Note:\n ‚ÄúConsistent‚Äù means it converges in probability to the value being estimated. When \\(n\\rightarrow \\infty\\), the distribution of \\(\\hat{\\theta}\\) is asymptotically normal. Note these are asymptotic results. Furthermore, their quality also strongly depends on the data, the distribution, and the parametrisation of the distribution.  Standard errors for SUVA data #  Note these are obviously unavailable for MMEs. For MLEs:\nfit.lnorm.mle$sd ## meanlog sdlog ## 0.005786951 0.004091492  summary(fit.lnorm.mle) ## Fitting of the distribution ' lnorm ' by maximum likelihood ## Parameters : ## estimate Std. Error ## meanlog 2.0140792 0.005786951 ## sdlog 0.1918442 0.004091492 ## Loglikelihood: -1958.359 AIC: 3920.717 BIC: 3930.722 ## Correlation matrix: ## meanlog sdlog ## meanlog 1 0 ## sdlog 0 1   fit.gamma.mle$sd ## shape rate ## 1.1799871 0.1560433  summary(fit.gamma.mle) ## Fitting of the distribution ' gamma ' by maximum likelihood ## Parameters : ## estimate Std. Error ## shape 27.825788 1.1799871 ## rate 3.646718 0.1560433 ## Loglikelihood: -1951.818 AIC: 3907.636 BIC: 3917.641 ## Correlation matrix: ## shape rate ## shape 1.00000 0.99103 ## rate 0.99103 1.00000   fit.weibull.mle$sd ## shape scale ## 0.12127037 0.04760155  summary(fit.weibull.mle) ## Fitting of the distribution ' weibull ' by maximum likelihood ## Parameters : ## estimate Std. Error ## shape 5.527347 0.12127037 ## scale 8.231313 0.04760155 ## Loglikelihood: -2003.26 AIC: 4010.519 BIC: 4020.524 ## Correlation matrix: ## shape scale ## shape 1.0000000 0.3301744 ## scale 0.3301744 1.0000000  \\(\\adv\\) Bootstrap #   It is advisable to compare the Wald approximation to the ones obtained using bootstrap procedures. Also, the Wald approximation assumes elliptical loglikelihood contours (related to the Gaussian asymptotic result), and hence having a look at the loglikelihood contours is also informative. More generally, one might want to simply use bootstrap to compute confidence intervals on parameters (or any function of those parameters). In R:  llplot will provide the loglikelihood contours; and bootdist will provide bootstrap results on a fitted object.    \\(\\adv\\) SUVA contours and bootstrap results #  llplot(fit.lnorm.mle)  fit.lnorm.mle.boot \u0026lt;- bootdist(fit.lnorm.mle, niter = 1001) fit.lnorm.mle.boot$fitpart # the Wald approximation ## Fitting of the distribution ' lnorm ' by maximum likelihood ## Parameters: ## estimate Std. Error ## meanlog 2.0140792 0.005786951 ## sdlog 0.1918442 0.004091492  summary(fit.lnorm.mle.boot) ## Parametric bootstrap medians and 95% percentile CI ## Median 2.5% 97.5% ## meanlog 2.0138122 2.0030576 2.0254507 ## sdlog 0.1916986 0.1831389 0.1997909  # CI to be compared with fit.lnorm.mle$estimate + cbind(estimate = 0, `2.5%` = -1.96 * fit.lnorm.mle$sd, `97.5%` = 1.96 * fit.lnorm.mle$sd) ## estimate 2.5% 97.5% ## meanlog 2.0140792 2.0027368 2.0254216 ## sdlog 0.1918442 0.1838249 0.1998635   plot(fit.lnorm.mle.boot)  llplot(fit.gamma.mle)  fit.gamma.mle.boot \u0026lt;- bootdist(fit.gamma.mle, niter = 1001) fit.gamma.mle.boot$fitpart # the Wald approximation ## Fitting of the distribution ' gamma ' by maximum likelihood ## Parameters: ## estimate Std. Error ## shape 27.825788 1.1799871 ## rate 3.646718 0.1560433  summary(fit.gamma.mle.boot) ## Parametric bootstrap medians and 95% percentile CI ## Median 2.5% 97.5% ## shape 27.858786 25.708293 30.215199 ## rate 3.649776 3.361932 3.949252  # CI to be compared with fit.gamma.mle$estimate + cbind(estimate = 0, `2.5%` = -1.96 * fit.gamma.mle$sd, `97.5%` = 1.96 * fit.gamma.mle$sd) ## estimate 2.5% 97.5% ## shape 27.825788 25.513014 30.138563 ## rate 3.646718 3.340873 3.952562   plot(fit.gamma.mle.boot)  llplot(fit.weibull.mle)  fit.weibull.mle.boot \u0026lt;- bootdist(fit.weibull.mle, niter = 1001) fit.weibull.mle.boot$fitpart # the Wald approximation ## Fitting of the distribution ' weibull ' by maximum likelihood ## Parameters: ## estimate Std. Error ## shape 5.527347 0.12127037 ## scale 8.231313 0.04760155  summary(fit.weibull.mle.boot) ## Parametric bootstrap medians and 95% percentile CI ## Median 2.5% 97.5% ## shape 5.533937 5.278860 5.806541 ## scale 8.229892 8.136889 8.319193  # CI to be compared with fit.weibull.mle$estimate + cbind(estimate = 0, `2.5%` = -1.96 * fit.weibull.mle$sd, `97.5%` = 1.96 * fit.weibull.mle$sd) ## estimate 2.5% 97.5% ## shape 5.527347 5.289657 5.765037 ## scale 8.231313 8.138014 8.324612   plot(fit.weibull.mle.boot) \\(\\adv\\) Confidence intervals on other quantities #   One can readily obtain confidence intervals on quantiles with the quantile function and a bootdist object More generally, this can be done of any function; see Delignette-Muller and Dutang (2015).   quantile(fit.gamma.mle.boot) ## (original) estimated quantiles for each specified probability (non-censored data) ## p=0.1 p=0.2 p=0.3 p=0.4 p=0.5 p=0.6 ## estimate 5.845174 6.394064 6.810817 7.181091 7.539156 7.908945 ## p=0.7 p=0.8 p=0.9 ## estimate 8.317716 8.81358 9.532902 ## Median of bootstrap estimates ## p=0.1 p=0.2 p=0.3 p=0.4 p=0.5 p=0.6 p=0.7 ## estimate 5.845315 6.395686 6.810459 7.180947 7.539751 7.90884 8.31787 ## p=0.8 p=0.9 ## estimate 8.814295 9.532867 ## ## two-sided 95 % CI of each quantile ## p=0.1 p=0.2 p=0.3 p=0.4 p=0.5 p=0.6 p=0.7 ## 2.5 % 5.742685 6.300067 6.722647 7.091769 7.447182 7.812046 8.215175 ## 97.5 % 5.938023 6.479399 6.889012 7.261597 7.624460 7.999226 8.417582 ## p=0.8 p=0.9 ## 2.5 % 8.699015 9.400036 ## 97.5 % 8.928283 9.676549  Dealing with left-truncation and right-censoring #  Coding of censored data in R #   We need two columns named left and right. The left column contains:  NA for left censored observations the left bound of the interval for interval censored observations the observed value for non-censored observations   The right column contains:  NA for right censored observations the right bound of the interval for interval censored observations the observed value for non-censored observations   This corresponds to the coding names interval2 in function Surv of the package survival   For example, consider twenty values from the canlifins of the CASdatasets package (see Delignette-Muller and Dutang (n.d.))\nexitage \u0026lt;- c(81.1, 78.9, 72.6, 67.9, 60.1, 78.3, 83.4, 66.9, 74.8, 80.5, 75.6, 67.1, 75.3, 82.8, 70.1, 85.4, 74, 70, 71.6, 76.5) death \u0026lt;- c(0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0)  the first value is someone who exited the study at 81.1, but not via death, so it is a right-censored observation this needs to be coded:  left: 81.1 right: NA     Overall, this becomes:\ncasdata \u0026lt;- cbind.data.frame(left = exitage, right = exitage) casdata$right[death == 0] \u0026lt;- NA # the censored values print(casdata) ## left right ## 1 81.1 NA ## 2 78.9 NA ## 3 72.6 72.6 ## 4 67.9 NA ## 5 60.1 NA ## 6 78.3 NA ## 7 83.4 NA ## 8 66.9 66.9 ## 9 74.8 NA ## 10 80.5 NA ## 11 75.6 NA ## 12 67.1 NA ## 13 75.3 NA ## 14 82.8 NA ## 15 70.1 70.1 ## 16 85.4 85.4 ## 17 74.0 NA ## 18 70.0 NA ## 19 71.6 NA ## 20 76.5 NA  (Note: How to use the function Surv() is explained in Delignette-Muller and Dutang, n.d.)\n Censored data can be plotted raw‚Ä¶\nplotdistcens(casdata, NPMLE = FALSE)  ‚Ä¶ or as an empirical distribution\nplotdistcens(casdata) Note that there are some technical subtleties with creating empirical distributions for censored data. This is out of scope, but details can be found in Delignette-Muller and Dutang (n.d.), Delignette-Muller and Dutang (2015), and references therein.\nFitting censored data #   Again, this is easily done with associated foocens functions  cas.gamma.fit \u0026lt;- fitdistcens(casdata, \u0026#34;gamma\u0026#34;) summary(cas.gamma.fit) ## Fitting of the distribution ' gamma ' By maximum likelihood on censored data ## Parameters ## estimate Std. Error ## shape 57.7413639 41.0308065 ## rate 0.6626575 0.5056793 ## Loglikelihood: -20.0179 AIC: 44.0358 BIC: 46.02727 ## Correlation matrix: ## shape rate ## shape 1.0000000 0.9983071 ## rate 0.9983071 1.0000000   Commands like cdfcompcens bootdistcens also exist. However, gofstat is not available as there do not exist results general enough to be coded in the package. Specific results for right-censored variables do exist, though.   plot(cas.gamma.fit) Left-truncation and R #   Unfortunately there is no pre-coded function for left-truncation. It can be done manually, with care. With left-truncation, the key (from definition) is that an observation will exist, if, and only if, it was beyond the truncation point. This means that the probability/likelihood associated to each observation is conditional on it being more than the truncation point. What follows is generalised to left- and right- truncation, and is taken from Delignette-Muller and Dutang (n.d.). For \\(X\\) before truncation, the \\(l\\)-left-truncated, \\(u\\)-right-truncated variable \\(Y\\) has density $$g_Y(y)= \\left\\{ \\begin{array}{cl} \\frac{g_X(x)}{G_X(u)-G_X(l)} \u0026amp; \\text{if }l\u0026lt;x\u0026lt;u \\\\ 0 \u0026amp; \\text{otherwise} \\end{array} \\right.$$   As an example in R, the d and p functions of a truncated exponential can be coded as:\ndtexp \u0026lt;- function(x, rate, low, upp) { PU \u0026lt;- pexp(upp, rate = rate) PL \u0026lt;- pexp(low, rate = rate) dexp(x, rate)/(PU - PL) * (x \u0026gt;= low) * (x \u0026lt;= upp) } ptexp \u0026lt;- function(q, rate, low, upp) { PU \u0026lt;- pexp(upp, rate = rate) PL \u0026lt;- pexp(low, rate = rate) (pexp(q, rate) - PL)/(PU - PL) * (q \u0026gt;= low) * (q \u0026lt;= upp) + 1 * (q \u0026gt; upp) }  If we generate 200 such truncated variables:\nset.seed(22042021) n \u0026lt;- 200 # number of observations x \u0026lt;- rexp(n) # simulating sample of x\u0026#39;s y \u0026lt;- x[x \u0026gt; 0.5 \u0026amp; x \u0026lt; 3] # truncating to get sample of y\u0026#39;s and then fit them with either left- and right- truncation estimated from the data:\nfit.texp.emp \u0026lt;- fitdist(y, \u0026#34;texp\u0026#34;, method = \u0026#34;mle\u0026#34;, start = list(rate = 3), fix.arg = list(low = min(y), upp = max(y))) ‚Ä¶ or in an informative way (i.e.¬†we know what the bounds are):\nfit.texp.inf \u0026lt;- fitdist(y, \u0026#34;texp\u0026#34;, method = \u0026#34;mle\u0026#34;, start = list(rate = 3), fix.arg = list(low = 0.5, upp = 3))  gofstat(list(fit.texp.emp, fit.texp.inf), fitnames = c(\u0026#34;unknown bounds\u0026#34;, \u0026#34;known bounds\u0026#34;)) ## Goodness-of-fit statistics ## unknown bounds known bounds ## Kolmogorov-Smirnov statistic 0.07546318 0.06747339 ## Cramer-von Mises statistic 0.13211330 0.09999122 ## Anderson-Darling statistic Inf 0.59863722 ## ## Goodness-of-fit criteria ## unknown bounds known bounds ## Akaike's Information Criterion 165.3424 169.7301 ## Bayesian Information Criterion 168.1131 172.5007   cdfcomp(list(fit.texp.emp, fit.texp.inf), legendtext = c(\u0026#34;unknown bounds\u0026#34;, \u0026#34;known bounds\u0026#34;)) Fitting a single level of left-trancated and right-censored data #   For our purposes, we shall represent our set of observations as $$\\left( t_{j},x_{j},\\delta_{j}\\right)$$ where  \\(t_{j}\\) is the left truncation point; \\(x_{j}\\) is the claim value that produced the data point; and \\(\\delta_{j}\\) is indicator whether limit has been reached.   For example:  \\(\\left( 50,250,0\\right)\\) \\(\\left( 100,1100,1\\right)\\)     Densities will be as follows:\n$$ L\\left( \\mathbf{\\theta };\\mathbf{x}\\right) =\\prod\\nolimits_{j}\\left[ \\frac{ f\\left( x_{j};\\mathbf{\\theta }\\right) }{1-F\\left( t_{j};\\mathbf{\\theta } \\right) }\\right] ^{1-\\delta_{j}}\\cdot \\prod\\nolimits_{j}\\left[ \\frac{ 1-F\\left( x_{j};\\mathbf{\\theta }\\right) }{1-F\\left( t_{j};\\mathbf{\\theta } \\right) }\\right] ^{\\delta_{j}}. $$\nThe contribution to the likelihood function for a data point where the limit has not been reached is given by\n$$ \\left[ \\frac{f\\left( x_{j}\\right) }{1-F\\left( t_{j}\\right) }\\right] ^{1-\\delta_{j}}. $$\nThe contribution to the likelihood function¬†for a data point where the limit has been reached is given by\n$$ \\left[ \\frac{1-F\\left( x_{j}\\right) }{1-F\\left( t_{j}\\right) }\\right] ^{\\delta_{j}}. $$\nNote here that the policy limit if reached would be equal to \\(x_{j}-t_{j}\\).\n  In R, the approach will be to code a left-truncated function, and then use the foocens functions. Let us do this on a gamma distribution:  dtgamma \u0026lt;- function(x, rate, shape, low) { PL \u0026lt;- pgamma(low, rate = rate, shape = shape) dgamma(x, rate = rate, shape = shape)/(1 - PL) * (x \u0026gt;= low) } ptgamma \u0026lt;- function(q, rate, shape, low) { PL \u0026lt;- pgamma(low, rate = rate, shape = shape) (pgamma(q, rate = rate, shape = shape) - PL)/(1 - PL) * (q \u0026gt;= low) }  Simulating one such dataset:\nset.seed(22042021) n \u0026lt;- 2000 # number of observations x \u0026lt;- rgamma(n, shape = 2, rate = 0.2) # simulating sample of x\u0026#39;s x \u0026lt;- x[x \u0026gt; 2] # left-truncation at 2 n - length(x) # observations that were truncated ## [1] 123  censoring \u0026lt;- x \u0026gt; 20 # we will censor at 20 x[x \u0026gt; 20] \u0026lt;- 20 # censoring at 20  Transforming this into the right format:\ncensoring[censoring == FALSE] \u0026lt;- x[censoring == FALSE] censoring[censoring == TRUE] \u0026lt;- NA xcens \u0026lt;- cbind.data.frame(left = x, right = censoring) And fitting:\n# Not allowing for truncation: fit.gamma.xcens \u0026lt;- fitdistcens(xcens, \u0026#34;gamma\u0026#34;, start = list(shape = mean(xcens$left)^2/var(xcens$left), rate = mean(xcens$left)/var(xcens$left))) summary(fit.gamma.xcens) ... ## Fitting of the distribution ' gamma ' By maximum likelihood on censored data ## Parameters ## estimate Std. Error ## shape 2.7153083 0.089141065 ## rate 0.2566683 0.009500217 ## Loglikelihood: -5412.521 AIC: 10829.04 BIC: 10840.12 ...   # Allowing for truncation fit.tgamma.xcens \u0026lt;- fitdistcens(xcens, \u0026#34;tgamma\u0026#34;, start = list(shape = mean(xcens$left)^2/var(xcens$left), rate = mean(xcens$left)/var(xcens$left)), fix.arg = list(low = min(xcens$left))) summary(fit.tgamma.xcens) ... ## Fitting of the distribution ' tgamma ' By maximum likelihood on censored data ## Parameters ## estimate Std. Error ## shape 2.0296237 0.10529978 ## rate 0.2006534 0.01009728 ## Fixed parameters: ## value ## low 2.013575 ## Loglikelihood: -5340.151 AIC: 10684.3 BIC: 10695.38 ...   cdfcompcens(list(fit.gamma.xcens, fit.tgamma.xcens), legendtext = c(\u0026#34;Not allowing for truncation\u0026#34;, \u0026#34;Allowing for truncation\u0026#34;)) Fitting multiple levels of left-trancated and right-censored data #  In real life, an insurance product would have more than one level of deductibles and limits to suit different policyholders. Simulating another dataset:\nset.seed(2022) n \u0026lt;- 3006 # number of observations x \u0026lt;- rgamma(n, shape = 2, rate = 0.2) # simulating sample of x\u0026#39;s deductibles \u0026lt;- rep(c(rep(1, 3), rep(3, 3), rep(5, 3)), 334) limits \u0026lt;- rep(c(15, 20, 30), 3 * 334) + deductibles Manually applying the deductibles and limits:\ncensored \u0026lt;- x \u0026gt; limits # we will censor at the limits x[censored] \u0026lt;- limits[censored] # censoring at the limits deducted \u0026lt;- x \u0026gt; deductibles x \u0026lt;- x[deducted] # left-truncation at all points n - length(x) # observations that were truncated ## [1] 431  claims \u0026lt;- data.frame(x = x, deduct = deductibles[deducted], limitI = censored[deducted]) Preliminary analysis: (note that there are no claims less than 1)\nclaims \u0026lt;- claims[sample(1:nrow(claims), nrow(claims)), ] # randomising data head(claims) ## x deduct limitI ## 444 17.196879 1 FALSE ## 1733 13.964488 3 FALSE ## 1414 3.521077 1 FALSE ## 1407 15.488936 1 FALSE ## 1138 11.509672 3 FALSE ## 1977 4.875571 3 FALSE  hist(claims$x, breaks = 100) Preparing our joint log-likelihood function. Here, we are maximising a negative log-likelihood instead of minimising a log-likelihood.\nnegloglik \u0026lt;- function(pdf, cdf, param, x, deduct, limitI) { # Function returns the negative log likelihood of the # censored and truncated dataset. Each data point\u0026#39;s # contribution to the log likelihood depends on the # theoretical distirbution pdf and cdf and also the # deductible and limit values to adjust for truncation # and censoring PL \u0026lt;- do.call(cdf, c(list(q = deduct), param)) PX \u0026lt;- do.call(cdf, c(list(q = x), param)) fX \u0026lt;- do.call(pdf, c(list(x = x), param)) lik.contr \u0026lt;- ifelse(limitI, log(1 - PX), log(fX)) - log(1 - PL) return(-sum(lik.contr)) } Fitting the distribution. Note that our objective function needs starting values for the maximisation. What other starting values could we use?\npdf \u0026lt;- dgamma cdf \u0026lt;- pgamma x \u0026lt;- claims$x deduct \u0026lt;- claims$deduct limitI \u0026lt;- claims$limitI # MME for starting values start \u0026lt;- list(shape = mean(x)^2/var(x), rate = mean(x)/var(x)) obj.fun \u0026lt;- function(shape, rate) { param \u0026lt;- list(shape = shape, rate = rate) return(negloglik(pdf, cdf, param, x, deduct, limitI)) } gamma.ll.fit \u0026lt;- stats4::mle(obj.fun, start = start, lower = c(0, 0)) summary(gamma.ll.fit) ## Length Class Mode ## 1 mle S4  param.g.ll \u0026lt;- stats4::coef(gamma.ll.fit) param.g.ll ## shape rate ## 2.1297568 0.2111871  Model selection (MW 3.3) #  Graphical approaches #  For judging quality of model, do some graphical comparisons:\n histogram vs.¬†fitted parametric density function; empirical CDF vs fitted parametric CDF; probability-probability (P-P) plot - theoretical vs empirical cumulative probabilities; quantile-quantile (Q-Q) plot - theoretical vs sample quantiles.  Let the (theoretical) fitted parametric distribution be denoted by \\(G\\left( x;\\widehat{\\mathbf{\\theta }}\\right)\\).\nP-P plot #  To construct the P-P plot:\n order the observed data from smallest to largest: \\(x_{(1)},x_{(2)},...,x_{(n)}\\). calculate the theoretical CDF at each of the observed data points: \\(G\\left( x_{(i)};\\widehat{\\mathbf{\\theta }}\\right)\\). for \\(i=1,2,...,n\\), plot the points \\(\\dfrac{i-0.5}{n}\\) against \\(G\\left(x_{(i)};\\widehat{\\mathbf{\\theta }}\\right)\\).  Note that using \\(\\dfrac{i-0.5}{n}\\) this is Hazen‚Äôs rule, as recommended by Blom (1959).\nsee also the following video on YouTube:\n  About the choice of \\((i-0.5)/n\\) in Q-Q and P-P plots  Q-Q plot #  To construct the Q-Q plot:\n order the observed data from smallest to largest: \\(x_{(1)},x_{(2)},...,x_{(n)}\\). for \\(i=1,2,...,n\\), calculate the theoretical quantiles: \\(G^{-1}\\left(\\dfrac{i-0.5}{n};\\widehat{\\mathbf{\\theta }}\\right)\\). for \\(i=1,2,...,n\\), plot the points \\(x_{(i)}\\) against \\(G^{-1}\\left(\\dfrac{i-0.5}{n};\\widehat{\\mathbf{\\theta }}\\right)\\).  These constructions hold only for the case where you have no censoring/truncation.\nsee also the following video on YouTube:\n  How to build Q-Q plots and P-P plots  SUVA GOF graphs #  plot.legend \u0026lt;- c(\u0026#34;Weibull\u0026#34;, \u0026#34;lognormal\u0026#34;, \u0026#34;gamma\u0026#34;) fitdistrplus::denscomp(list(fit.weibull.mle, fit.lnorm.mle, fit.gamma.mle), legendtext = plot.legend, fitlwd = 3)  fitdistrplus::cdfcomp(list(fit.weibull.mle, fit.lnorm.mle, fit.gamma.mle), legendtext = plot.legend, fitlwd = 4, datapch = 20)  fitdistrplus::ppcomp(list(fit.weibull.mle, fit.lnorm.mle, fit.gamma.mle), legendtext = plot.legend, fitpch = 20)  fitdistrplus::qqcomp(list(fit.weibull.mle, fit.lnorm.mle, fit.gamma.mle), legendtext = plot.legend, fitpch = 20) Hypothesis tests #  We will test the null\n \\(H_{0}:\\) data came from population with the specified model, against \\(H_{a}:\\) the data did not come from such a population.  Some commonly used tests, and their test statistics:\n Kolmogorov-Smirnoff: $$K.S.=\\underset{y}{\\sup }\\left\\vert \\widehat{G} \\left( y\\right) -G\\left( y;\\widehat{\\mathbf{\\theta }}\\right)\\right\\vert$$ Anderson-Darling: $$A.D.=n \\int \\frac{\\left[ \\widehat{G}\\left( y\\right)-G\\left( y;\\widehat{\\mathbf{\\theta }}\\right) \\right] ^{2}}{G\\left(y; \\widehat{\\mathbf{\\theta }}\\right) \\left[ 1-G\\left(y;\\widehat{\\mathbf{\\theta }}\\right) \\right] } dy$$ \\(\\chi ^{2}\\) goodness-of-fit: $$\\chi^{2}=\\sum_{j}\\frac{\\left(\\text{observed}-\\text{expected}\\right) ^{2}}{\\text{expected}}$$  Computational formulas for those tests are available in Delignette-Muller and Dutang (2015).\nKolmogorov-Smirnoff test #  $$K.S.=\\underset{y}{\\sup }\\left\\vert \\widehat{G} \\left( x\\right) -G\\left( x;\\widehat{\\mathbf{\\theta }}\\right) \\right\\vert,$$ where\n \\(\\widehat{G}\\left( y\\right)\\) is the empirical distribution \\(G\\left( y;\\theta \\right)\\) is the assumed theoretical distribution in the null hypothesis \\(G\\left( y;\\theta \\right)\\) is assumed to be (must be) continuous \\(\\hat{\\theta}\\) is the maximum likelihood estimate for \\(\\theta\\) under the null hypothesis  Note:\n There are tables for the critical values. There are several variations of these tables in the literature that use somewhat different scalings for the K-S test statistic and critical regions. The test (obviously) does not work for grouped data.  Anderson-Darling test #  $$A.D.=n\\int \\frac{\\left[ \\widehat{G}\\left( x\\right)-G\\left( x;\\widehat{\\mathbf{\\theta }}\\right) \\right] ^{2}}{G\\left(x; \\widehat{\\mathbf{\\theta }}\\right) \\left[ 1-G\\left( x;\\widehat{\\mathbf{\\theta }}\\right) \\right] } dx,$$ where \\(n\\) is the sample size.\nNote:\n The critical values for the Anderson-Darling test are dependent on the specific distribution that is being tested. There are tabulated values and formulas for a few specific distributions. The theoretical distribution is assumed to be (must be) continuous. The test does not work for grouped data.  \\(\\chi ^{2}\\) goodness-of-fit test #  Procedure:\n Break down the whole range into \\(k\\) subintervals: \\(c_0\u0026lt;c_1\u0026lt;\\cdots\u0026lt;c_k=\\infty\\) \\(\\chi ^{2}\\) goodness-of-fit test: \\(\\chi ^{2}=\\sum_{j}^k\\frac{(E_j-O_j)^{2}}{E_j}\\) Let \\(\\hat{p_j}=G(c_j;\\hat{\\theta})-G(c_{j-1};\\hat{\\theta})\\). Then, the number of expected observations in the interval \\((c_{j-1},c_j]\\) assuming that the hypothesized model is true: $$E_j=n\\hat{p}_j \\quad \\text{(Here, } n \\text{ is the sample size)}$$ Let \\({p_j}=\\hat{G}(c_j)-\\hat{G}(c_{j-1})\\). Then, the number of observations in the interval \\((c_{j-1},c_j]\\) : $$ O_j=n{p}_j$$  The statistic has chi-square distribution with the degree of freedom equal to: \\(k-1- \\alert{\\text{number of parameters estimated}}\\)\nHow these tests are used #   Besides testing whether data came from specified model or not, generally we would prefer models with:  lowest K-S test statistic lowest A-D test statistic lowest \\(\\chi ^{2}\\) goodness-of-fit test statistic (or equivalently highest \\(p\\)-value) highest value of the likelihood function at the maximum   Perform formal statistical test, or use as `horse race‚Äô  Comparison #   K-S and A-D tests are quite similar - both look at the difference between the empirical and model distribution functions.  K-S in absolute value, A-D in squared difference. But A-D is weighted average, with more emphasis on good fit in the tails than in the middle; K-S puts no such emphasis.   For K-S and A-D tests, no adjustments are made to account for increase in the number of parameters, nor sample size. Result: more complex models often will fare better on these tests. For K-S and A-D tests, no adjustments are made to account for sample size. Result: large sample size increases probability of rejecting all models. The \\(\\chi ^{2}\\) test adjusts the degrees of freedom for increases in the number of parameters, but is easily manipulated as the choice of brackets is arbitrary.  Information criteria #   Information criteria penalise the log likelihood with a function that depends on the number of parameters. Akaike Information Criterion (AIC): $$\\text{AIC}^{(i)}=-2\\ell^{(i)}_\\mathbf{Y}+2d^{(i)},$$ where \\(d^{(i)}\\) denotes the number of estimated parameters in the density \\(g_i\\) that is considered, and where \\(\\ell^{(i)}_\\mathbf{Y}\\) is the maximum log likelihood that can be achieved with that density. Bayesian Information Criterion (BIC) $$\\text{BIC}^{(i)}=-2\\ell^{(i)}_\\mathbf{Y}+\\log (n) d^{(i)}.$$ This is \\(-2\\text{SBC}\\), where SBC is Schwarz Bayesian Criterion.    Note that these can sometimes be presented in inverse sign. Here, because the penalty is added, it is obvious that smaller values of the IC are preferred.  SUVA GOF stats #  For MLE:\ngofstat(list(fit.weibull.mle, fit.lnorm.mle, fit.gamma.mle), fitnames = plot.legend) ## Goodness-of-fit statistics ## Weibull lognormal gamma ## Kolmogorov-Smirnov statistic 0.07105097 0.04276791 0.03376236 ## Cramer-von Mises statistic 1.74049707 0.26568341 0.19438834 ## Anderson-Darling statistic 10.69572021 1.70209314 1.10385675 ## ## Goodness-of-fit criteria ## Weibull lognormal gamma ## Akaike's Information Criterion 4010.519 3920.717 3907.636 ## Bayesian Information Criterion 4020.524 3930.722 3917.641   For MME:\ngofstat(list(fit.weibull.mme, fit.lnorm.mme, fit.gamma.mme), fitnames = plot.legend) ## Goodness-of-fit statistics ## Weibull lognormal gamma ## Kolmogorov-Smirnov statistic 0.08023228 0.0374087 0.0327241 ## Cramer-von Mises statistic 1.66608962 0.1869752 0.1823197 ## Anderson-Darling statistic 10.53097690 1.4369047 1.0539353 ## ## Goodness-of-fit criteria ## Weibull lognormal gamma ## Akaike's Information Criterion 4044.239 3922.121 3907.677 ## Bayesian Information Criterion 4054.243 3932.125 3917.681  Note that when fitting discrete distributions, the chi-squared statistic is computed by the gofstat function (see Delignette-Muller and Dutang (2015) for details).\nGOF Actual hypothesis test results #  R can also provide the results from the GOF hypothesis tests. For instance:\ngammagof \u0026lt;- gofstat(list(fit.gamma.mle, fit.lnorm.mle), fitnames = c(\u0026#34;gamma MLE\u0026#34;, \u0026#34;lognormal MLE\u0026#34;), chisqbreaks = c(10:20/2)) gammagof$chisqpvalue ## gamma MLE lognormal MLE ## 1.374256e-03 1.690633e-05  gammagof$adtest ## gamma MLE lognormal MLE ## \u0026quot;rejected\u0026quot; \u0026quot;not computed\u0026quot;  gammagof$kstest ## gamma MLE lognormal MLE ## \u0026quot;not rejected\u0026quot; \u0026quot;rejected\u0026quot;   gammagof$chisqtable ## obscounts theo gamma MLE theo lognormal MLE ## \u0026lt;= 5 36 23.96251 19.19169 ## \u0026lt;= 5.5 28 39.64865 39.53512 ## \u0026lt;= 6 60 72.77895 76.73318 ## \u0026lt;= 6.5 110 109.47716 116.38286 ## \u0026lt;= 7 130 139.03193 145.08202 ## \u0026lt;= 7.5 191 152.62949 154.45787 ## \u0026lt;= 8 134 147.62688 144.65495 ## \u0026lt;= 8.5 141 127.77796 121.96771 ## \u0026lt;= 9 91 100.25469 94.30131 ## \u0026lt;= 9.5 62 72.07629 67.84850 ## \u0026lt;= 10 51 47.91524 45.97086 ## \u0026gt; 10 65 65.82026 72.87394  GOF graphical comparisons #  plot.legend \u0026lt;- c(\u0026#34;lognormal MLE\u0026#34;, \u0026#34;lognormal MME\u0026#34;) fitdistrplus::denscomp(list(fit.lnorm.mle, fit.lnorm.mme), legendtext = plot.legend, fitlwd = 3)  fitdistrplus::cdfcomp(list(fit.lnorm.mle, fit.lnorm.mme), legendtext = plot.legend, fitlwd = 4, datapch = 20)  fitdistrplus::ppcomp(list(fit.lnorm.mle, fit.lnorm.mme), legendtext = plot.legend, fitpch = 20)  fitdistrplus::qqcomp(list(fit.lnorm.mle, fit.lnorm.mme), legendtext = plot.legend, fitpch = 20)  plot.legend \u0026lt;- c(\u0026#34;lognormal MLE\u0026#34;, \u0026#34;lognormal MME\u0026#34;) fitdistrplus::denscomp(list(fit.gamma.mle, fit.gamma.mme), legendtext = plot.legend, fitlwd = 3)  fitdistrplus::cdfcomp(list(fit.gamma.mle, fit.gamma.mme), legendtext = plot.legend, fitlwd = 4, datapch = 20)  fitdistrplus::ppcomp(list(fit.gamma.mle, fit.gamma.mme), legendtext = plot.legend, fitpch = 20)  fitdistrplus::qqcomp(list(fit.gamma.mle, fit.gamma.mme), legendtext = plot.legend, fitpch = 20) \\(\\adv\\) Other advanced topics #  \\(\\adv\\) Alternative methods for estimation #  \\(\\adv\\) Maximum Goodness Estimation (MGE) #   This is one form of ‚Äúminimum distance estimation,‚Äù whereby parameters are chosen such that a distance (between empirical and theoretical) is minimised Here we focus on the GOF tests AD, CvM, and KS This can be readily chosen in R using fitdist, with method=\u0026quot;mge\u0026quot; and where gof= one of AD, CvM, or KS.   With the SUVA data:\nfit.gamma.mge.ad \u0026lt;- fitdist(log(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0]), \u0026#34;gamma\u0026#34;, method = \u0026#34;mge\u0026#34;, gof = \u0026#34;AD\u0026#34;) fit.gamma.mge.ks \u0026lt;- fitdist(log(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0]), \u0026#34;gamma\u0026#34;, method = \u0026#34;mge\u0026#34;, gof = \u0026#34;KS\u0026#34;) gof.mge.legend \u0026lt;- c(\u0026#34;gamma MLE\u0026#34;, \u0026#34;gamma MGE AD\u0026#34;, \u0026#34;gamma MGE KS\u0026#34;) gofstat(list(fit.gamma.mle, fit.gamma.mge.ad, fit.gamma.mge.ks), fitnames = gof.mge.legend) ## Goodness-of-fit statistics ## gamma MLE gamma MGE AD gamma MGE KS ## Kolmogorov-Smirnov statistic 0.03376236 0.02841676 0.0208791 ## Cramer-von Mises statistic 0.19438834 0.14533386 0.1444858 ## Anderson-Darling statistic 1.10385675 0.96063188 1.6991376 ## ## Goodness-of-fit criteria ## gamma MLE gamma MGE AD gamma MGE KS ## Akaike's Information Criterion 3907.636 3908.736 3919.475 ## Bayesian Information Criterion 3917.641 3918.740 3929.480   denscomp(list(fit.gamma.mle, fit.gamma.mge.ad, fit.gamma.mge.ks), legendtext = gof.mge.legend, fitlwd = 3)  cdfcomp(list(fit.gamma.mle, fit.gamma.mge.ad, fit.gamma.mge.ks), legendtext = gof.mge.legend, fitlwd = 4, datapch = 20)  ppcomp(list(fit.gamma.mle, fit.gamma.mge.ad, fit.gamma.mge.ks), legendtext = gof.mge.legend, fitpch = 20)  qqcomp(list(fit.gamma.mle, fit.gamma.mge.ad, fit.gamma.mge.ks), legendtext = gof.mge.legend, fitpch = 20) \\(\\adv\\) AD based parameter estimation #   Recall the Anderson-Darling test statistic, which focused on the tails. What if we wanted to generalise the idea of AD to left tail or right tail only, or put even more weight on either of those tails? The AD test considers a weighted ‚Äúsum‚Äù (integral) of the squared difference between empirical and theoretical cdf‚Äôs In its original formulation, the weight is of the form $$ w_i= \\frac{1}{G(y) [1-G(y)]},$$ which goes to infinity when \\(y\\rightarrow 0\\) or \\(y\\rightarrow \\infty\\). There are 5 alternatives in fitdistrplus.    ADR: Right-tail AD, where $$w_i = \\frac{1}{1-G(y)};$$ ADL: Left-tail AD, where $$w_i = \\frac{1}{G(y)};$$ ADR2: Right-tail AD, 2nd order, where $$w_i = \\frac{1}{[1-G(y)]^2};$$ ADL2: Left-tail AD, 2nd order, where $$w_i = \\frac{1}{[G(y)]^2};$$ AD2: AD, 2nd order. Here, R will minimise ADR2 + ADL2.   With the SUVA data:\nfit.gamma.mge.adr \u0026lt;- fitdist(log(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0]), \u0026#34;gamma\u0026#34;, method = \u0026#34;mge\u0026#34;, gof = \u0026#34;ADR\u0026#34;) fit.gamma.mge.adr2 \u0026lt;- fitdist(log(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0]), \u0026#34;gamma\u0026#34;, method = \u0026#34;mge\u0026#34;, gof = \u0026#34;AD2R\u0026#34;) gof.mge.legend2 \u0026lt;- c(\u0026#34;gamma MGE AD\u0026#34;, \u0026#34;gamma MGE ADR\u0026#34;, \u0026#34;gamma MGE ADR2\u0026#34;)  gofstat(list(fit.gamma.mge.ad, fit.gamma.mge.adr, fit.gamma.mge.adr2), fitnames = gof.mge.legend2) ## Goodness-of-fit statistics ## gamma MGE AD gamma MGE ADR ## Kolmogorov-Smirnov statistic 0.02841676 0.0309970 ## Cramer-von Mises statistic 0.14533386 0.1553754 ## Anderson-Darling statistic 0.96063188 0.9799768 ## gamma MGE ADR2 ## Kolmogorov-Smirnov statistic 0.03980158 ## Cramer-von Mises statistic 0.24760685 ## Anderson-Darling statistic 1.38154307 ## ## Goodness-of-fit criteria ## gamma MGE AD gamma MGE ADR ## Akaike's Information Criterion 3908.736 3908.223 ## Bayesian Information Criterion 3918.740 3918.228 ## gamma MGE ADR2 ## Akaike's Information Criterion 3908.049 ## Bayesian Information Criterion 3918.054   denscomp(list(fit.gamma.mge.ad, fit.gamma.mge.adr, fit.gamma.mge.adr2), legendtext = gof.mge.legend2, fitlwd = 3)  cdfcomp(list(fit.gamma.mge.ad, fit.gamma.mge.adr, fit.gamma.mge.adr2), legendtext = gof.mge.legend2, fitlwd = 4, datapch = 20)  ppcomp(list(fit.gamma.mge.ad, fit.gamma.mge.adr, fit.gamma.mge.adr2), legendtext = gof.mge.legend2, fitpch = 20)  qqcomp(list(fit.gamma.mge.ad, fit.gamma.mge.adr, fit.gamma.mge.adr2), legendtext = gof.mge.legend2, fitpch = 20) \\(\\adv\\) Quantile matching #   Quantile matching is easily implemented in R:  set method=\u0026quot;qme\u0026quot; in the call to fitdist; and add an argument probs defining the probabilities for which the quantile matching is performed.   The number of quantiles to match must be the same as the number of parameters to estimate. The quantile matching is carried out numerically, by minimising the sum of squared differences between observed and theoretical quantiles.  For example:\nfit.gamma.qme \u0026lt;- fitdist(log(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0]), \u0026#34;gamma\u0026#34;, method = \u0026#34;qme\u0026#34;, probs = c(0.5, 0.75)) fit.lnorm.qme \u0026lt;- fitdist(log(SUVA$dailyallow[SUVA$dailyallow \u0026gt; 0]), \u0026#34;lnorm\u0026#34;, method = \u0026#34;qme\u0026#34;, probs = c(0.5, 0.75)) gof.qme.legend \u0026lt;- c(\u0026#34;gamma MLE\u0026#34;, \u0026#34;gamma QME\u0026#34;, \u0026#34;lognormal QME\u0026#34;)  denscomp(list(fit.gamma.mle, fit.gamma.qme, fit.lnorm.qme), legendtext = gof.qme.legend, fitlwd = 3)  cdfcomp(list(fit.gamma.mle, fit.gamma.qme, fit.lnorm.qme), legendtext = gof.qme.legend, fitlwd = 4, datapch = 20)  ppcomp(list(fit.gamma.mle, fit.gamma.qme, fit.lnorm.qme), legendtext = gof.qme.legend, fitpch = 20)  qqcomp(list(fit.gamma.mle, fit.gamma.qme, fit.lnorm.qme), legendtext = gof.qme.legend, fitpch = 20) \\(\\adv\\) Zero-inflated severity model \\(X=IB\\) #  In this approach \\(X=IB\\), where\n \\(I\\) is an indicator of claim with $$\\Pr\\left[ I=1\\right] =q\\text{ and }\\Pr\\left[ I=0\\right] =1-q$$ \\(B\\) is the {claim amount given \\(I=1\\)} (given a claim occurs).  This allows us to avoid a large probability mass at 0 for rare losses.\n  Note that this is a good approach for modifying continuous distributions, which are generally used for severity. In the case of discrete distributions‚Äîusually used for frequency, modifications of the usual distributions (in the form of ‚Äúzero-truncated‚Äù or ‚Äúzero-modified‚Äù) are well known, and readily available in the package actuar. This is discussed in Module 2. In practice a frequency and severity model would be chosen at the same time, and the way zero claims are dealt with should be determined in a consistent way, e.g.:  ‚Äúfrequency‚Äù models strictly positive claims, and ‚Äúseverity‚Äù is a strictly positive continuous distribution; ‚Äúfrequency‚Äù models insurable events (which may lead to claims of 0), and ‚Äúseverity‚Äù includes a mass at 0 (such as in this section); etc‚Ä¶    \\(\\adv\\) Resulting distribution #  As a consequence,\n$$\\begin{array}{rcl} \\Pr\\left[ X\\leq x \\right] \u0026amp;=\u0026amp;\\Pr\\left[ X\\leq x | I=0\\right]\\Pr[I=0]\\\\\u0026amp;\u0026amp;+\\Pr\\left[ X\\leq x | I=1\\right]\\Pr[I=1] \\\\ \u0026amp;=\u0026amp;1-q+q\\Pr\\left[ B\\leq x \\right] \\end{array}$$ and\n$$\\begin{array}{rcl} M_X(t)\u0026amp;=\u0026amp;E[e^{tX}|I=0]\\Pr[I=0]\\\\ \u0026amp;\u0026amp;+E[e^{tX}|I=1]\\Pr[I=1]\\\\ \u0026amp;=\u0026amp;1-q+q E[e^{tB}]. \\end{array}$$\n\\(\\adv\\) Mean and Variance #  The mean can be determined using $$E\\left[ X\\right] =E\\left[ E\\left[ X\\left\\vert I\\right. \\right]\\right] =E\\left[ X\\left\\vert I\\right. =1\\right] \\Pr\\left[ I=1\\right]=qE\\left( B\\right),$$ after noting that \\(E\\left[ X\\left\\vert I\\right. =0\\right] =0\\).\nThe variance can be determined using\n$$\\begin{array}{rcl} Var\\left( X\\right) \u0026amp;=\u0026amp;Var\\left( E\\left[ X\\left\\vert I\\right.\\right] \\right)+E\\left[ Var\\left( X\\left\\vert I\\right. \\right) \\right] \\\\ \u0026amp;=\u0026amp;\\left[ E\\left( B\\right) \\right] ^{2}Var\\left( I\\right)+qVar\\left(B\\right) \\\\ \u0026amp;=\u0026amp;q\\left( 1-q\\right) \\left( E\\left[ B\\right] \\right)^{2}+qVar\\left( B\\right) \\end{array}$$\nafter noting that\n$$\\begin{array}{rcl} E[ X|I] \u0026amp;=\u0026amp;I \\cdot E[ B],\\text{ and that} \\\\ Var( X| I)\u0026amp;=\u0026amp;I^2 \\cdot Var(B), \\end{array}$$\nwhich are both random variables (functions of \\(I\\)).\n\\(\\adv\\) Special case: \\(X=Ib\\) #   Fixed claim: \\(B=b\\) with probability 1. The individual claim random variable becomes $$X=Ib=\\left\\{ \\begin{array}{ll} b, \u0026amp; \\text{w.p. }q \\\\ 0, \u0026amp; \\text{w.p. }1-q \\end{array} \\right. \\text{.}$$ Mean: \\(E\\left[ X\\right] =bq\\) Variance: \\(Var\\left( X\\right) =b^{2}Var\\left( I\\right)=b^{2}q\\left( 1-q\\right)\\)  This is nothing more than a scaled Bernoulli‚Ä¶ (and if you add them, the sum becomes a scaled Binomial)\n\\(\\adv\\) Example: Bicycle Theft (get \\(B\\) out of \\(X\\)) #   Insurance policy against bicycle theft (insured amount is 400) Only half is paid if bicycle is not locked. Assume: \\(\\Pr\\left[ X=400\\right] =0.05\\) and \\(\\Pr\\left[X=200\\right] =0.15\\). Probability of a claim: \\(q=\\Pr\\left[ I=1\\right] =0.20\\) [law of total probability] The pmf of \\(B\\) is computed in the following way  $$\\begin{array}{rcl} \\Pr\\left[ B=400 \\right] \u0026amp;=\u0026amp;\\Pr[ X=400 | I=1] = \\frac{\\Pr[X=400 \\ \\cap\\ I=1]}{\\Pr[I=1]}\\\\ \u0026amp;=\u0026amp;\\frac{0.05}{0.20} = 0.25 \\end{array}$$\n\\(\\adv\\) Example #  In an insurance portfolio, there are 15 insured:\n ten of the insured persons have 0.1 probability of making a claim, and the other 5 have a 0.2 probability of making a claim.  All claims are independent and follow an exponential distribution with mean \\(1/\\lambda\\). What is the mgf of the aggregate claims distribution?\n Let \\(X_i\\) be the total amount of claims incurred from the \\(i\\)th person, and \\(B_i\\) denote the amount of claim, if there is one. Then \\(X_i=I_iB_i\\).\nIf \\(q_i=\\Pr(I_i=1)\\) then \\(q_1=\\cdots=q_{10}=0.1\\) and \\(q_{11}=\\cdots=q_{15}=0.2.\\)\nThe mgf of \\(X_i\\):\n$$\\begin{array}{rcl} M_{X_i}(t)\u0026amp;=\u0026amp;E[e^{tX_i}|I_i=0]\\Pr(I_i=0)+E[e^{tX_i}|I_i=1]\\Pr(I_i=1) \\\\ \u0026amp;=\u0026amp;1-q_i+E[e^{tB_i}]q_i=1-q_i+\\frac{\\lambda}{\\lambda-t}q_i \\end{array}$$\nSince the aggregate claims are \\(S=X_1+\\cdots+X_{15}\\) the mgf of \\(S\\) is\n$$\\begin{array}{rcl} M_{S}(t)\u0026amp;=\u0026amp;\\prod_{i=1}^{15}E[e^{tX_i}] \\\\ \u0026amp;=\u0026amp;\\left(1-0.1+0.1\\frac{\\lambda}{\\lambda-t}\\right)^{10}\\left(1-0.2+0.2\\frac{\\lambda}{\\lambda-t}\\right)^{5} \\end{array}$$\nCalculating within layers for claim sizes (MW 3.4) #  Usual policy transformations #  Deductible and Policy Limit #  One way to control the cost (and variability) of individual claim losses is to introduce deductibles and policy limits.\n Deductible \\(d\\): the insurer starts paying claim amounts above the deductible \\(d\\) Limit \\(M\\): the insurer pays up to the limit \\(M\\).  If we denote the damage random variable by \\(D\\), then if a claim occurs the insurer is liable for\n$$\\begin{array} X=\\min \\left[ \\max \\left( D-d,0\\right) ,M\\right] \\text{.} \\end{array}$$\nReinsurance #  Reinsurance is a risk transfer from an insurer (the direct writer) to a reinsurer:\n in other words, some of the (random) risk faced by the insurer is ‚Äútransfered‚Äù to the reinsurer (that means the reinsurer will cover that risk), in exchange of a (deterministic) premium (which will obviously generally be higher than the expected value of the risk that was transferred) the risk that the insurer keeps is called the retention   There are different types of reinsurance:\n proportional  quota share: the proportion is the same for all risks surplus: the proportion can vary from risk to risk   nonproportional  (individual) excess of loss: on each individual loss \\((X_i)\\) stop loss: on the aggregate loss \\((S)\\)   cheap (reinsurance premium is the expected value), or non cheap (reinsurance premium is loaded) Alternative Risk Transfers (‚ÄúART‚Äù), where usually the idea is to transfer the risk to different / deeper pockets. For instance, for transfers to the financial markets:  Catastrophe bonds (‚ÄúCAT bonds‚Äù) Longevity bonds Pandemic bonds    Proportional reinsurance #  The retained proportion \\(\\alpha\\) defines who pays what:\n the insurer pays \\(Y=\\alpha X\\) the reinsurer pays \\(Z=(1-\\alpha) X\\)  This is nothing else but a change of scale and we have $$\\mu_Y=\\alpha \\mu_X,\\;\\;\\;\\sigma^2_Y=\\alpha^2\\sigma^2_X,\\;\\;\\;\\gamma_Y=\\gamma_X.$$ In some cases it suffices to adapt the scale parameter. Example:\n If \\(X\\) is exponential with parameter \\(\\beta\\) $$\\Pr[Y\\le y]=\\Pr[\\alpha X \\le y]=\\Pr[X \\le y/\\alpha] = 1-e^{-\\beta y / \\alpha}$$ and thus \\(Y\\) is exponential with parameter \\(\\beta/\\alpha\\).  Nonproportional reinsurance #  Basic arrangements:\n the reinsurer pays the excess over a retention (excess point) \\(d\\)  the insurer pays \\(Y=\\min(X,d)\\) the reinsurer pays \\(Z=(X-d)_+\\)  \\(E[(X-d)_+]\\) is called stop-loss premium.     the reinsurer limits his payments to an amount \\(M\\). In that case  the insurer pays \\(Y=\\min(X,d)+(X-M-d)_+\\) the reinsurer pays \\(Z=\\min\\left\\{(X-d)_+,M\\right\\}\\)    Example #  Consider a life insurance company with \\(16,\\!000\\) 1-year term life insurance policies. The associated insured amounts are:\n   Benefit (10000‚Äôs) # policies}     1 8000   2 3500   3 2500   5 1500   1 \u0026amp;500    The probability of death \\((q)\\) for each of the \\(16,\\!000\\) lives is 0.02. This company has an EoL reinsurance contract with retention limit \\(30,\\!000\\) at a cost of 0.025 per dollar of coverage.\nWhat is the approximate probability (using CLT) that the total cost will exceed \\(8,\\!250,\\!000\\)?\n The portfolio of retained business is given by\n   \\(k\\) retained benefit \\(b_k\\) (10000‚Äôs) # policies \\(n_k\\)     1 1 8000   2 2 3500   3 3 4500    Now\n$$\\begin{array}{rcl} E[S]\u0026amp;=\u0026amp;\\sum_{k=1}^3 n_k E[X_k]=\\sum_{k=1}^3 n_k b_kq_k \\\\ \u0026amp;=\u0026amp; 8000\\cdot1\\cdot 0.02+3500\\cdot2\\cdot 0.02+4500\\cdot3\\cdot 0.02 \\\\ \u0026amp;=\u0026amp;570,\\quad\\text{ and} \\\\ Var[S]\u0026amp;=\u0026amp;\\sum_{k=1}^3 n_k Var[X_k]=\\sum_{k=1}^3 n_k b_k^2q_k(1-q_k) \\\\ \u0026amp;=\u0026amp; 8000\\cdot1^2\\cdot 0.02\\cdot 0.98+3500\\cdot2^2\\cdot 0.02\\cdot 0.98 \\\\ \u0026amp;\u0026amp;+4500\\cdot3^2\\cdot 0.02\\cdot 0.98\\\\\u0026amp;=\u0026amp;1225 \\end{array}$$\n The reinsurance cost is $$[(5-3)\\cdot 1500+(10-3)\\cdot 500] \\cdot 0.025=162.5.$$\nThus, the desired probability becomes\n$$\\begin{array}{rcl} \\Pr[S+162.5\u0026gt;825]\u0026amp;=\u0026amp;\\Pr\\left[\\frac{S-E[S]}{\\sqrt{Var(S)}}\u0026gt;\\frac{662.5-E[S]}{\\sqrt{Var(S)}}\\right] \\\\ \u0026amp;\\approx\u0026amp; \\Pr\\left[Z\u0026gt;\\frac{662.5-570}{\\sqrt{1225}}\\right] \\\\ \u0026amp;=\u0026amp;\\Pr[Z\u0026gt;2.643]=0.0041. \\end{array}$$\n Discussion:\n Without reinsurance, exp/var is 700/2587.20 so the associated probability of shortfall is \\(\\approx \\Pr[Z\u0026gt;2.458]\\), which is higher even though it is not cheap reinsurance. However, there is lower expected gain:  With reinsurance the expected gain is $$P-570-162.5=P-732.5$$ Without reinsurance it is $$P-700,$$ which is higher.    \\(\\adv\\) The actuar coverage function #   The package actuar allows for direct specification of the pdf of a modified random variable after possible left-trunction and right-censoring. Given the pdf or cdf of the original loss \\(D\\), coverage returns a function object to compute the pdf or cdf of the modified random variable after one or several of the following modifications:  ordinary deductible \\(d\\); franchise deductible \\(d\\); limit \\(u\\); coinsurance \\(\\alpha\\); inflation \\(r\\).   The vignette on loss modeling features of actuar provides precise definitions, and this document summarises all the formulas.   Assume that insurance payments are $$X = \\left\\{ \\begin{array}{cc} D-d, \u0026amp; d\\le D\\le u \\\\ u-d, \u0026amp; D\\ge u \\end{array}\\right.$$ with mixed distribution $$f_X(y) = \\left\\{ \\begin{array}{cc} 0, \u0026amp; x=0 \\\\ \\frac{f_D(y+d)}{1-F_D(d)}, \u0026amp; 0\u0026lt; x \u0026lt; u-d \\\\ \\frac{1-F_D(u)}{1-F_D(d)}, \u0026amp; x=u-d \\\\ 0 \u0026amp; x\u0026gt;u-d \\end{array}\\right.$$ as seen before. Note however that the \\(u\\) is expressed on the raw variable \\(D\\), not the payment, so that the maximum payment is \\(u-d\\).\n If \\(D\\) is gamma, \\(d=2\\), and \\(u=20\\), one can get \\(f_X(x)\\) in R as follows:\nf \u0026lt;- coverage(pdf = dgamma, cdf = pgamma, deductible = 2, limit = 20) The function can be then used to fit distributions to data.\nAs an example we will use the previously generated data xcens. Note it needs to be shifted by \\(d\\) down, because what we have is the left-truncated and right-censored \\(D\\), not the insurance payments as per the formulation above.\n fit.gamma.xcens2 \u0026lt;- fitdistr(xcens$left - 2, f, start = list(shape = mean(xcens$left)^2/var(xcens$left), rate = mean(xcens$left)/var(xcens$left))) fit.gamma.xcens2 ## shape rate  ## 2.03990496 0.20142871  ## (0.10516140) (0.01009694) fit.tgamma.xcens # our previous fit with fitdist ## Fitting of the distribution \u0026#39; tgamma \u0026#39; on censored data by maximum likelihood  ## Parameters: ## estimate ## shape 2.0296237 ## rate 0.2006534 ## Fixed parameters: ## value ## low 2.013575 c(fit.gamma.xcens2$loglik, fit.tgamma.xcens$loglik) ## [1] -5341.551 -5340.151 Note that this is using MASS::fitdistr rather than the (arguably more flexible and possibly advanced) fitdistrplus::fitdist. This approach works as well, but does not seem as precise in this particular instance.\nA useful identity #  Note that $$\\min(X,c)=X-(X-c)_+$$ and thus $$E[\\min(X,c)]=E[X]-E[(X-c)_+].$$ The amount \\(E[(X-c)_+]=\\Pr[X\u0026gt;c]e(c)\\)\n is commonly called ‚Äústop loss premium‚Äù with retention \\(c\\). is identical to the expected payoff of a call with strike price \\(c\\), and thus results from financial mathematics can sometimes be directly used (and vice versa).  Stop loss premiums #  Let $$E[(X-d)_+]=P_d.$$ Then we have (for positive rv‚Äôs) $$P_d=\\left\\{ \\begin{array}{ll}\\int_d^\\infty \\left[1-F_X(x)\\right] dx \u0026amp; \\text{if }X\\text{ is continuous} \\\\ \\sum_d^\\infty \\left[1-F_X(x)\\right] \u0026amp; \\text{if }X\\text{ is discrete} \\end{array}\\right.$$\nExample #  Calculate \\(P_d\\) if \\(X\\) is Exponential with mean \\(1/\\beta\\).\n\\(\\adv\\) Recursive formulas in the discrete case #  First moment:\n if \\(d\\) is an integer $$P_{d+1}=P_d-[1-F_X(d)]\\text{ with }P_0=E[X]$$ if \\(d\\) is not an integer $$P_d=P_{\\lfloor d \\rfloor}-(d-\\lfloor d \\rfloor)[1-F_X(\\lfloor d \\rfloor)].$$ Second moment \\(P^2_d=E[(X-d)_+^2]\\): $$P_{d+1}^2=P_{d}^2-2P_{d}+[1-F_X(d)]\\text{ with }P_0^2=E[X^2].$$  Note that \\(\\lfloor x \\rfloor\\) is the integer part of \\(x\\) (e.g.¬†\\(\\lfloor 2.5 \\rfloor=2\\)).\n\\(\\adv\\) Numerical example #  For the distribution \\(F_{1+2+3}\\) derived in Module 2 we have \\(E[S]=4=128/32\\) and \\(E[S^2]=19.5=624/32\\) and thus\n   \\(d\\) \\(f_{1+2+3}(d)\\) \\(F_{1+2+3}(d)\\) \\(P_d\\) \\(P_d^2\\) \\(Var((X-d)_+)\\)     \\(0\\) \\(1/32\\) \\(1/32\\) 128/32 624/32 3.500   \\(1\\) \\(2/32\\) \\(3/32\\) 97/32 399/32 3.280   \\(2\\) \\(4/32\\) \\(7/32\\) 68/32 234/32 2.797   \\(3\\) \\(6/32\\) \\(13/32\\) 43/32 123/32 2.038   \\(4\\) \\(6/32\\) \\(19/32\\) 24/32 56/32 1.188   \\(5\\) \\(6/32\\) \\(25/32\\) 11/32 21/32 0.538   \\(6\\) \\(4/32\\) \\(29/32\\) 4/32 6/32 0.172   \\(7\\) \\(2/32\\) \\(31/32\\) 1/32 1/32 0.030   \\(8\\) \\(1/32\\) \\(32/32\\) 0 0 0.000    $$P_{2.6}=P_2-0.6\\cdot(1-F_{1+2+3}(2))=53/32.$$\nLeverage effect of claims inflation #  Choose a fixed deductible \\(d \u0026gt;0\\) and assume that the claim at time 0 is given by \\(Y_0\\). Assume that there is a deterministic inflation index \\(i \u0026gt; 0\\) such that the claim at time 1 can be represented by \\(Y_1 =(1+i)Y_0\\).We have $$E[(Y_1-d)_+] \\ge (1+i)E[(Y_0-d)_+].$$ When tax brackets are not adapted, this leads to the so-called ‚Äúbracket creep‚Äù‚Ä¶\nReferences #  Avanzi, Benjamin, Luke C. Cassar, and Bernard Wong. 2011. ‚ÄúModelling Dependence in Insurance Claims Processes with L√©vy Copulas.‚Äù ASTIN Bulletin 41 (2): 575‚Äì609.\n Blom, Gunnar. 1959. Statistical Estimates and Transformed Beta Variables. John Wiley \u0026amp; Sons.\n Delignette-Muller, Marie Laure, and Christophe Dutang. 2015. ‚ÄúFitdistrplus: An r Package for Fitting Distributions.‚Äù Journal of Statistical Software 64 (4).\n ‚Äî‚Äî‚Äî. n.d. ‚ÄúPackage ‚ÄòFitdistrplus‚Äò: Frequently Asked Questions.‚Äù\n Jordan, Alexander, Fabian Kr√ºger, and Sebastian Lerch. 2019. ‚ÄúEvaluating Probabilistic Forecasts with ‚ÄòscoringRules‚Äò.‚Äù Journal of Statistical Software 90 (12).\n Wuthrich, Mario V. 2020. ‚ÄúNon-Life Insurance: Mathematics \u0026amp; Statistics.‚Äù Lecture notes. RiskLab, ETH Zurich; Swiss Finance Institute.\n  "},{"id":10,"href":"/docs/0-prerequisite-knowledge/","title":"Prerequisite Knowledge","section":"Docs","content":"Summary of some of the prerequisite knowledge required for this subject: The first two parts ( Mathematics and Probability) below are basic reminders, and could probably be skipped by most students.\nThe third part is more advanced and corresponds to Section 1.2 of the prescribed textbook MW.\nStudents are recommended to review all those concepts prior to week 1, and to ask questions at the week 1 tutorial if needed.\nMathematics #  Functions and their derivatives #   Be familiar with functions \\(x^\\alpha, e^{\\alpha x}, \\ln(1+x)\\) Basic derivatives: $$(x^\\alpha)' = \\alpha x^{\\alpha - 1}$$ $$(e^{\\alpha x})' = \\alpha e^{\\alpha x}$$ $$(\\ln(1+x))' = \\frac{1}{1+x}$$ $$(a^x)' = a^x \\ln(a)$$  Taylor's expansion (here for an exponential random variable): $$e^x = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + ... + \\frac{x^n}{n!} + ...$$  Be able to find expressions for following summations #  $$\\sum_{i=1}^n x^i, \\sum_{i=1}^n ix^i, \\sum_{i=1}^n i$$ See Tutorial 0 (Revisions) for solutions.\nChange the order of double summation #  $$\\sum_{k=1}^n \\sum_{j=1}^k a_{k,j} = \\sum_{j=1}^n \\sum_{k=j}^n a_{k,j} $$\nThe solution to a quadratic equation #  The equation $$ax^2 +bx+c = 0$$ has two solutions:\n$$x_1 = \\frac{-b+\\sqrt{b^2-4ac}}{2a}, \\quad\\quad b^2 \u0026gt; 4ac$$ $$x_2 = \\frac{-b-\\sqrt{b^2-4ac}}{2a}, \\quad\\quad b^2 \u0026gt; 4ac$$\nIf \\(b^2 - 4ac\\) = 0, the equation has a double solution: \\(x = \\frac{-b}{2a}\\)\nExample #  For example, find a value for \\(v\\) such that \\(v \\in (0,1)\\) and \\(v\\) satisfies the equation: \\(v - 2v^{0.5} + \\dfrac{3}{4} = 0\\).\nSolution: let \\(v^{\\frac{1}{2}} = x\\). Then the equation above simplifies to \\(x^2 - 2x +\\frac{3}{4} = 0\\) which has two solutions:\n$$x_1 = \\frac{2+\\sqrt{4-3}}{2} = 1.5, \\ \\ \\ \\ \\ \\ \\ \\ x_2 = \\frac{2-\\sqrt{4-3}}{2} = 0.5$$\nWe reject the solution \\(x_1 = 1.5\\) as \\(x_1 \u0026gt; 1\\). Then \\(v = x_2^2 = 0.5^2 = 0.25\\) is the required solution.\nBe able to solve simple differential equations #  Example 1 #  For example, solve $$f'(x) = 2x$$ with initial condition \\(f(0)=1\\).\nSolution:\n\\begin{align*} f(x) =\u0026amp; f(0) + \\int_0^x f'(t) dt \\\\ =\u0026amp; 1 + \\int_0^x 2t dt \\\\ =\u0026amp;1 + t^2 \\large |_0^x \\\\ =\u0026amp; 1+x^2 \\end{align*}\nExample 2 #  Solve $$f'(x) = 2f(x)$$ with initial condition \\(f(0)=1\\).\nSolution:\n$$\\frac{f'(x)}{f(x)} = 2 \\Longleftrightarrow (\\ln f(x))' = 2$$. $$\\Rightarrow \\ln f(x) - \\ln f(0) = \\int_0^x (\\ln f(t))' dt = \\int_0^x 2 dt = 2x$$ $$\\Rightarrow \\ln f(x) = 2x \\Rightarrow f(x) = e^{2x}$$\nIntegrals #   We have $$\\int_a^b f(x) dx = F(b) - F(a),$$ where \\(F(x)\\) is the anti-derivative of \\(f(x)\\), such that \\(F'(x) = f(x)\\). The integration variable is just a tool, that is, $$\\int_a^b f(x) dx = \\int_a^b f(y) dy,$$ it does not matter to use \\(x\\) or \\(y\\). We have $$$\\int_0^{\\infty} f(x) dx = \\lim\\limits_{b \\rightarrow \\infty} \\int_0^{b} f(x) dx = \\lim\\limits_{b \\rightarrow \\infty} F(b) - F(0).$$ For example: \\begin{align*} \\int_0^{\\infty} e^{-x} dx =\u0026amp; \\lim_{b \\rightarrow \\infty} \\int_0^{b} e^{-x} dx \\\\ =\u0026amp; \\lim_{b \\rightarrow \\infty} \\left(-e^{-x} |_0^b\\right ) \\\\ =\u0026amp; 1- \\lim_{b \\rightarrow a} e^{-b} \\\\ =\u0026amp; 1 \\end{align*}  Integration by parts: \\begin{align*} \\int_a^{b} f(x)g'(x) dx =\u0026amp; f(x)g(x) |_a^b - \\int_a^{b} g(x)f'(x) dx \\\\ =\u0026amp; f(b)g(b) - f(a)g(a) - \\int_a^{b} g(x)f'(x) dx \\end{align*}  The average of a function on \\([a,b]\\) #  Let \\(f(x)\\) be a continuous function on \\([a,b]\\). Then there exists a point \\(c \\in [a,b]\\) such that $$\\int_a^{b} f(x) dx = f(c)(b-a), \\ \\ \\ \\ \\ \\ \\ \\ c\\in[a,b]$$\nInterpretation: \\(\\int_a^{b} f(x) dx\\) is the area of the region enclosed by \\(f(x)\\), \\(x\\)-axis, \\(x=a\\), \\(x=b\\). \\(f(c)(b-a)\\) is the area of the rectangle of height \\(f(c)\\) and length \\((b-a)\\).\nDefinition: \\(\\frac{\\int_a^{b} f(x) dx}{b-a} = f(c)\\): the average value of \\(f(x)\\) on \\([a,b]\\) interval.\nExample 1: #  $$\\frac{\\int_{-1}^{1} x dx}{2} = 0$$\nThe average value of \\(f(x) = x\\) on \\([-1,1]\\) is 0.\nExample 2: #  $$\\frac{\\int_{0}^{1} x dx}{1-0} = \\frac{1}{2}$$\nThe average value of \\(f(x) = x\\) on \\([0,1]\\) is \\(\\frac{1}{2}\\).\nExample 3: #  $$\\frac{\\int_{-1}^{1} x^2 dx}{2} = \\frac{1}{3}$$ The average value of \\(f(x) = x^2\\) on \\([-1,1]\\) is \\(\\frac{1}{3}\\).\nThe trapezoid rule in integration #  $$\\int_{a}^{b} f(x) dx \\approx \\frac{1}{2}[f(b)+f(a)] (b-a)$$ $$\\Longleftrightarrow \\frac{\\int_{a}^{b} f(x) dx}{b-a} \\approx \\frac{1}{2}[f(b)+f(a)]$$\nThe average value of \\(f(x)\\) on \\([a,b]\\) can be approximated by \\(\\frac{1}{2}[f(b)+f(a)]\\).\nThe definition of \\(\\int_{a}^{b} f(x) dx\\) and its numerical calculations #  $$\\int_{a}^{b} f(x) dx = \\lim_{n \\rightarrow \\infty} \\sum_{k=0}^{n-1} f(x_k)\\frac{b-a}{n}, \\quad\\quad (1)$$ where \\(x_0 = a, x_1 = x_0 + \\frac{b-a}{n}, ..., x_{k+1} = x_k + \\frac{b-a}{n},..., x_n = b\\)\nIn the summation in \\((1)\\), each term represents the area of a rectangle. \\(f(x_k)\\frac{b-a}{n}\\) represents the area of the \\(k\\)-th rectangle.\nApproximations:\n \\(\\int_{a}^{b} f(x) dx \\approx f(a)(b-a) \\ \\ \\ \\ \\ \\ \\ (n=1)\\) \\(\\int_{a}^{b} f(x) dx \\approx \\frac{b-a}{2} \\left[f(a) + f\\left(\\frac{b+a}{2}\\right)\\right]\\) \\(\\int_{a}^{b} f(x) dx \\approx (b-a)\\frac{f(x_0)+f(x_1)+...+f(x_{n-1})}{n}\\): the average of \\(f(x_0), f(x_1), ..., f(x_{n-1})\\) times \\((b-a)\\). If \\(a=0, b=1\\), \\(\\int_{0}^{1} f(x) dx \\approx \\frac{f(x_0)+f(x_1)+...+f(x_{n-1})}{n}\\).  Alternatively,\n$$\\int_{a}^{b} f(x) dx = \\lim_{n \\rightarrow \\infty} \\sum_{k=1}^{n} f(x_k)\\frac{b-a}{n} \\quad\\quad (2),$$ where \\(x_1 = x_0 +\\frac{b-a}{n}, x_2 = x_1 +\\frac{b-a}{n}, ..., x_n = b\\).\nApproximations:\n \\(\\int_{a}^{b} f(x) dx \\approx f(b)(b-a)\\) \\(\\int_{a}^{b} f(x) dx \\approx \\frac{b-a}{2} \\left[f\\left(\\frac{b+a}{2}\\right)+ f(b)\\right]\\) \\(\\int_{a}^{b} f(x) dx \\approx (b-a)\\frac{f(x_1)+f(x_2)+...+f(x_{n})}{n}\\)  The average number of \\(n\\) numbers #  Let \\(x_1, x_2, ..., x_n\\) be \\(n\\) numbers. Then $$\\frac{x_1+...+x_n}{n} = \\frac{1}{n} \\sum_{i=1}^n x_i$$ is the average value of \\(x_1,x_2,...,x_n\\).\nExample 1 #  The average value of \\(1,2,...,n\\) is $$\\frac{1}{n} \\sum_{i=1}^n i = \\frac{1}{n}\\frac{n(n+1)}{2} = \\frac{n+1}{2},$$ where \\(1+2+3+...+n = \\frac{n(n+1)}{2}\\) is given in Tutorial 0.\nExample 2 #  One student took 8 subjects in his first year at University of Melbourne. The results are as follows: Semester 1: 75, 83, 65, 90; Semester 2: 60, 76, 80, 50.\nThen\n \\(75+83+65+90+60+76+80+50 = 579\\) is the total marks from year 1. The average mark is \\(\\frac{579}{8} = 72.4\\) The average mark for S1 is \\(\\frac{75+83+65+90}{4} = 78.2\\) The average mark for S2 is \\(\\frac{60+76+80+50}{4} = 66.5\\)  The weighted average of \\(n\\) numbers #  Let \\(x_1, x_2, ..., x_n\\) be \\(n\\) real numbers.\nLet \\(\\theta_1, \\theta_2, ...., \\theta_n\\) be \\(n\\) numbers such that $$0 \\leq \\theta_i \\leq 1 \\quad\\text{ and }\\quad \\sum_{i=1}^n \\theta_i = 1.$$ Then $$\\sum_{i=1}^n \\theta_i x_i$$ is called the weighted average of \\(x_1, x_2, ..., x_n\\).\nNote:\n \\(\\theta_i\\) is the weight attached to \\(x_i\\). if \\(\\theta_i =\\frac{1}{n}\\), then $$\\sum_{i=1}^n \\frac{1}{n} x_i = \\frac{1}{n} \\sum_{i=1}^n x_i$$ is the average of \\(x_1, x_2, ..., x_n\\) (equally weighted).  Example #  In the assessment of ACTL10001, the assignments account for 20%, the mid-semester exam accounts for 10%, and the final exam accounts for 70%. A student got 70 out of 100 for mid-semester result, 95 out of 100 for assignments, and 80 for final exam. Then the overall weighted average mark is $$70 \\times 10\\% + 95 \\times 20\\% + 80 \\times 70\\% = 82.$$\nProbability #  The following contents are the object of a video recorded in August 2021 for the subject ACTL10001 Introduction to Actuarial Studies: annotated pdf\n If you wish to watch the embedded videos from Lecture Capture, you need to have logged in and entered Lecture Capture via Canvas once for each session. This is to restrict access to students enrolled at the University of Melbourne only. Events and Probability #  Vocabulary: events vs probability #  It is important to understand the difference between events and probability:\n Event: what could happen - an actual ‚Äúthing,‚Äù in real life, that could happen; Probability: our understanding of the ‚Äúlikelihood‚Äù (or frequency) of an event (something that could occur).  So when we are building a mathematical model for uncertain outcomes:\n The first step is to work out what are all the possible things that could occur (for instance, ‚Äúrain‚Äù or ‚Äúno rain‚Äù). The full set of those is denoted \\(\\Omega\\). The second step is to make assumptions about how likely those things can occur. Here ‚Äù \\(\\Pr\\) ‚Äù is an operator that maps an event into a probability. For instance, \\(\\Pr[\\text{rain}]=0.2\\) means that the likelihood corresponding to the event ‚Äúrain‚Äù is 20%.  In what follows we outline basic results and axioms around events and their probabilities. Often logic means that a result or definition on one side (e.g.¬†events) can be translated on the other side (e.g.¬†probabilities).\nFor instance, the complement to an event is exactly whatever could happen, that is not the event. Hence, the probability of the complement must be 1 minus the probability of the original event; see 2.1.2.4 below.\nEvents, operations of events, probability of an event #   \\(\\emptyset\\): empty set, that it, it is an impossible event: $$\\Pr(\\emptyset) = 0.$$ \\(\\Omega\\): the full set of possible outcomes, that is, it is a certain event: $$\\Pr(\\Omega) = 1.$$ \\(A\\): an event (within \\(\\Omega\\)), \\(0 \\leq \\Pr(A) \\leq 1\\). \\(A^C\\): the event that \\(A\\) does not occur (called a ‚Äúcomplement‚Äù): $$\\Pr(A^C) = 1 - \\Pr(A).$$ \\(A \\cap B\\): \\(A\\) and \\(B\\), the event that both \\(A\\) and \\(B\\) occur. \\(A \\cup B\\): \\(A\\) or \\(B\\), the event that either \\(A\\) or \\(B\\), or both events occur. \\(A \\subseteq B\\): If \\(A\\) occurs, \\(B\\) must, and:  \\(\\Pr(A) \\leq \\Pr(B)\\); \\(A \\cap B = A\\).\nExample: \\(A\\) = {a 20-year old survives to age 70}, \\(B\\) = {the 20-year old survives to age 50}. Then \\(A \\subseteq B\\).    Mutually exclusive events #  If $$A \\cap B = \\emptyset,$$ then \\(A\\) and \\(B\\) are mutually exclusive. Also,\n$$\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B).$$\nIndependent events \\(A\\) and \\(B\\) #  If \\(A\\) and \\(B\\) are independent, then $$\\Pr(A \\cap B) = \\Pr(A)\\Pr(B).$$\nConditional probability formula #  We have\n$$\\Pr(A|B) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}.$$\nThis leads to Bayes‚Äô theorem, see for instance this.\nAlso,\n If \\(A \\subseteq B\\), then \\(A \\cap B = A\\) and $$\\Pr(A|B) = \\frac{\\Pr(A)}{\\Pr(B)}.$$ If \\(A\\) and \\(B\\) are independent, then $$\\Pr(A|B) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)} = \\frac{\\Pr(A)\\Pr(B)}{\\Pr(B)} = \\Pr(A).$$ If \\(B \\subseteq A\\), then \\(A \\cap B = B\\) and $$\\Pr(A|B) = \\frac{\\Pr(B)}{\\Pr(B)} = 1.$$ Given \\(B\\) has occurred, \\(A\\) is certain.  Random variables and their distribution #  Definition #  A random variable, denoted by capital letters \\(X, Y, Z\\), is a quantity whose value is subject to variations due to chance.\nDistribution Function #  Definition: $$F(x) = \\Pr(X \\leq x) \\ \\ \\ \\ \\ \\ \\ \\ x\\in \\mathbb{R}$$ \\(F(x)\\) is called the distribution function of \\(X\\), and it has the following properties:\n \\(F(-\\infty) = 0, F(\\infty) = 1\\). \\(F(x_1) \\leq F(x_2)\\), if \\(x_1 \\leq x_2\\). \\(F(x)\\) is right-continuous (aka ‚Äúc√†dl√†g‚Äù), i.e., \\(\\lim_{x \\rightarrow x_0^+} F(x) = F(x_0)\\). \\(F(b) - F(a) = \\Pr (a \u0026lt; X \\leq b)\\). \\(F(b-) - F(a) = \\Pr (a \u0026lt; X \u0026lt; b)\\). \\(F(b) - F(a-) = \\Pr (a \\leq X \\leq b)\\). \\(F(b) - F(b-) = \\Pr (X = b) \\ge 0\\)  In our subject, we generally assume that \\(X \\geq 0\\) so that \\(F(x) = 0\\) for \\(x\u0026lt;0\\).\nDifference betweeen continuous and discrete random variables #  As an introduction to differences between continuous and discrete random variables, review this video:\n Continuous random variables ( \\(X \\geq 0\\) ) #  \\(X\\) is said to be a continuous r.v. if \\(X\\) has a probability density function \\(f(x)\\), \\(x \\geq 0\\), with the following properties:\n \\(f(x) = F'(x)\\). \\(F(x) = \\int_{0}^x f(y) dy\\). \\(\\Pr(a \u0026lt; X \\leq b) = \\Pr(a \u0026lt; X \u0026lt; b) = \\Pr(a \\leq X \\leq b) = \\Pr(a \\leq X \u0026lt; b) = \\int_{a}^b f(x) dx\\) \\(F(x)\\) typically looks like but note that it does not need to be concave. \\(E(X) = \\int_0^{\\infty} xf(x) dx = \\int_0^{\\infty} [1-F(x)]dx\\)  Discrete random variables #  A random variable \\(X\\) is said to be a discrete random variable if \\(X\\) takes values from a countable set of numbers \\(\\{x_1,x_2, ..., x_n, ...\\}\\).\n  Probability distribution of \\(X\\) where \\(p_n = \\Pr(X = x_n), n=1, 2, 3, ...\\)\n  \\(E(X) = \\sum_{n=1}^{\\infty} x_n \\, p_n\\)\n  The distribution function \\(F(x)\\) is a piece-wise constant function (also called step function).   Moments of a random variable #  Expectation and variance #  Expectation of \\(X\\): $$E(X) = \\int_0^{\\infty} xf(x) dx.$$ Variance of \\(X\\): $$Var(X) = E\\{[X-E(X)]^2\\} = E(X^2) -[E(X)]^2.$$\nFurthermore:\n \\(Var(X)\\) measures the variability of \\(X\\). The larger the variance, the more variability \\(X\\) has. If \\(Var(X) = 0\\), \\(X \\equiv c \\equiv E(X)\\). There is no variability for \\(X\\). \\(X\\) is a constant. If \\(X\\) and \\(Y\\) are independent, then $$Var(X+Y) = Var(X) + Var(Y).$$ \\(Var(aX)= a^2 Var(X)\\)  Moments of the average of iid rv‚Äôs #  Assume \\(X_1, X_2, ..., X_n\\) are independently and identically distributed, with $$E(X_1) = \\mu,\\text{ and } Var(X_1) = \\sigma ^2.$$ Define $$Y_n = \\frac{1}{n}(X_1+X_2+...+X_n)$$ to be the average of \\(X_1, X_2, ..., X_n\\). Then $$E(Y_n) = \\frac{1}{n}(E(X_1)+E(X_2)+...+E(X_n)) = \\frac{1}{n}(\\mu +\\mu+...+\\mu) =\\mu$$ and $$Var(Y_n) = \\frac{1}{n^2}(Var(X_1) + Var(X_2) + ... +Var(X_n)) = \\frac{1}{n^2}(\\sigma^2+\\sigma^2+...+\\sigma^2) = \\frac{\\sigma^2}{n}.$$ When \\(n \\rightarrow \\infty\\), \\(Var(Y_n) \\rightarrow 0\\). That is to say, as \\(n \\rightarrow \\infty\\), \\(Y_n \\rightarrow \\mu\\). With an infinite sample of \\(X\\)‚Äôs, you can estimate \\(\\mu\\) with certainty.\nSelected distributions #  Binomial distribution #  If \\(X \\sim Bin(n, p)\\), then $$\\Pr(X=k) = \\binom{n}{k} p^k(1-p)^{n-k}, \\ \\ \\ 0\u0026lt;p\u0026lt;1, k=0, 1, 2,..., n.$$\nNote:\n \\(E(X) = np\\), \\(Var(X) = np(1-p)\\). \\(F(k) = \\Pr(X\\leq k) = \\sum_{j=0}^k \\Pr(X=j) = \\sum_{j=0}^k {\\binom{n}{j}} p^j (1-p)^{n-j}, \\ \\ \\ k =0,1,2,...,n\\). \\(F(2.5) = \\Pr(X \\leq 2.5) = \\Pr(X \\leq 2) = F(2)\\). \\(X\\) represents # of successes out of \\(n\\) independent trials, each trail has two outcomes: sucess with probability \\(p\\) OR failure with probability \\(1-p\\).  Exponential distribution #  If \\(X \\sim Exp(\\lambda)\\) then $$f(x) = \\lambda e^{-\\lambda x}, x\u0026gt;0, \\lambda\u0026gt;0.$$\nNote:\n \\(F(x) = 1-e^{-\\lambda x}, x \\geq 0\\). \\(E(X) = \\int_0^{\\infty} xf(x) dx = \\int_0^{\\infty} [1-F(x)] dx = \\frac{1}{\\lambda}\\).  Uniform distribution #  If \\(X \\sim U(0, M)\\) then $$f(x) = \\frac{1}{M}, 0 \\leq x \\leq M.$$\nNote:\n \\(F(x) = \\frac{x}{M}, 0 \\leq x \\leq M\\). \\(E(X) = \\int_0^{M} x\\frac{1}{M} dx = \\int_0^{M} (1-\\frac{x}{M})dx = \\frac{M}{2}\\).  Probablility theory and statistics (MW 1.2) #  Please review:\n Random variables and distribution functions (MW 1.2.1) Terminology in statistics (MW 1.2.2)  Tutorial exercises for Module 1 are focused on revisions of assumed known materials.\nContinuous vs discrete random variables #  Random variables are either\n discrete distribution function (df) is \\(\\Pr[X\\le x] \\equiv F_X(x)\\) (cadlag) probability mass function (pmf) is \\(\\Pr[X=x]\\) continuous cumulative distribution function (cdf) is \\(F_X(x)\\) (cadlag) probability density function (pdf) is defined by $$f_X(x)\\equiv\\frac{d F_X(x)}{d x}.$$ This is NOT a probability. mixed  The Riemann-Stieltjes integral #  The Riemann-Stieltjes notation allows to write expressions for any type of rv. For instance, $$E\\left[ g\\left( X\\right) \\right] =\\int_{-\\infty }^{\\infty }g\\left( x\\right) dF_{X}\\left(x\\right),$$ where \\(dF_{X}\\left(x\\right)\\) is to be interpreted as\n \\(f_X(x) dx\\) for the continuous bits of \\(F\\), and \\(F_X(x)-F_X(x-0)=\\Pr[X=x]\\) (remember \\(F\\) is cadlag) for the discrete bits of \\(F\\).  Moments of random variables #  There are two types of moments:\n the moments around the origin: \\(E[X^k]\\), \\(k\u0026gt;0\\)  \\(k=1\\): the mean $$E\\left( X\\right) =\\mu _{X}=\\int_{-\\infty }^{\\infty}xdF_{X}\\left( x\\right)$$   the central moments: \\(E\\left[(X - E [X])^k\\right]\\), \\(k\u0026gt;0\\)  \\(k=2\\): the variance $$Var\\left( X\\right) =\\sigma _{X}^{2}=E\\left[\\left( X-\\mu _{X}\\right) ^{2}\\right] =E\\left( X^{2}\\right) -\\mu_{X}^{2}$$    Some extremely useful formulas: \\begin{eqnarray} E[S] \u0026amp;=\u0026amp; E\\left[ E[S|N] \\right] \u0026amp;\\quad\\text{(LIE)}\\\\ Var(S) \u0026amp;=\u0026amp; Var\\left( E[S|N] \\right) + E\\left[ Var(S|N) \\right] \u0026amp;\\quad\\text{(DVR)}\\\\ Cov(X,Y) \u0026amp;=\u0026amp; E\\left[ Cov(X,Y|Z) \\right] + Cov\\left( E[X|Z],E[Y|Z]\\right) \u0026amp;\\quad\\text{(DCR)} \\end{eqnarray}\nTail value method of calculating expectation #  For positive random variables we have $$E[X]=\\left\\{ \\begin{array}{ll} \\int_0^\\infty \\left[1-F_X(x)\\right] dx \u0026amp; \\text{if }X\\text{ is continuous} \\\\ \\sum_0^\\infty \\left[1-F_X(x)\\right] \u0026amp; \\text{if }X\\text{ is discrete} \\end{array}\\right.$$\nDescriptive statistics #  Some indicators are functions of the moments\n Coefficient of variation (measure of spread): $$\\text{Vco}(X)=\\frac{\\sigma_X}{\\mu_X}.$$ Skewness (measure of‚Ä¶ skewness!): $$\\varsigma_X=\\dfrac{E\\left[ \\left( X-\\mu _{X}\\right) ^{3}\\right] }{\\sigma _{X}^{3}}.$$ If symmetric, \\(\\varsigma_X=0\\) [vice versa not true] \\(\\varsigma_X\u0026gt;0\\) indicates heavy right-tail [skewed to the right] \\(\\varsigma_X\u0026lt;0\\) indicates heavy left-tail [skewed to the left]   *Excess Kurtosis** (measure of peakedness) $$\\gamma_2(X)=\\dfrac{E\\left[ \\left( X-\\mu _{X}\\right) ^{4}\\right] }{\\sigma _{X}^{4}}-3.$$\n \\(\\gamma _2(X)=0\\) mesokurtic [like Normal, Binomial \\((p=0.5)\\) ] \\(\\gamma _2(X)\u0026gt;0\\) leptokurtic [fatter tails] \\(\\gamma _2(X)\u0026lt;0\\) platykurtic [thinner tails]  Note that these indicators have no units, which allows comparisons between distributions.\nGenerating functions #  Probability generating function pgf‚Äîonly for discrete rv! \\begin{eqnarray} p_{X\\left( t\\right)}\u0026amp;=\u0026amp;E\\left( t^{X}\\right) \\\\ \u0026amp;=\u0026amp;\\Pr[X=0]+\\Pr[X=1] t+ \\Pr[X=2] t^2 +\\Pr[X=3] t^3 + \\ldots \\end{eqnarray} Moment generating function mgf \\begin{eqnarray} M_{X\\left( t\\right)}\u0026amp;=\u0026amp;E\\left( e^{tX}\\right) \\\\ \u0026amp;=\u0026amp;1+E[X] t+ E[X^2] \\frac{t^2}{2} +E[X^3] \\frac{t^3}{6} + \\ldots + E[X^k] \\frac{t^k}{k!}+\\ldots \\end{eqnarray} and thus $$E[X^k]=\\left.\\dfrac{d^{k}}{dt^{k}}m_{X}\\left( t\\right) \\right|_{t=0}$$\n Example: \\(X\\sim\\text{norm}(\\mu,\\sigma^2)\\) distribution\n$$E\\left[ e^{tX} \\right] = e^{\\mu t + \\frac{1}{2}\\sigma^2t^2}$$\n Cumulant generating function cgf \\begin{eqnarray} \\kappa _{X\\left( t\\right)} \u0026amp;=\u0026amp; \\ln\\left[ m_{X}\\left( t\\right) \\right] \\\\ \u0026amp;=\u0026amp;E[X] t+ Var(X) \\frac{t^2}{2} +E\\left[ (X-\\mu_X)^3\\right] \\frac{t^3}{6} + \\gamma_2(X)\\left[Var(X)\\right]^2\\frac{t^4}{4!}+\\ldots^* \\end{eqnarray} and thus $$E\\left[ (X-\\mu_X)^k\\right]=\\left.\\dfrac{d^{k}}{dt^{k}}\\kappa_{X}\\left( t\\right) \\right|_{t=0},\\;\\;\\;\\;**k=2 \\text{ and **3}^*.$$\n CAUTION: the second and third cumulants are the second and third central moments, but NOT the following ones! Cumulants are additive: the \\(k\\)-th cumulant of a sum is the sum of the \\(k\\)-th cumulants (conditions?)  @: References: MW 1\nUse of the R software #  R is required prior knowledge for this course, and part of the prerequisite course MAST20005 (Statistics).\nThis is because R is a required software for the actuarial professional exams CS1 and CS2 (see https://www.actuaries.org.uk/studying/curriculum/actuarial-statistics and also https://www.actuaries.org.uk/studying/curriculum/frequently-asked-questions-curriculum ). It is used very widely by actuaries in industry (see, for instance, http://insightriskconsulting.co.uk/blog/r-for-actuaries/ or https://www.actuaries.digital/2019/09/26/my-top-10-r-packages-for-data-analytics/ ). Some companies also use R to produce presentations and documentation for generally.\nIn order to help you with R I have put together a website that summarises all the things I think you should know before starting your first grad role: https://communicate-data-with-r.netlify.app At the very least, you need to know what is under ‚ÄúBase R.‚Äù Learning the ‚Äútidyverse‚Äù will be most useful, as well as ‚Äúggplot2‚Äù for better visualisations. ‚Äúhtmlwidgets‚Äù is more advanced, and not required for the course. You may want to create your assignment with ‚ÄúR Markdown‚Äù (under ‚ÄúCommunicate Data‚Äù), although this is not required either.\nThe main reference for Base R is the book http://biostatisticien.eu/springeR/index-en.html, which is also available in other languages (including mandarin). The English version can be downloaded for free from the Unimelb library: https://go.openathens.net/redirector/unimelb.edu.au?url=http%3A%2F%2Fdx.doi.org%2F10.1007%2F978-1-4614-9020-3\nI strongly recommend you review the R materials mentioned above before the semester starts.\nSee also the Actuaries Institute Analytics Cookbook: https://www.actuaries.digital/2021/11/30/the-actuaries-analytics-cookbook-recipes-for-data-success/ https://actuariesinstitute.github.io/cookbook/docs/index.html\nCredit #  The initial version of Part 1 and 2 were developed by Professor Shuanming Li in 2018. These were then transcribed modified and augmented by Professor Benjamin Avanzi in 2021 for the subject ACTL10001.\n"},{"id":11,"href":"/docs/m1-introduction/","title":"M1 Introduction","section":"Docs","content":"The nature of general insurance (MW 1.1) #  General insurance #   also called non-life, or property and casualty  Includes: car, liability, property, workers compensation, marine, credit, legal, travel, health, ‚Ä¶   efficient because of the (weak) law of large numbers (Bernoulli):  $$\\lim_{n\\rightarrow \\infty} \\Pr \\left[ \\left| \\frac{1}{n} \\sum_{i=1}^n Y_i - E[Y_i]\\right| \\ge \\epsilon\\right] = 0$$      for more background:  see general insurance practice for further details about the general insurance area see Pooling and Insurance for further details about the law of large numbers mechanism    Risk components #  Insurance organises a risk transfer:\n costing of this transfer is an actuarial problem makes sense only because people are risk averse, unless insurance is forced  Risk / randomness comes from different sources:\n Pure randomness (also called ‚Äúprocess risk‚Äù or ‚Äúaleatoric risk‚Äù)  Nature of the risk Can be ‚Äúcontrolled‚Äù by volume (law of large numbers)   Model risk (‚Äúepistemic risk‚Äù)  All models are wrong, some are useful model world \\(\\neq\\) real world even if model was right, wrong parameters non-stationarity    \\(\\Longrightarrow\\) we need to add a buffer/margin to the cost of the risk transfer.\nPremium components #  \\begin{eqnarray} \\text{gross premium} \u0026amp;=\u0026amp; \\text{pure risk premium} \\\\ \u0026amp;\u0026amp; + \\text{risk margin} \\\\ \u0026amp;\u0026amp; + \\text{profit margin} \\\\ \u0026amp;\u0026amp; - \\text{financial gains on investments} \\\\ \u0026amp;\u0026amp; + \\text{underwriting expenses} \\\\ \u0026amp;\u0026amp; + \\text{loss adjustment expenses (LAE)} \\\\ \u0026amp;\u0026amp; (+ \\text{taxes}) \\end{eqnarray}\nThis is not necessarily the premium that is charged to customers, but calculating the right hand side is one of the actuary‚Äôs roles.\nConnections with the course contents #   We typically insure multiple risks:  We need to know how to aggregate them (Module 2) We need distributions for counts and sums, including random sums (Modules 2, 3, and 4) Those risk may not be independent (Module 5)   We need a distribution for the losses  The ‚Äúpure risk premium‚Äù is the expectation of the risk (Module 3) The ‚Äúrisk margin‚Äù is typically function of the distribution of the insured loss‚Äìa quantile, or a function of variance (Modules 3 and 4) Sometimes those risks can be extreme (Module 6)   Losses arise over time, and there may be time dependencies (relationships across time) that are relevant to the modelling (Modules 7-10)  R packages used in this course #  The following packages are useful and should be installed and loaded on your machines:\n stats is a generalist package providing statistical functions MASS (‚ÄúModern Applied Statistics with S‚Äù) is a powerful package for data analysis tidyverse is a package for wrangling and preparing data for analysis actuar is a package with functions that are specific to actuarial studies see Dutang, Goulet, and Pigeon (2008) fitdistrplus builds on the abovementioned packages for advanced fitting features see Delignette-Muller and Dutang (2015) evir and extRemes will be used extensively in Module 6 (Extreme Value Theory) see Gilleland and Katz (2016) xts and astsa will be used extensively in Module 7‚Äì10 (Time Series and Analysis)   In the lectures that follows, I will indicate which package a function comes from the first time it appears by writing package::function, and then will drop the package:: part as it is not needed once you load that library. [Note this allows you to call a specific function from a package without loading it (useful when there are package clashes).]\nDelignette-Muller, Marie Laure, and Christophe Dutang. 2015. ‚ÄúFitdistrplus: An r Package for Fitting Distributions.‚Äù Journal of Statistical Software 64 (4).\n Dutang, Christophe, Vincent Goulet, and Mathieu Pigeon. 2008. ‚ÄúActuar: An r Package for Actuarial Science.‚Äù Journal of Statistical Software 25 (7).\n Gilleland, Eric, and Richard W. Katz. 2016. ‚ÄúextRemes 2.0: An Extreme Value Analysis Package in R.‚Äù Journal of Statistical Software 72 (8).\n  "},{"id":12,"href":"/docs/1-claims-modelling/","title":"Claims Modelling (CS2 Section 1)","section":"Docs","content":"Claims Modelling\n\nThis section covers contents in the Section 1 in the CS2 syllabus ; see also the learning outcomes mappings in the Subject Guide.\nThe main references for Modules 1-4 is: [MW] W√ºthrich, Mario V., Non-Life Insurance: Mathematics \u0026amp; Statistics (December 17, 2020). Available at SSRN: https://ssrn.com/abstract=2319328  MW can be downloaded from SSRN.\n The main references for Modules 5-6 is:\n[CS2] Institute and Faculty of Actuaries, CS2 Core Reading, Unit 3 Copulas and Unit 4 Extreme Value Theory (respectively)  CS2 can be downloaded from the subject\u0026rsquo;s \u0026ldquo;Readings Online\u0026rdquo;.\n"},{"id":13,"href":"/docs/1-claims-modelling/m4-compound-approx/","title":"M4 Approximations for Compound Distributions","section":"Claims Modelling (CS2 Section 1)","content":"\\(\\adv\\) Module 4 Out of Scope #  Please note that this entire module is out of scope for Semester 1, 2022.\nAlgorithms for compound distributions #  Preliminary: Discretisation #   In the algorithms considered in this module (and in real life..!), severity distributions are discrete. In Module 3 we discussed how to fit parametric distributions to real data, but these were continuous (which often allows to define a whole df easily with just a few parameters). This is not a restriction as the distribution of \\(Y\\) can easily be discretised to an arbirary level of precision. Different methods have different properties and are used for different purposes. A number of those methods are easily implemented, thanks to the function discretise of actuar.   There are three (four) methods, all covered by the R function actuar::discretise:\n Mass dispersal (method=\u0026quot;rounding\u0026quot;): a first try to allocate weights in a rather `unbiased‚Äô way. In this case, the true cdf passes through the midpoints of the discretisation intervals. Unbiased (method=\u0026quot;unbiased\u0026quot;): this method matches the first moment of the discretised and the true distributions. Lower (method=\u0026quot;lower\u0026quot;) and Upper (method=\u0026quot;upper\u0026quot;) bounds, which sandwich the correct distribution between two arbitrarily close bounds. Here the idea is to intentionally underestimate and overestimate the distribution. However, the gap in-between can be chosen to be arbitrarily small.    in discretise, we will also need to specify:\n step \\(=h\\) : a currency span (say, $10 or $100) from \\(=0\\) : the left limit of discretisation (we set this to 0 here, a natural bound for positive claims distributions) to \\(=m\\) : the right limit of discretisation cdf: the distribution to discretise (in cumulative, cdf form‚Äîthe pfoo function)  Method of rounding (mass dispersal) #   apportion the probability mass to a finite set of points \\(0, h, 2h, \\ldots, mh\\) if the range of \\(X\\) is not finite, \\(m\\) has to be chosen in order to have a good representation of the right tail the span \\(h\\) should be carefully chosen as to be relatively small compared to the mean (beware of units!) Let \\(g_j\\) be the probability mass placed at \\(jh\\), \\(j=0,1,\\ldots,m\\). We have  $$\\begin{array}{rcl} g_0 \u0026amp;=\u0026amp; G\\left( h/2 \\right) \\\\ g_j \u0026amp;=\u0026amp; G\\left( jh + h/2\\right)-G\\left( jh - h/2\\right),\\quad g=1,2,\\ldots,m-1 \\\\ g_m \u0026amp;=\u0026amp; 1-G\\left(mh-h/2\\right) \\end{array}$$\nsuch that \\(\\sum_{j=0}^m g_j=1\\).\n(Note that if \\(G\\) is mixed and a mass happens to be at an interval threshold, it is customary‚Äîfor conservativeness‚Äîto allocate it to the next interval, so that \\(G\\) would be understood as c√†gl√†d here.)\nUnbiased method #   Let \\(g_j\\) be the probability mass placed at \\(jh\\), \\(j=0,1,\\ldots,m\\). We have  $$\\begin{array}{rcl} g_0 \u0026amp;=\u0026amp; 1 - E[\\min(X,h)]/h \\\\ g_j \u0026amp;=\u0026amp; (2 E[\\min(X,j)]-E[\\min(X,j-h)]-E[\\min(X,j+h)])/h,\\\\ \u0026amp;\u0026amp;j=1,2,\\ldots,m-1 \\\\ g_m \u0026amp;=\u0026amp; (E[\\min(X,m)]-E[\\min(X,m-h)])/h-(1-G(m)) \\end{array}$$\nsuch that \\(\\sum_{j=0}^m g_j=G(m)\\).\n Note that in this case the parameter lev must be specified. This is an expression that computes the limited expected value of the distribution corresponding to cdf. This is readily available in actuar for many distributions.  Method of lower and upper bounds #  Define:\n$$G((k+1)d)-G(kd) = \\left\\{ \\begin{array}{ll} \\Pr[Y_1^+=(k+1)d] \\equiv g_{k+1}^+ \u0026amp; \\text{(shifted to the right)} \\\\ \\Pr[Y_1^-=k d] \\equiv g_{k}^- \u0026amp; \\text{(shifted to the left)} \\end{array} \\right.$$ Note that from a stochastic dominance point of view\n \\(Y_1^+\\) (method=\u0026quot;lower\u0026quot;) overestimates (is larger than) \\(Y_1\\), whereas \\(Y_1^-\\) (method=\u0026quot;upper\u0026quot;) underestimates (is smaller than) \\(Y_1\\).  This means that we can sandwich the true distribution: $$\\Pr[Y^- \u0026gt; y] \\le \\Pr[Y\u0026gt;y] \\le \\Pr[Y^+ \u0026gt; y], \\text{ or}$$ $$\\Pr[Y^- \\le y] \\ge \\Pr[Y\\le y] \\ge \\Pr[Y^+ \\le y] \\Longleftrightarrow G^+(y)\\le G(y) \\le G^-(y).$$ This can be the best option, especially for patchwork distributions; see Example 4.12 in Wuthrich (2020).\nExamples #  Let us discretise\nstp \u0026lt;- 5 final \u0026lt;- 80 # these are pmf\u0026#39;s gamma.discr.round \u0026lt;- discretise(pgamma(x, 2, 0.1), from = 0, to = final, step = stp, method = \u0026#34;rounding\u0026#34;) gamma.discr.unbia \u0026lt;- discretise(pgamma(x, 2, 0.1), from = 0, to = final, step = stp, method = \u0026#34;unbiased\u0026#34;, lev = levgamma(x, 2, 0.1)) gamma.discr.lower \u0026lt;- discretise(pgamma(x, 2, 0.1), from = 0, to = final, step = stp, method = \u0026#34;lower\u0026#34;) gamma.discr.upper \u0026lt;- discretise(pgamma(x, 2, 0.1), from = 0, to = final, step = stp, method = \u0026#34;upper\u0026#34;)  length(gamma.discr.round) ## [1] 16  length(gamma.discr.unbia) ## [1] 17  length(gamma.discr.lower) ## [1] 17  length(gamma.discr.upper) ## [1] 16  Note the different lengths (either \\(m\\) or \\(m+1\\)), although they all start with \\(g_0\\).\n Furthermore, the masses don‚Äôt add up to 1 necessarily:\nsum(gamma.discr.round) ## [1] 0.996231  pgamma(final - stp/2, 2, 0.1) ## [1] 0.996231  sum(gamma.discr.unbia) ## [1] 0.9969808  sum(gamma.discr.lower) ## [1] 0.9969808  sum(gamma.discr.upper) ## [1] 0.9969808  pgamma(final, 2, 0.1) ## [1] 0.9969808   This is because the algorithm does not adjust for the tail:\n rounding has \\(g_m = G(m+h/2)-G(m-h/2)\\) rather than \\(g_m = 1-G(m-h/2)\\) so that \\(1-G(m+h/2)\\) is missing. unbiased has that correction \\(-(1-G(m))\\) (due to the fact that this mass does not correspond to this interval) which is then missing. lower has \\(g_0=0\\) and then the mass \\(G(m)\\) is apportioned to the end of the \\(m\\) intervals. \\(1-G(m)\\) remains unallocated. upper has its first allocation from \\(G(m)\\) at 0, so has one less mass in the vector. \\(1-G(m)\\) remains unallocated.  In practice one would choose \\(m\\) sufficiently large so that these are issues are not present. If they are, then, an adjustment might be necessary, and rounding becomes the most natural one to use (just add the missing mass at \\(g_m\\) as per the definition above).\n # getting df\u0026#39;s gamma.discr.round.df \u0026lt;- cumsum(gamma.discr.round) gamma.discr.unbia.df \u0026lt;- cumsum(gamma.discr.unbia) gamma.discr.lower.df \u0026lt;- cumsum(gamma.discr.lower) gamma.discr.upper.df \u0026lt;- cumsum(gamma.discr.upper) # comparing discretisation techniques curve(pgamma(x, 2, 0.1), from = 0, to = final) lines((0:(final/stp - 1)) * stp, gamma.discr.round.df, type = \u0026#34;s\u0026#34;, pch = 20, col = \u0026#34;red\u0026#34;) lines((0:(final/stp)) * stp, gamma.discr.unbia.df, type = \u0026#34;s\u0026#34;, pch = 20, col = \u0026#34;green\u0026#34;) lines((0:(final/stp)) * stp, gamma.discr.lower.df, type = \u0026#34;s\u0026#34;, pch = 20, col = \u0026#34;blue\u0026#34;) lines((0:(final/stp - 1)) * stp, gamma.discr.upper.df, type = \u0026#34;s\u0026#34;, pch = 20, col = \u0026#34;magenta\u0026#34;)  Panjer‚Äôs recursion algorithm #   The remarkable property of the \\((a,b)\\) class of (frequency) distributions allows us to develop a recursive method to get the distribution of \\(S\\) for discrete \\(Y\\)‚Äôs. When \\(Y\\) is continuous, simply discretise its cdf first. The algorithm is very stable when \\(N\\) is Poisson and Negative Binomial, but less stable when \\(N\\) is Binomial.   Let \\(S\\) have a compound distribution on \\(Y\\), where the following are mutually independent:\n \\(N\\) belongs to the \\((a,b)\\) class of distributions; \\(Y\\) are identically distributed, non-negative and discrete.  We have then $$f_{S}\\left( s\\right) =\\frac{1}{1-a g_0 }\\sum_{j=1}^{s}\\left( a+b\\frac{j}{s}\\right) g_j f_{S}\\left( s-j\\right),\\quad s=1,2,\\ldots,$$ with starting value\n$$f_{S}\\left( 0\\right) =\\left\\{ \\begin{array}{ll} \\Pr\\left[ N=0\\right] , \u0026amp; \\text{if }g_0 =0 \\\\ m_{N}\\left[ \\ln g_0 \\right] , \u0026amp; \\text{if }g_0 \u0026gt;0. \\end{array} \\right.$$\nNote that if \\(g_y=0\\) for \\(y\u0026gt;y_{\\text{max}}\\) then the upper bound of the sum can be reduced to \\(\\min(s,y_{\\text{max}}).\\)\nPanjer‚Äôs recursion in R #  Panjer‚Äôs recursion can be performed using the aggregateDist function using the method=\u0026quot;recursive\u0026quot;.\n The frequency distribution can be any of the \\((a,b,0)\\) or \\((a,b,1)\\) class of distributions (that is, with arbitrary masses at 0). The severity distribution must be discrete on \\(0,1,\\ldots,m\\) for some monetary unit.  Important parameters include:\n model.freq: name of the distribution (e.g.¬†=\u0026quot;poisson\u0026quot;) model.sev: df of the discrete (-ised) distribution of \\(Y\\) x.scale: value of an amount 1 in the severity model (monetary unit) maxit: maximum number of iterations, which often needs to be increased (if it is too small then the df of \\(S\\) does not reach 1, leading to an error message)  Example #  We want to calculate the distribution of $$S=\\sum_{i=1}^N Y_i,$$ where \\(N\\sim\\text{Poi}(10)\\) and \\(Y_1\\sim\\text{gamma}(2,0.1)\\).\nstp \u0026lt;- 1 final \u0026lt;- 200 # these are pmf\u0026#39;s gamma.discr.unbia \u0026lt;- discretise(pgamma(x, 2, 0.1), from = 0, to = final, step = stp, method = \u0026#34;unbiased\u0026#34;, lev = levgamma(x, 2, 0.1)) gamma.discr.lower \u0026lt;- discretise(pgamma(x, 2, 0.1), from = 0, to = final, step = stp, method = \u0026#34;lower\u0026#34;) gamma.discr.upper \u0026lt;- discretise(pgamma(x, 2, 0.1), from = 0, to = final, step = stp, method = \u0026#34;upper\u0026#34;)  S.unbia.pmf \u0026lt;- aggregateDist(method = \u0026#34;recursive\u0026#34;, model.freq = \u0026#34;poisson\u0026#34;, lambda = 10, model.sev = gamma.discr.unbia, x.scale = stp, maxit = 1000) S.lower.pmf \u0026lt;- aggregateDist(method = \u0026#34;recursive\u0026#34;, model.freq = \u0026#34;poisson\u0026#34;, lambda = 10, model.sev = gamma.discr.lower, x.scale = stp, maxit = 1000) S.upper.pmf \u0026lt;- aggregateDist(method = \u0026#34;recursive\u0026#34;, model.freq = \u0026#34;poisson\u0026#34;, lambda = 10, model.sev = gamma.discr.upper, x.scale = stp, maxit = 1000) plot(S.unbia.pmf, pch = 20, xlim = c(0, 500), col = \u0026#34;black\u0026#34;, cex = 0.5) lines(S.upper.pmf, pch = 20, col = \u0026#34;blue\u0026#34;, cex = 0.5) lines(S.lower.pmf, pch = 20, col = \u0026#34;magenta\u0026#34;, cex = 0.5)  Panjer‚Äôs recursion for compound Poisson #  If \\(S\\sim\\text{compound Poisson}(\\lambda,g_x)\\) the algorithm reduces to $$f_{S}\\left( s\\right) =\\frac{\\lambda}{s}\\sum_{j=1}^{s} \\, j\\,g_j f_{S}\\left( s-j\\right) \\text{.}$$ with starting value $$f_{S}\\left( 0\\right) =e^{\\lambda(g_0-1)}$$ (whether \\(g_0\\) is positive or not).\nExample A 12.4.2 (recomputed) #  Effectively, the recursion formula boils down to $$f_{S}\\left( s\\right) =\\frac{1}{s}\\left[ 0.2f_{S}\\left( s-1\\right)+0.6f_{S}\\left( s-2\\right) +0.9f_{S}\\left( s-3\\right) \\right], \\;\\; (\\text{for }s\u0026gt;2)$$ with starting value $$f_{S}\\left( 0\\right) =\\Pr\\left[ N=0\\right]=e^{-0.8}=0.44933.$$ We have then\n$$\\begin{array}{rcl} f_{S}\\left( 1\\right) \u0026amp;=\u0026amp; 0.2f_{S}\\left( 0\\right) =0.2e^{-0.8}=0.089866 \\\\ f_{S}\\left( 2\\right) \u0026amp;=\u0026amp;\\frac{1}{2}\\left[ 0.2f_{S}\\left( 1\\right)+0.6f_{S}\\left( 0\\right) \\right] =0.32e^{-0.8}=0.14379 \\\\ f_{S}\\left( 3\\right) \u0026amp;=\u0026amp;\\frac{1}{3}\\left[ 0.2f_{S}\\left( 2\\right)+0.6f_{S}\\left( 1\\right) +0.9f_{S}\\left( 0\\right) \\right]=0.3613e^{-0.8}=0.16236 \\\\ \u0026amp;\u0026amp; etc \\ldots \\end{array}$$\n fs \u0026lt;- aggregateDist(method = \u0026#34;recursive\u0026#34;, model.freq = \u0026#34;poisson\u0026#34;, lambda = 0.8, model.sev = c(0, 0.25, 0.375, 0.375), x.scale = 1) diff(fs)[1:4] ## [1] 0.44932896 0.08986579 0.14378527 0.16235753   plot(fs) de Pril‚Äôs recursion algorithm #  As a corollary, a recursion algorithm can be developed for the \\(n\\)-th convolution of a non-negative and discrete random variable \\(Y\\) with positive mass of probability at 0. Let \\(g_k\\) be its pmf.\nFirst let us rewrite\n$$\\begin{array}{rcl} M_Y(t)\u0026amp;=\u0026amp;\\sum_{k=0}^\\infty g_k e^{tk}=g_0+\\sum_{k=1}^\\infty g_k e^{tk} \\\\ \u0026amp;=\u0026amp;g_0 + [1-g_0]\\sum_{k=1}^\\infty \\frac{g_k}{1-g_0} e^{tk} \\\\ \u0026amp;=\u0026amp; q + p m_{\\widetilde{Y}}(t). \\end{array}$$\nwhere\n \\(q=g_0\\), \\(p=1-g_0\\), and the pmf of \\(\\widetilde{Y}\\) is: \\(\\widetilde{g}_0=0\\), \\(\\widetilde{g}_k=\\frac{g_k}{1-g_0}\\), \\(k=1,2,\\cdots\\).   Thus $$E\\left[e^{t(Y_1+\\ldots+Y_n)}\\right]=\\left(M_Y(t)\\right)^n=\\left(q + p m_{\\widetilde{Y}}(t)\\right)^n,$$ which means that the \\(n\\)-th convolution of \\(Y\\) is compound Binomial with parameters \\((m=n,p=1-g_0,\\widetilde{G}(y))\\), a member of the \\((a,b)\\) class with\n$$\\begin{array} a\u0026amp;=\u0026amp;-\\frac{p}{1-p} \\\\ \u0026amp;=\u0026amp; -\\frac{1-g_0}{g_0}, \\\\ b\u0026amp;=\u0026amp;\\frac{(m+1)p}{1-p} \\\\ \u0026amp;=\u0026amp; \\frac{(n+1)(1-g_0)}{g_0}. \\end{array}$$\n Applying Panjer‚Äôs algorithm yields with $$g^{*n}_0=g_0^n.$$ Note that the \\(g_j\\)‚Äôs are indeed the original ones.\nNumerical example #  Let \\(g_k=(1+k)/10\\), \\(k=0,1,2\\) and \\(3\\). We have $$g^{*2}_k=\\sum_{j=1}^{\\min(k,3)}\\left[3\\frac{j}{k}-1\\right](1+j)g^{*2}_{k-j}\\:\\text{ with }\\:g^{*2}_0=[g_0]^2$$\n$$\\begin{array}{rcl} g^{*2}_0\u0026amp;=\u0026amp;(0.1)^2=0.01\\\\ g^{*2}_1\u0026amp;=\u0026amp;[3/1-1] \\cdot 2 \\cdot 0.01=0.04 \\\\ g^{*2}_2\u0026amp;=\u0026amp;[3/2-1] \\cdot 2 \\cdot 0.04+[6/2-1] \\cdot 3 \\cdot 0.01=0.10 \\\\ g^{*2}_3\u0026amp;=\u0026amp;[3/3-1] \\cdot 2 \\cdot 0.10+[6/3-1] \\cdot 3 \\cdot 0.04+[9/3-1] \\cdot 4 \\cdot 0.01=0.20 \\\\ g^{*2}_4\u0026amp;=\u0026amp;[3/4-1] \\cdot 2 \\cdot 0.20+[6/4-1] \\cdot 3 \\cdot 0.10+[9/4-1] \\cdot 4 \\cdot 0.04=0.25 \\\\ g^{*2}_5\u0026amp;=\u0026amp;[3/5-1] \\cdot 2 \\cdot 0.25+[6/5-1] \\cdot 3 \\cdot 0.20+[9/5-1] \\cdot 4 \\cdot 0.10=0.24 \\\\ g^{*2}_6\u0026amp;=\u0026amp;[3/6-1] \\cdot 2 \\cdot 0.24+[6/6-1] \\cdot 3 \\cdot 0.25+[9/6-1] \\cdot 4 \\cdot 0.20=0.16 \\end{array}$$\n brute force convolution:\npmf \u0026lt;- c(1/10, 2/10, 3/10, 4/10) range \u0026lt;- length(pmf) range2 \u0026lt;- (range - 1) * 2 + 1 pmf \u0026lt;- c(pmf, rep(0, (range2 - range))) ps2.conv \u0026lt;- c() for (i in 1:range2) { ps2.conv \u0026lt;- c(ps2.conv, sum(pmf[1:i] * pmf[i:1])) } ps2.conv ## [1] 0.01 0.04 0.10 0.20 0.25 0.24 0.16   Using de Pril‚Äôs algorithm\nps2 \u0026lt;- aggregateDist(method = \u0026#34;recursive\u0026#34;, model.freq = \u0026#34;binomial\u0026#34;, size = 2, prob = 1 - 1/10, model.sev = c(0, 2/10, 3/10, 4/10)/(1 - 1/10), x.scale = 1, xlim = c(0, 6)) diff(ps2) ## [1] 0.01 0.04 0.10 0.20 0.25 0.24 0.16  plot(ps2) Fast Fourier Transform #  Not covered this year.\nApproximations #  Possible motivations:\n It is not possible to compute the distribution of \\(S\\):  no detailed data is available except for the moments of \\(S\\) technical issues (impossible to fit a tractable model to data)   The risk of having a sophisticated‚Äîbut wrong‚Äîmodel is too high  Only limited data is available to fit the model It may be argued that the approximation is very accurate   A quick approximation is needed. A higher level of accuracy is not required (does not justify the resources necessary to calculate an exact probability)  Note: let \\(\\varsigma_S\\equiv\\gamma_1\\) and \\(\\gamma_2(S)\\equiv\\gamma_2\\) in this section.\nApproximations assuming a symmetrical distribution #  The Normal approximation #   The Central Limit Theorem suggests that  $$\\begin{array}{rcl} \\Pr\\left[ S\\leq s\\right] \u0026amp;=\u0026amp; =\\Pr\\left[ \\frac{S-E[S] }{\\sqrt{Var(S)} }\\leq \\frac{s-E[S] }{\\sqrt{Var(S)} }\\right] \\\\ \u0026amp;\\approx \u0026amp;\\Pr\\left[ Z\\leq \\frac{s-E[S] }{\\sqrt{Var(S)} }\\right]=\\Phi \\left( \\frac{s-E[S] }{\\sqrt{Var(S)} }\\right) , \\end{array}$$\nwhere \\(Z\\) is a standard Normal random variable and where \\(\\Phi \\left( \\cdot \\right)\\) denotes its cdf\n The classical CLT approximation holds for a fixed number of claims. Here, we typically have a random number of claims.    Assuming \\(S\\sim\\text{CompPoi}(\\lambda v,G)\\), Theorem 4.1 of Wuthrich (2020) states that $$ \\frac{S-\\lambda v E[Y]}{\\sqrt{\\lambda v E[Y^2]}} \\Longrightarrow \\mathcal{N}(0,1)\\quad \\text{as }v\\rightarrow \\infty.$$ This leads to the approximation $$\\Pr[S\\le s] = \\Pr \\left[\\frac{S-\\lambda v E[Y]}{\\sqrt{\\lambda v E[Y^2]}} \\le \\frac{s-\\lambda v E[Y]}{\\sqrt{\\lambda v E[Y^2]}}\\right] \\approx \\Phi\\left(\\frac{s-\\lambda v E[Y]}{\\sqrt{\\lambda v E[Y^2]}}\\right).$$ Note that this holds only when \\(G\\) has a finite second moment, which suggests that this should be used only for \\(S_{\\text{sc}}\\).  Note that this approximation performs poorly\n individual model: for small \\(n\\) (CLT not effective) collective model: for small \\(\\lambda\\) (compound Poisson) and small \\(r\\) (compound negative binomial) for highly skewed distributions  Normal Power #  Two levels:\n NP1: this is the CLT approximation NP2: the same idea, but with a correction taking the skewness into account. We have $$\\Pr\\left[ \\frac{S-E[S] }{\\sqrt{Var(S)} }\\leq s\\right] \\approx \\Phi \\left( z\\right)$$ with $$s(z)=z+\\frac{\\gamma_1}{6} \\left(z^{2}-1\\right)\\quad\\text{ or }\\quad z(s)=\\sqrt{\\frac{9}{\\gamma_1 ^{2}}+\\frac{6s}{\\gamma_1 }+1}-\\frac{3}{\\gamma_1 }$$ NP2 is effective for \\(S\u0026gt;E[S]+\\sqrt{Var(S)}\\).    NP1 and NP2 approximations are available with the aggregateDist function For instance here use on a compound Poisson random variable with gamma distributed claims:  gamma \u0026lt;- 2 c \u0026lt;- 0.1 lambda \u0026lt;- 10 moments \u0026lt;- c(lambda * gamma/c, lambda * (gamma/c^2 + (gamma/c)^2), gamma * (gamma + 1) * (gamma + 2)/c^3/lambda^(1/2)/(gamma/c^2 + (gamma/c)^2)^(3/2)) Fs.NP1 \u0026lt;- aggregateDist(\u0026#34;normal\u0026#34;, moments = moments) curve(Fs.NP1, xlim = c(300, 450), ylim = c(0.9, 1), col = \u0026#34;red\u0026#34;, lwd = 3) lines(S.unbia.pmf, pch = 20, cex = 0.5) lines(S.upper.pmf, pch = 20, col = \u0026#34;blue\u0026#34;, cex = 0.5) lines(S.lower.pmf, pch = 20, col = \u0026#34;magenta\u0026#34;, cex = 0.5)   Fs.NP2 \u0026lt;- aggregateDist(\u0026#34;npower\u0026#34;, moments = moments) curve(Fs.NP2, xlim = c(300, 450), ylim = c(0.9, 1), col = \u0026#34;red\u0026#34;, lwd = 3) lines(S.unbia.pmf, pch = 20, cex = 0.5) lines(S.upper.pmf, pch = 20, col = \u0026#34;blue\u0026#34;, cex = 0.5) lines(S.lower.pmf, pch = 20, col = \u0026#34;magenta\u0026#34;, cex = 0.5)   curve(Fs.NP2, xlim = c(200, 450), ylim = c(0.4, 1), col = \u0026#34;red\u0026#34;, lwd = 3) lines(S.unbia.pmf, pch = 20, cex = 0.5) lines(S.upper.pmf, pch = 20, col = \u0026#34;blue\u0026#34;, cex = 0.5) lines(S.lower.pmf, pch = 20, col = \u0026#34;magenta\u0026#34;, cex = 0.5)  Approximations using a skewed distribution #  The translated gamma and LN approximations #  Idea: Rather than correcting the CLT approximation, we use here a distribution that is naturally (positively) skewed, and use the third moment (skewness), which will be fitted thanks to a translation of the distribution (a third parameter to match the third moment).\nWe have then $$Y = k+Z, \\text{ where }Z\\sim \\left\\{ \\begin{array}{l} \\Gamma(\\gamma,c), \\text{ or}\\\\\\text{LN}(\\mu,\\sigma^2)\\end{array}\\right. .$$ Then match expected value, variance and skewness.\nNote: \\(k\\) appears only in the expected value. It allows to shift the distribution without affecting its scale (and central moments).\n As an example, fitting a translated gamma \\((k,\\gamma,c)\\) to a compPois$(\\lambda v,G(y))$ boils down to solving\n$$\\begin{array}{rcl} \\lambda v E[Y] \u0026amp;=\u0026amp; k+\\gamma/c; \\\\ \\lambda v E[Y^2]\u0026amp;=\u0026amp; \\gamma/c^2; \\\\ \\frac{E[Y^3]}{(\\lambda v)^{1/2} E[Y^2]^{3/2}} \u0026amp;=\u0026amp; 2\\gamma^{-1/2}; \\end{array}$$\nsuch that\n$$\\begin{array}{rcl} \\gamma \u0026amp;=\u0026amp; \\left(\\frac{2(\\lambda v)^{1/2} E[Y^2]^{3/2}}{E[Y^3]}\\right)^2; \\\\ c \u0026amp;=\u0026amp; \\left(\\frac{\\gamma}{\\lambda v E[Y^2]}\\right)^{1/2}; \\\\ k \u0026amp;=\u0026amp; \\lambda v E[Y] - \\frac{\\gamma}{c}. \\end{array}$$\n Properties of the translated gamma approximation:\n If \\(\\gamma\\rightarrow\\infty\\), \\(c\\rightarrow\\infty\\) and \\(k\\rightarrow -\\infty\\) such that $$E[S]=k+\\frac{\\gamma}{c}=\\mu\\text{ (constant)}\\quad\\text{and}\\quad \\frac{\\gamma}{c^2}=\\sigma^2\\text{ (constant)}$$ then the translated gamma converges to the Normal \\((\\mu,\\sigma^2)\\) distribution. In this sense, this approximation is a generalisation of the CLT approximation. The distribution of a compound binomial random variable approaches the gamma distribution if the expected number of claims is large and the claim amount distribution has relatively small dispersion (see Theorem 12.A.1 in the Appendix of Chapter 12 of Bowers et al. (1997)). This approximation should be used for distributions with a positive skewness only.   # matching moments: gamma \u0026lt;- (2/moments[3])^2 c \u0026lt;- (gamma/moments[2])^(1/2) k \u0026lt;- moments[1] - gamma/c # check: c(gamma/c + k, moments[1]) ## [1] 200 200  c((gamma/c^2), moments[2]) ## [1] 6000 6000  c(2 * gamma^(-1/2), moments[3]) ## [1] 0.5163978 0.5163978  plot(200:450, pgamma(200:450 - k, shape = gamma, rate = c), xlim = c(200, 450), ylim = c(0.4, 1), col = \u0026#34;red\u0026#34;, lwd = 3, type = \u0026#34;l\u0026#34;, xlab = c(\u0026#34;\u0026#34;)) lines(S.unbia.pmf, pch = 20, cex = 0.5) lines(S.upper.pmf, pch = 20, col = \u0026#34;blue\u0026#34;, cex = 0.5) lines(S.lower.pmf, pch = 20, col = \u0026#34;magenta\u0026#34;, cex = 0.5)   curve(Fs.NP2, xlim = c(320, 390), ylim = c(0.93, 0.98), col = \u0026#34;red\u0026#34;, lwd = 1) lines(300:450, pgamma(300:450 - k, shape = gamma, rate = c), col = \u0026#34;green\u0026#34;, lwd = 1, type = \u0026#34;l\u0026#34;) lines(S.unbia.pmf, pch = 20, cex = 0.5) lines(S.upper.pmf, pch = 20, col = \u0026#34;blue\u0026#34;, cex = 0.5) lines(S.lower.pmf, pch = 20, col = \u0026#34;magenta\u0026#34;, cex = 0.5)   When approximating a discrete distribution with a continuous one, a half-integer discontinuity correction needs to be applied. Let \\(S\\sim\\text{Poisson}(\\lambda=16)\\). Possible approximations are\n Translated gamma with \\(\\gamma=64\\), \\(c=2\\) and \\(k=-16\\). CLT with \\(E[S]=Var(S)=16\\)  Results are below:\nNote that the discretisation techniques take this into account.\nEdgeworth‚Äôs approximation #  Let $$\\Phi^{(k)}(x)=\\frac{d^k}{dx^k}\\Phi(x).$$ We have then (each term improves the accuracy) $$\\Pr\\left[ \\frac{S-E[S] }{\\sqrt{Var(S)} }\\leq z\\right] \\approx\\Phi(z)-\\frac{\\gamma_1}{6}\\Phi^{(3)}(z)+\\frac{\\gamma_2}{24}\\Phi^{(4)}(z)+\\frac{\\gamma_1^2}{72}\\Phi^{(6)}(z).$$ Note\n$$\\begin{array}{rcl} \\Phi^{(1)}(x)\u0026amp;=\u0026amp;\\varphi(x)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}x^2} \\\\ \\Phi^{(2)}(x)\u0026amp;=\u0026amp;-x\\varphi(x) \\\\ \\Phi^{(3)}(x)\u0026amp;=\u0026amp;(x^2-1)\\varphi(x) \\\\ \u0026amp; \\text{etc} \\ldots \u0026amp; \\end{array}$$\n\\(\\qquad\\) Problem is: it sometimes leads to negative probabilities‚Ä¶\nApproximating the Individual Model with the Collective Model #  Mainly motivated by the properties of compound distributions, in particular the compound Poisson:\n aggregation recursive methods  We want to approximate \\(\\widetilde{S}\\) (IRM) by \\(S\\) (CRM): $$\\widetilde{S}=\\sum_{i=1}^{n}I_{i}b_{i}\\approx S=\\sum_{i=1}^{n}N_{i}b_{i},$$ where \\(N_i\\sim\\text{Poisson}(\\lambda_i)\\) represents the number of claims of amount \\(b_i\\). \\(S\\) is then compound Poisson with parameters $$\\lambda =\\sum_{i=1}^{n}\\lambda _{i}\\text{ \\ and \\ }P\\left(x\\right) =\\sum_{i=1}^{n}\\frac{\\lambda _{i}}{\\lambda }I_{\\left[b_{i},\\infty \\right) }\\left( x\\right).$$\nTwo possible assumptions for \\(\\lambda_i\\) #   \\(\\lambda_i=E[I_i]=q_i\\): the canonical collective model  expected number of claims of size \\(b_{i}\\) will be the same between the two models also implies equal expected claim numbers in both \\(\\widetilde{S}\\) and \\(S\\) finally $$\\begin{array}{rcl} E\\left( \\widetilde{S}\\right)=\\sum_{i=1}^{n}q_{i}b_{i} \u0026amp;=\u0026amp;\\lambda \\sum_{i=1}^{n}\\frac{q_{i}}{\\lambda }b_{i}=E\\left( S\\right),\\text{ but} \\\\ Var\\left( \\widetilde{S}\\right) =\\sum_{i=1}^{n}q_{i}\\left(1-q_{i}\\right) b_{i}^{2} \u0026amp; \u0026lt; \u0026amp; \\sum_{i=1}^{n}q_{i}b_{i}^{2}=Var\\left( S\\right). \\end{array}$$   \\(\\lambda_i=-\\ln (1-q_i) \u0026gt;q_i\\)  \\(\\Pr[S=0]=\\Pr[\\widetilde{S}=0]\\)    Example #  qi \u0026lt;- c(0.01, 0.02, 0.03, 0.01, 0.05) bi \u0026lt;- c(10, 20, 10, 20, 30) set.seed(10052021) nsim \u0026lt;- 1e+07 simulated \u0026lt;- cbind(c(rbinom(nsim, 1, qi[1]) * bi[1]), c(rbinom(nsim, 1, qi[2]) * bi[2]), c(rbinom(nsim, 1, qi[3]) * bi[3]), c(rbinom(nsim, 1, qi[4]) * bi[4]), c(rbinom(nsim, 1, qi[5]) * bi[5])) simul \u0026lt;- rowSums(simulated) mean(simul) ## [1] 2.497127  plot(ecdf(simul))   lambdai.1 \u0026lt;- qi lambda.1 \u0026lt;- sum(lambdai.1) pmf.1 \u0026lt;- c(rep(0, 10), (lambdai.1[1] + lambdai.1[3])/lambda.1, rep(0, 9), (lambdai.1[2] + lambdai.1[4])/lambda.1, rep(0, 9), lambdai.1[5]/lambda.1) fs.1 \u0026lt;- aggregateDist(\u0026#34;recursive\u0026#34;, model.freq = \u0026#34;poisson\u0026#34;, model.sev = pmf.1, lambda = lambda.1) plot(fs.1, ylim = c(0.85, 1), pch = \u0026#34;-\u0026#34;) lines(ecdf(simul), col = \u0026#34;red\u0026#34;, lwd = 2)   lambdai.2 \u0026lt;- -log(1 - qi) lambda.2 \u0026lt;- sum(lambdai.2) pmf.2 \u0026lt;- c(rep(0, 10), (lambdai.2[1] + lambdai.2[3])/lambda.2, rep(0, 9), (lambdai.2[2] + lambdai.2[4])/lambda.2, rep(0, 9), lambdai.2[5]/lambda.2) fs.2 \u0026lt;- aggregateDist(\u0026#34;recursive\u0026#34;, model.freq = \u0026#34;poisson\u0026#34;, model.sev = pmf.2, lambda = lambda.2) plot(fs.2, ylim = c(0.85, 1), pch = \u0026#34;-\u0026#34;) lines(ecdf(simul), col = \u0026#34;red\u0026#34;, lwd = 2)  Bringing it all together: SUVA case study #   We come back to the SUVA data; see Avanzi, Cassar, and Wong (2011). The right tail of the medical costs will be best modelled using an Extreme Value distribution (see Module 6), but this will result in a poor fit in the left tail. As a consequence we will fit the SUVA medical costs using two layers, \\(S_{\\text{sc}}\\) and \\(S_{\\text{lc}}\\). If \\(S\\) is one medical cost claim (which can be scaled up easily), then $$S = S_{\\text{sc}}+S_{\\text{lc}}= \\sum_{i=1}^{N_{\\text{sc}}} X_{i,\\text{sc}} + \\sum_{j=1}^{N_{\\text{lc}}} X_{j,\\text{lc}},$$ where \\(N_{\\text{sc}}\\sim\\text{Poi}(\\lambda_{\\text{sc}})\\) and \\(N_{\\text{lc}}\\sim\\text{Poi}(\\lambda_{\\text{lc}})\\), all mutually independent with the \\(X_{\\text{sc}}\\) and \\(X_{\\text{lc}}\\).    The small claims layer can be tackled in different ways:  use the empirical distribution for \\(X_{\\text{sc}}\\) and get the distribution of \\(S_{\\text{sc}}\\) with Panjer. This is OK if the data is smooth enough and we don‚Äôt necessarily want the smoothing effect of a parametric distribution. use an approximation such as translated gamma or normal power on \\(S_{\\text{sc}}\\). This has the potential of smoothing out a coarse empirical distribution.   The large claims layer fits \\(X_{\\text{lc}}\\) using EVT and then uses Panjer to get the distribution of \\(S_{\\text{lc}}\\).  A threshold of $10,000 is chosen; see Module 6 for a justification.    Preliminaries #  Basic parameters:\nM \u0026lt;- 10000 data.scale \u0026lt;- 100 Loading data:\nSUVA \u0026lt;- read_excel(\u0026#34;SUVA.xls\u0026#34;) as_tibble(SUVA) ## # A tibble: 2,326 x 2 ## medcosts dailyallow ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 407 0 ## 2 12591 13742 ## 3 269 0 ## 4 142 0 ## 5 175 0 ## 6 298 839 ## 7 47 0 ## 8 59 0 ## 9 191 7446 ## 10 159 0 ## # ... with 2,316 more rows  Split claims #  Xsc \u0026lt;- SUVA$medcosts[SUVA$medcosts \u0026lt;= M] Xsc \u0026lt;- Xsc[Xsc \u0026gt; 0] Xlc \u0026lt;- SUVA$medcosts[SUVA$medcosts \u0026gt; M] lambda.sc \u0026lt;- length(Xsc)/(length(Xsc) + length(Xlc)) lambda.lc \u0026lt;- length(Xlc)/(length(Xsc) + length(Xlc)) c(lambda.sc, lambda.lc) ## [1] 0.97109827 0.02890173  Modelling the small claims #  plotdist(Xsc)  Xsc.cullen \u0026lt;- descdist(Xsc, boot = 1000) Xsc.cullen ## summary statistics ## ------ ## min: 15 max: 9854 ## median: 240 ## mean: 699.1978 ## estimated sd: 1390.096 ## estimated skewness: 4.010796 ## estimated kurtosis: 20.79387   \\(S_{\\text{sc}}\\) using the empirical distribution of \\(X_{\\text{sc}}\\) and Panjer #  Xsc.emp \u0026lt;- ecdf(Xsc) Xsc.emp.pmf \u0026lt;- discretize(Xsc.emp, from = 0, to = M, step = data.scale, method = \u0026#34;rounding\u0026#34;) sum(Xsc.emp.pmf) ## [1] 1  # and this the cdf Xsc.emp.df \u0026lt;- cumsum(Xsc.emp.pmf) # Panjer Ssc.emp.df \u0026lt;- aggregateDist(method = \u0026#34;recursive\u0026#34;, model.freq = \u0026#34;poisson\u0026#34;, model.sev = Xsc.emp.pmf, lambda = lambda.sc, x.scale = data.scale, maxit = 1000) # Ssc pmf Ssc.emp.pmf \u0026lt;- diff(Ssc.emp.df) # plotting the fit plot(Xsc.emp(0:10000), type = \u0026#34;l\u0026#34;, col = \u0026#34;black\u0026#34;) lines((0:(M/data.scale) * data.scale), Ssc.emp.df((0:(M/data.scale) * data.scale)), pch = \u0026#34;-\u0026#34;, col = \u0026#34;blue\u0026#34;)  \\(S_{\\text{sc}}\\) using an translated gamma approximation #  # Moments of Xsc: moments \u0026lt;- c(Xsc.cullen$mean, Xsc.cullen$sd^2, Xsc.cullen$skewness) # getting parameters of translated gamma: gamma \u0026lt;- (2/moments[3])^2 c \u0026lt;- (gamma/moments[2])^(1/2) k \u0026lt;- moments[1] - gamma/c # checking moments match: c(gamma/c + k, moments[1]) ## [1] 699.1978 699.1978  c((gamma/c^2), moments[2]) ## [1] 1932366 1932366  c(2 * gamma^(-1/2), moments[3]) ## [1] 4.010796 4.010796   No Panjer here - just discretisation of the gamma:\nSsc.tlg.pmf \u0026lt;- discretize(pgamma(x - k, shape = gamma, rate = c), from = 0, to = 3 * M, step = data.scale, method = \u0026#34;rounding\u0026#34;) Ssc.tlg.df \u0026lt;- cumsum(Ssc.tlg.pmf) plot(Xsc.emp(0:10000), type = \u0026#34;l\u0026#34;, col = \u0026#34;black\u0026#34;) lines((0:(M/data.scale) * data.scale), Ssc.tlg.df[1:(M/data.scale + 1)], pch = \u0026#34;-\u0026#34;, col = \u0026#34;blue\u0026#34;) One can see with this graph that the right tail is good (as expected), but the left tail not so much. It will be interesting to compare final results with the ecdf version (which would have a better match on this graph obviously)\n Comparison #  plot((0:(M/data.scale) * data.scale), Ssc.tlg.df[1:(M/data.scale + 1)], pch = \u0026#34;-\u0026#34;, col = \u0026#34;blue\u0026#34;, xlim = c(0, 10000), ylim = c(0, 1), ylab = \u0026#34;df\u0026#34;) lines(Ssc.emp.df, pch = \u0026#34;-\u0026#34;, col = \u0026#34;red\u0026#34;) lines(Xsc.emp(0:10000), type = \u0026#34;l\u0026#34;, col = \u0026#34;black\u0026#34;)   plot((0:(M/data.scale) * data.scale), Ssc.tlg.df[1:(M/data.scale + 1)], pch = \u0026#34;-\u0026#34;, col = \u0026#34;blue\u0026#34;, xlim = c(2000, 10000), ylim = c(0.9, 1), ylab = \u0026#34;df\u0026#34;) lines(Ssc.emp.df, pch = \u0026#34;-\u0026#34;, col = \u0026#34;red\u0026#34;) lines(Xsc.emp(0:10000), type = \u0026#34;l\u0026#34;, col = \u0026#34;black\u0026#34;)  Modelling the large claims #  fit.SUVA \u0026lt;- fevd(SUVA$medcosts, threshold = M, type = \u0026#34;GP\u0026#34;, time.units = \u0026#34;1/year\u0026#34;) # do we have mass below M? pextRemes(fit.SUVA, M, lower.tail = TRUE) ## [1] 0  # getting the pmf: Xlc.pmf \u0026lt;- discretize(pextRemes(fit.SUVA, x, lower.tail = TRUE), from = 0, to = 1e+06, step = data.scale) sum(Xlc.pmf) ## [1] 1  # distribution of Slc Slc.df \u0026lt;- aggregateDist(method = \u0026#34;recursive\u0026#34;, model.freq = \u0026#34;poisson\u0026#34;, model.sev = Xlc.pmf, lambda = lambda.lc, x.scale = data.scale, maxit = 50000) plot(Slc.df, ylim = c(lambda.sc, 1)) # pmf of Slc Slc.pmf \u0026lt;- diff(Slc.df)  Getting the overall distribution #  Getting ready #  # we\u0026#39;ll put the masses in here Stotal.emp.pmf \u0026lt;- c() Stotal.tlg.pmf \u0026lt;- c() # working out vector sizes lastpoint \u0026lt;- 1e+06 masses \u0026lt;- lastpoint/data.scale + 1 # adding 0\u0026#39;s as required for the convolutions to work Ssc.emp.pmf \u0026lt;- c(Ssc.emp.pmf, rep(0, masses - length(Ssc.emp.pmf))) Ssc.tlg.pmf \u0026lt;- c(Ssc.tlg.pmf, rep(0, masses - length(Ssc.tlg.pmf))) Slc.pmf \u0026lt;- c(Slc.pmf, rep(0, masses - length(Slc.pmf))) Performing the convolutions #  # and the convolutions for (i in 1:masses) { Stotal.emp.pmf[i] \u0026lt;- sum(Ssc.emp.pmf[1:i] * Slc.pmf[i:1]) Stotal.tlg.pmf[i] \u0026lt;- sum(Ssc.tlg.pmf[1:i] * Slc.pmf[i:1]) } sum(Stotal.emp.pmf) ## [1] 0.999998  sum(Stotal.tlg.pmf) ## [1] 0.9999981  Stotal.emp.cdf \u0026lt;- cumsum(Stotal.emp.pmf) Stotal.tlg.cdf \u0026lt;- cumsum(Stotal.tlg.pmf) Examining the ECDF approach fit #  plot(Stotal.emp.cdf, pch = 20, xlim = c(100, 1100), ylim = c(0.98, 1)) lines(ecdf(SUVA$medcosts[SUVA$medcosts \u0026gt; 0]/data.scale), pch = 20, col = \u0026#34;blue\u0026#34;) plot(Stotal.emp.cdf, pch = 20, xlim = c(10, 500), ylim = c(0.95, 1)) lines(ecdf(SUVA$medcosts[SUVA$medcosts \u0026gt; 0]/data.scale), pch = 20, col = \u0026#34;blue\u0026#34;) plot(Stotal.emp.cdf, pch = 20, xlim = c(0, 500), ylim = c(0.9, 1)) lines(ecdf(SUVA$medcosts[SUVA$medcosts \u0026gt; 0]/data.scale), pch = 20, col = \u0026#34;blue\u0026#34;) plot(Stotal.emp.cdf, pch = 20, xlim = c(0, 20), ylim = c(0, 1)) lines(ecdf(SUVA$medcosts[SUVA$medcosts \u0026gt; 0]/data.scale), pch = 20, col = \u0026#34;blue\u0026#34;)     Examining the translated gamma approach fit #  plot(Stotal.tlg.cdf, pch = 20, xlim = c(100, 1100), ylim = c(0.98, 1)) lines(ecdf(SUVA$medcosts[SUVA$medcosts \u0026gt; 0]/data.scale), pch = 20, col = \u0026#34;blue\u0026#34;) plot(Stotal.tlg.cdf, pch = 20, xlim = c(10, 500), ylim = c(0.95, 1)) lines(ecdf(SUVA$medcosts[SUVA$medcosts \u0026gt; 0]/data.scale), pch = 20, col = \u0026#34;blue\u0026#34;) plot(Stotal.tlg.cdf, pch = 20, xlim = c(0, 500), ylim = c(0.9, 1)) lines(ecdf(SUVA$medcosts[SUVA$medcosts \u0026gt; 0]/data.scale), pch = 20, col = \u0026#34;blue\u0026#34;) plot(Stotal.tlg.cdf, pch = 20, xlim = c(0, 20), ylim = c(0, 1)) lines(ecdf(SUVA$medcosts[SUVA$medcosts \u0026gt; 0]/data.scale), pch = 20, col = \u0026#34;blue\u0026#34;)     Moments #  em1 \u0026lt;- c(emm(SUVA$medcosts[SUVA$medcosts \u0026gt; 0], 1), sum((((1:masses) - 1) * data.scale) * (Stotal.emp.pmf)), sum((((1:masses) - 1) * data.scale) * (Stotal.tlg.pmf))) em2 \u0026lt;- c(emm(SUVA$medcosts[SUVA$medcosts \u0026gt; 0], 2), sum((((1:masses) - 1) * data.scale)^2 * (Stotal.emp.pmf)), sum((((1:masses) - 1) * data.scale)^2 * (Stotal.tlg.pmf))) em3 \u0026lt;- c(emm(SUVA$medcosts[SUVA$medcosts \u0026gt; 0], 3), sum((((1:masses) - 1) * data.scale)^3 * (Stotal.emp.pmf)), sum((((1:masses) - 1) * data.scale)^3 * (Stotal.tlg.pmf))) sd \u0026lt;- (em2 - em1^2)^0.5 skew \u0026lt;- (em3 - 3 * em1 * sd^2 - em1^3)/sd^3 Smoments \u0026lt;- data.frame(rbind(em1, sd, skew)) names(Smoments) \u0026lt;- c(\u0026#34;SUVA\u0026#34;, \u0026#34;Emp\u0026#34;, \u0026#34;Tlg\u0026#34;) Smoments ## SUVA Emp Tlg ## em1 1492.765229 1478.788445 1494.851875 ## sd 5763.108871 5932.145105 5897.220298 ## skew 8.885693 9.739098 9.893872  References #  Avanzi, Benjamin, Luke C. Cassar, and Bernard Wong. 2011. ‚ÄúModelling Dependence in Insurance Claims Processes with L√©vy Copulas.‚Äù ASTIN Bulletin 41 (2): 575‚Äì609.\n Bowers, Newton L. Jr, Hans U. Gerber, James C. Hickman, Donald A. Jones, and Cecil J. Nesbitt. 1997. Actuarial Mathematics. Second. Schaumburg, Illinois: The Society of Actuaries.\n Wuthrich, Mario V. 2020. ‚ÄúNon-Life Insurance: Mathematics \u0026amp; Statistics.‚Äù Lecture notes. RiskLab, ETH Zurich; Swiss Finance Institute.\n  "},{"id":14,"href":"/docs/1-claims-modelling/m5-copulas/","title":"M5 Copulas","section":"Claims Modelling (CS2 Section 1)","content":"Dependence and multivariate modelling #  Introduction to Dependence #  Motivation #  How does dependence arise?\n Events affecting more than one variable (e.g., storm on building, car and business interruption covers) Underlying economic factors affecting more than one risk area (e.g., inflation, unemployment) Clustering/concentration of risks (e.g., household, geographical area)   Reasons for modelling dependence:\n Pricing:  get adequate risk loadings (note that dependence affects quantiles, not the mean)   Solvency assessment:  bottom up: for given risks, get capital requirements (get quantiles of aggregate quantities)   Capital allocation:  top down: for given capital, allocate portions to risk (for profitability assessment)   Portfolio structure: (or strategic asset allocation)  how do assets and liability move together?    Examples #   World Trade Centre (9/11) causing losses to Property, Life, Workers‚Äô Compensation, Aviation insurers Enron causing losses to the stock market and to Surety Bonds, Errors \u0026amp; Omissions and Directors \u0026amp; Officers underwriters Dot.com market collapse and GFC causing losses to the stock market and to insurers of financial institutions and Directors \u0026amp; Officers (D\u0026amp;O) writers WTC / Enron / stock market losses causing impairment to reinsurers solvency, so increasing credit risk on payments by reinsurers Asbestos affecting many past liability years at once Australian 2019-2020 Bushfires causing losses to Property, Life, credit, etc \\(\\ldots\\) Covid-19 impacting financial markets, travel insurance, health, credit, D\u0026amp;O, business interruption covers, etc \\(\\dots\\)  Example of real actuarial data (Avanzi, Cassar, and Wong 2011) #   Data were provided by the SUVA (Swiss workers compensation insurer) Random sample of 5% of accident claims in construction sector with accident year 1999 (developped as of 2003) 1089 of those are common (!) Two types of claims: 2249 medical cost claims, et 1099 daily allowance claims  SUVA \u0026lt;- read_excel(\u0026#34;SUVA.xls\u0026#34;) # filtering and logging the common claims SUVAcom \u0026lt;- log(SUVA[SUVA$medcosts \u0026gt; 0 \u0026amp; SUVA$dailyallow \u0026gt; 0, ])  Scatterplot of those 1089 common claims (LHS) amd their log (RHS):\npar(mfrow = c(1, 2), pty = \u0026#34;s\u0026#34;) plot(exp(SUVAcom), pch = 20, cex = 0.5) plot(SUVAcom, pch = 20, cex = 0.5)  Scatterplot of the log of the 1089 common claims (LHS) and their empirical copula (RHS):\npar(mfrow = c(1, 2), pty = \u0026#34;s\u0026#34;) plot(SUVAcom, pch = 20, cex = 0.5) plot(copula::pobs(SUVAcom)[, 1], copula::pobs(SUVAcom)[, 2], pch = 20, cex = 0.5) There is obvious right tail dependence.\nMultivariate Normal Distributions #  The multivariate Normal distribution #  \\(\\mathbf{Z}=\\left( Z_{1},...Z_{n}\\right) ^{\\prime }\\sim\\) \\(MN\\left(\\mathbf{0},\\Sigma \\right)\\) if its joint p.d.f. is \\(f\\left(z_{1},...,z_{n}\\right) =\\dfrac{1}{\\sqrt{\\left( 2\\pi \\right)^{n}\\left\\vert \\Sigma\\right\\vert }}\\exp \\left\\{-\\frac{1}{2}\\mathbf{z}^{\\prime } \\Sigma^{-1}\\mathbf{z}\\right\\}\\).\n standard Normal marginals, i.e.¬†\\(Z_{i}\\sim N\\left( 0,1\\right)\\) positive definite correlation matrix:  $$ \\Sigma = \\begin{pmatrix} 1 \u0026amp; \\rho_{12} \u0026amp; \\cdots \u0026amp; \\rho_{1n} \\\\\\\\ \\rho_{21} \u0026amp; 1 \u0026amp; \\cdots \u0026amp; \\rho_{2n} \\\\\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\\\\ \\rho_{n1} \u0026amp; \\rho_{n2} \u0026amp; \\cdots \u0026amp; 1 \\end{pmatrix} $$\nwhere \\(\\rho_{ij} = \\rho_{ji}\\) is the correlation between \\(Z_{i}\\) and \\(Z_{j}\\).\n if \\(\\rho _{ij}=0\\) for all \\(i\\neq j\\), then we have the standard MN.  Properties #  If \\(\\mathbf{Z}\\sim MN\\left(\\mathbf{0},{\\Sigma }\\right)\\), then with appropriate dimensions \\(\\mathbf{A}\\) and \\(\\mathbf{C}\\), the vector\n$$\\mathbf{X=AZ+C}$$\nhas a multivariate Normal distribution with mean\n$$\\text{E}\\left( \\mathbf{X}\\right) =\\mathbf{C}$$\nand covariance\n$$\\text{Cov}\\left( \\mathbf{X}\\right) = \\mathbf{A} \\Sigma\\mathbf{A}^\\prime.$$\nCholesky‚Äôs decomposition #  We can construct a lower triangular matrix\n$$B = \\begin{pmatrix} b_{11} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ b_{21} \u0026amp; b_{22} \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ b_{n1} \u0026amp; b_{n2} \u0026amp; \\cdots \u0026amp; b_{nn} \\end{pmatrix}$$\nsuch that \\(\\Sigma =\\mathbf{BB}^{\\top}\\).\nThe matrix \\(\\mathbf{B}\\) can be determined using Cholesky‚Äôs decomposition algorithm (standard function in most software‚Äîsee chol() in R).\nThis will be useful later on for the simulation of multivariate Gaussian random variables.\nMeasures of dependence #  Pearson‚Äôs correlation measure #  Pearson‚Äôs correlation coefficient is defined by\n$$\\rho \\left( Z_{i},Z_{j}\\right) =\\rho _{ij}=\\frac{\\text{Cov}\\left( Z_{i},Z_{j}\\right) }{\\sqrt{\\text{Var}\\left( Z_{i}\\right) \\text{Var}\\left( Z_{j}\\right) }}.$$\nNote:\n This measures the degree of linear relationship. It does not reveal all the information on the dependence structure of random couples.  Kendall‚Äôs tau #  Kendall‚Äôs tau correlation coefficient is defined by\n$$\\begin{array}{rcl} \\tau \\left( Z_{i},Z_{j}\\right) \u0026amp;=\u0026amp; \\tau _{ij} \\\\ \u0026amp;=\u0026amp;P\\left[ \\left( Z_{i}-Z_{i}^{\\prime }\\right) \\left( Z_{j}-Z_{j}^{\\prime }\\right) \u0026gt;0\\right] -P\\left[ \\left( Z_{i}-Z_{i}^{\\prime }\\right) \\left( Z_{j}-Z_{j}^{\\prime }\\right) \u0026lt;0\\right] \\end{array}$$\nwhere \\(\\left( Z_{i},Z_{j}\\right)\\) and \\(\\left( Z_{i}^{\\prime},Z_{j}^{\\prime }\\right)\\) are two independent realisations.\nNote:\n The first term is called the probability of concordance; the latter, probability of discordance. Its value is also between -1 and 1. It can be shown to equal: \\(\\tau \\left( Z_{i},Z_{j}\\right) =4E\\left[F\\left( Z_{i},Z_{j}\\right) \\right] -1\\). Concordance and discordance only depends on ranks, and this indicator is hence less affected by the marginal distributions of \\(Z_i\\) and \\(Z_j\\) than Pearson‚Äôs correlation.  (Spearman‚Äôs) rank correlation #  Spearman‚Äôs rank correlation coefficient is defined by\n$$r\\left( Z_{i},Z_{j}\\right) =r_{ij}=\\rho \\left( F_{i}\\left( Z_{i}\\right) ,F_{j}\\left( Z_{j}\\right) \\right)$$\nwhere \\(F_{i}\\) and \\(F_{j}\\) are the respective marginal distributions.\nNote:\n It is indeed the Pearson‚Äôs correlation but applied to the transformed variables \\(F_{i}\\left( Z_{i}\\right)\\) and \\(F_{j}\\left( Z_{j}\\right)\\). Its value is also between -1 and 1. It is directly formulated on ranks, and hence is less affected by the marginal distributions of \\(Z_i\\) and \\(Z_j\\) than Pearson‚Äôs correlation.  Example: the case of multivariate Normal #   Pearson‚Äôs correlation: \\(\\rho _{ij}\\) Kendall‚Äôs tau: \\(\\tau _{ij}=\\dfrac{2}{\\pi }\\arcsin \\left( \\rho_{ij}\\right)\\) Spearman‚Äôs rank correlation: \\(r_{ij}=\\dfrac{6}{\\pi }\\arcsin \\left(\\dfrac{\\rho _{ij}}{2}\\right)\\)  Example: SUVA data #  cor(SUVAcom, method = \u0026#34;pearson\u0026#34;) # default ## medcosts dailyallow ## medcosts 1.0000000 0.7489701 ## dailyallow 0.7489701 1.0000000  cor(SUVAcom, method = \u0026#34;kendall\u0026#34;) ## medcosts dailyallow ## medcosts 1.0000000 0.5154526 ## dailyallow 0.5154526 1.0000000  cor(SUVAcom, method = \u0026#34;spearman\u0026#34;) ## medcosts dailyallow ## medcosts 1.0000000 0.6899156 ## dailyallow 0.6899156 1.0000000   Repeating those on the original claims (before \\(\\log\\) transformation):\ncor(exp(SUVAcom), method = \u0026#34;pearson\u0026#34;) # default ## medcosts dailyallow ## medcosts 1.0000000 0.8015752 ## dailyallow 0.8015752 1.0000000  cor(exp(SUVAcom), method = \u0026#34;kendall\u0026#34;) ## medcosts dailyallow ## medcosts 1.0000000 0.5154526 ## dailyallow 0.5154526 1.0000000  cor(exp(SUVAcom), method = \u0026#34;spearman\u0026#34;) ## medcosts dailyallow ## medcosts 1.0000000 0.6899156 ## dailyallow 0.6899156 1.0000000  We see that Kendall‚Äôs \\(\\tau\\) and Spearman‚Äôs \\(r\\) are unchanged. This is because the \\(\\log\\) transformation does not affect ranks in the data. The more extreme nature of the data, however, leads to a higher Pearson‚Äôs correlation coefficient.\nLimits of correlation #  Correlation = dependence? #  Correlation between the consumption of cheese and deaths by becoming tangled in bedsheets (in the US, see Vigen 2015):\nCorrelation = 0.95!!\nCommon fallacies #  Fallacy 1: a small correlation \\(\\rho(X_1,X_2)\\) implies that \\(X_1\\) and \\(X_2\\) are close to being independent\n wrong! Independence implies zero correlation BUT A correlation of zero does not always mean independence. See example 1 below.  Fallacy 2: marginal distributions and their correlation matrix uniquely determine the joint distribution.\n This is true only for elliptical families (including multivariate normal), but wrong in general! See example 2 below.  Example 1 #  Company‚Äôs two risks \\(X_1\\) and \\(X_2\\)\n Let \\(Z\\sim N(0,1)\\) and $ (U=-1)=1/2=(U=1)$ \\(U\\) stands for an economic stress generator, independent of \\(Z\\) Consider: $$X_1=Z\\sim N(0,1)$$ and $$X_2=UZ\\sim N(0,1).$$ Now Cov$(X_1,X_2)=E(X_1X_2)=E(UZ^2)=E(U)E(Z^2)=0$ hence \\(\\rho(X_1,X_2)=0\\). However, \\(X_1\\) and \\(X_2\\) are strongly dependent, with 50% probability co-monotone and 50% counter\u0026ndash;monotone.  This example can be made more realistic\nExample 2 #  Marginals and correlations‚Äînot enough to completely determine joint distribution\nConsider the following example:\n Marginals: Gamma \\(\\left( 5,1\\right)\\) Correlation: \\(\\rho =0.75\\) Different dependence structures: Normal copula vs Cook-Johnson copula More generally, check the Copulatheque  Example 2 illustration: Normal vs Cook-Johnson copulas #  Copula theory #  What is a copula? #  Sklar‚Äôs representation theorem #  The copula couples, links, or connects the joint distribution to its marginals.\nSklar (1959): There exists a copula function \\(C\\) such that $$F\\left( x_{1},x_{2},...,x_{n}\\right) =C\\left( F_{1}\\left( x_{1}\\right) ,F_{2}\\left( x_{2}\\right) ,...,F_{n}\\left( x_{n}\\right) \\right)$$ where \\(F_{k}\\) is the marginal df of \\(X_{k}\\), \\(k=1,2,...,n\\). Equivalently, $$\\Pr\\left( X_{1}\\leq x_{1},...,X_{n}\\leq x_{n}\\right) =C\\left( \\Pr\\left[ X_{1}\\leq x_{1}\\right] ,...,\\Pr\\left[ X_{n}\\leq x_{n}\\right] \\right) .$$\n Under certain conditions, the copula\n$$C\\left( u_{1},...,u_{n}\\right) =F\\left( F_{1}^{-1}\\left( u_{1}\\right) ,...,F_{n}^{-1}\\left( u_{n}\\right) \\right)$$\nis unique, where \\(F_{k}^{-1}\\) denote the quantile functions.\nNote:\n This is one way of constructing copulas. These are called implicit copulas. Elliptical copulas are a prominent example (e.g., Gaussian copula)  Example #  Let $$F(x,y) = \\left\\{\\begin{array}{cl} \\frac{(x+1)(e^y-1)}{x+2e^{y}-1} \u0026amp; (x,y) \\in [-1,1]\\times[0,\\infty] \\\\\\\\ 1-e^{-y} \u0026amp; (x,y)\\in(1,\\infty]\\times[0,\\infty] \\\\\\\\ 0 \u0026amp; \\text{elsewhere} \\end{array}\\right.$$ Hence\n$$\\begin{array}{rcl} F(x) \u0026amp;=\u0026amp; \\frac{x+1}{2}, \\quad x\\in[-1,1] \\\\ F^{-1}(u) \u0026amp;=\u0026amp; 2u-1 = x \\\\ G(y) \u0026amp;=\u0026amp; 1-e^{-y},\\quad y\\ge 0 \\\\ G^{-1}(v) \u0026amp;=\u0026amp; -\\ln (1-v) = y \\end{array}$$\n Finally,\n$$\\begin{array}{rcl} C(u,v) \u0026amp;=\u0026amp; \\frac{(2u-1+1)[(1-v)^{-1}-1]}{2u-1+2(1-v)^{-1}-1} \\\\ \u0026amp;=\u0026amp; \\frac{2u (1-1+v)}{(2u-2)(1-v)+2} \\\\ \u0026amp;=\u0026amp;\\frac{2uv}{2u-2uv-2+2v+2} \\\\ \u0026amp;=\u0026amp;\\frac{uv}{u+v-uv} \\\\ \u0026amp;=\u0026amp; uv \\times \\frac{1}{u+v-uv} \\end{array}$$\nSome remarks #   Independence copula: \\(C\\left( u_{1},...,u_{n}\\right) =u_{1}\\cdot \\cdot\\cdot u_{n}.\\) Other copulas generally contain parameter(s) that describe the dependence. Copula captures the dependence structure, while separating the effects of the marginals.   For \\(n=2\\), \\(C\\) is a function mapping \\(\\left[ 0,1\\right] ^{2}\\) to \\(\\left[ 0,1\\right]\\) that is non-decreasing and right continuous, and:\n \\(\\lim_{u_{k}\\rightarrow 0}C\\left( u_{1},u_{2}\\right) =0\\) for \\(k=1,2\\); \\(\\lim_{u_{1}\\rightarrow 1}C\\left( u_{1},u_{2}\\right) =u_{2}\\) and \\(\\lim_{u_{2}\\rightarrow 1}C\\left( u_{1},u_{2}\\right) =u_{1}\\); and \\(C\\) satisfies the inequality \\(C\\left( v_{1},v_{2}\\right) -C\\left(u_{1},v_{2}\\right) -C\\left( v_{1},u_{2}\\right) +C\\left(u_{1},u_{2}\\right) \\geq 0\\) for any \\(u_{1}\\leq v_{1},u_{2}\\leq v_{2}\\).  Corresponding heuristics are:\n If the event on one variable is impossible, the joint probability is impossible. If the event on one variable is certain, the joint probability boils down to the marginal of the other one. There cannot be negative probabilities.  \\(\\adv\\) Multivariate distribution function #  A function \\(F:R^{n}\\rightarrow \\lbrack 0,1]\\) is a multivariate d.f. if it satisfies:\n right-continuous; \\(\\lim_{x_{i}\\rightarrow -\\infty }F\\left( x_{1},...,x_{n}\\right) =0\\) for \\(i=1,2,...,n;\\) \\(\\lim_{x_{i}\\rightarrow \\infty ,\\forall i}F\\left(x_{1},...,x_{n}\\right) =1;\\) and rectangle inequality holds: for all \\(\\left( a_{1},...,a_{n}\\right)\\) and \\(\\left( b_{1},...,b_{n}\\right)\\) with \\(a_{i}\\leq b_{i}\\) for \\(i=1,...,n\\), we have  $$\\sum_{i_{1}=1}^{2}\\cdots \\sum_{i_{n}=1}^{2}\\left( -1\\right) ^{i_{1}+\\cdots +i_{n}}F\\left( x_{1i_{1}},...,x_{ni_{n}}\\right) \\geq 0,$$\nwhere \\(x_{i1}=a_{i}\\) and \\(x_{i2}=b_{i}\\).\n\\(\\adv\\) Multivariate copula #  A copula \\(C:[0,1]^{n}\\rightarrow \\lbrack 0,1]\\) is a multivariate distribution function whose univariate marginals are Uniform \\((0,1)\\) .\nProperties of a multivariate copula:\n \\(C\\left( u_{1},...,u_{k-1},0,u_{k+1},...,u_{n}\\right) =0\\) \\(C\\left( 1,...,1,u_{k},1,...,1\\right) =u_{k}\\) the rectangle inequality leads us to  $$\\begin{array}{rcl} \u0026amp;\u0026amp;P\\left( a_{1}\\leq U_{1}\\leq b_{1},...,a_{n}\\leq U_{n}\\leq b_{n}\\right) \\\\ \u0026amp;=\u0026amp;\\sum_{i_{1}=1}^{2}\\cdots \\sum_{i_{n}=1}^{2}\\left( -1\\right) ^{i_{1}+\\cdots +i_{n}}C\\left( u_{1i_{1}},...,u_{ni_{n}}\\right) \\geq 0 \\end{array}$$\nfor all \\(u_{i}\\in \\lbrack 0,1]\\), \\(\\left( a_{1},...,a_{n}\\right)\\) and \\(\\left( b_{1},...,b_{n}\\right)\\) with \\(a_{i}\\leq b_{i},\\) and \\(u_{i1}=a_{i}\\) and \\(u_{i2}=b_{i}.\\)\nHeuristics are the same as before.\nDensity associated with a copula #  For continuous marginals with respective pdf \\(f_{1},...f_{n}\\), the joint pdf of \\(\\mathbf{X}\\) can be written as\n$$f\\left( x_{1},...,x_{n}\\right) =f_{1}\\left( x_{1}\\right) \\cdot \\cdot \\cdot f_{n}\\left( x_{n}\\right) \\times c\\left( F_{1}\\left( x_{1}\\right) ,...,F_{n}\\left( x_{n}\\right) \\right)$$\nwhere the copula density \\(c\\) is given by\n$$c\\left( u_{1},...,u_{n}\\right) =\\frac{\\partial ^{n}C\\left( u_{1},...,u_{n}\\right) }{\\partial u_{1}\\partial u_{2}\\cdot \\cdot \\cdot \\partial u_{n}}.$$\nNote:\n The copula \\(c\\) distorts the independence to induce the actual dependence structure. If independent, \\(c\\left( u_{1},...,u_{n}\\right)=1\\).  Example #  Let \\(X\\) and \\(Y\\) be two random variables and the copula \\(C\\) of \\(X\\) and \\(Y\\) is\n$$C(u,v)=\\frac{uv}{u+v-uv}.$$\nDerive its associated density \\(c\\).\nSurvival copulas #  What if we want to working with (model) survival functions $$\\overline{F}_i(x_i)=1-F_i(x_i)\\left(=S_i(x_i)\\right)$$ rather than distribution functions?\n\\(\\longrightarrow\\) We can couple \\(\\overline{F}_i\\)‚Äôs with the survival copulas \\(\\overline{C}\\).\nIn the bivariate case, this yields $$\\overline{F}(x_1,x_2)=\\Pr[X_1\u0026gt;x_1,X_2\u0026gt;x_2]=\\overline{C}\\left(\\overline{F}_1(x_1),\\overline{F}_2(x_2)\\right),$$ where $$\\overline{C}(1-u,1-v)=1-u-v+C(u,v).$$ This is because\n$$\\begin{array}{rcl} \\Pr[X_1\u0026gt;x_1,X_2\u0026gt;x_2]\u0026amp;=\u0026amp;1-\\Pr[X_1 \\le x_1]-\\Pr[X_2 \\le x_2] \\\\ \u0026amp;\u0026amp;+\\Pr[X_1\\le x_1,X_2\\le x_2]. \\end{array}$$\nInvariance property #   Suppose random vector \\(\\mathbf{X}\\) has copula \\(C\\) and suppose \\(T_{1},...,T_{n}\\) are non-decreasing continuous functions of \\(X_{1},...,X_{n}\\) , respectively. The random vector defined by \\(\\left( T_{1}\\left( X_{1}\\right),...,T_{n}\\left( X_{n}\\right) \\right)\\) has the same copula \\(C\\). Proof: see Theorem 2.4.3 (p.¬†25) of Nelsen (1999) The usefulness of this property can be illustrated in many ways. If you have a copula describing joint distribution of insurance losses of various types, and you decide the quantity of interest is a transformation (e.g.¬†logarithm) of these losses, then the multivariate distribution structure does not change. The copula is then also invariant to inflation. Only the marginals distributions change.   Empirical copula of the 1089 common claims (LHS) and of their log (RHS):\npar(mfrow = c(1, 2), pty = \u0026#34;s\u0026#34;) plot(copula::pobs(exp(SUVAcom))[, 1], copula::pobs(exp(SUVAcom))[, 2], pch = 20, cex = 0.5) plot(copula::pobs(SUVAcom)[, 1], copula::pobs(SUVAcom)[, 2], pch = 20, cex = 0.5) Main bivariate copulas #  The Fr√©chet bounds #  Define the Fr√©chet bounds as:\n Fr√©chet lower bound: \\(L_{F}\\left( u_{1},...,u_{n}\\right) =\\max \\left(\\sum_{k=1}^{n}u_{k}-\\left( n-1\\right) ,0\\right)\\) Fr√©chet upper bound: \\(U_{F}\\left( u_{1},...,u_{n}\\right) =\\min \\left(u_{1},...,u_{n}\\right)\\)  Any copula function satisfies the following bounds:\n$$L_{F}\\left( u_{1},...,u_{n}\\right) \\leq C\\left(u_{1},...,u_{n}\\right) \\leq U_{F}\\left( u_{1},...,u_{n}\\right).$$\nThe Fr√©chet upper bound satisfies the definition of a copula, but the Fr√©chet lower bound does not for dimensions \\(n\\geq 3\\).\n Source: Wikipedia (2020)\nThe Normal (aka Gaussian) copula #  Recall that \\(\\mathbf{Z}=\\left( Z_{1},...Z_{n}\\right) ^{\\prime }\\sim\\) \\(MN\\left( \\mathbf{0},\\Sigma \\right)\\) if its joint p.d.f. is\n$$f\\left(z_{1},...,z_{n}\\right)=\\frac{1}{\\sqrt{\\left( 2\\pi \\right) ^{n} \\vert \\Sigma \\vert }}\\exp \\left[ -\\frac{1}{2} \\mathbf{z}^{\\prime }\\Sigma^{-1}\\mathbf{z}\\right]$$\nwhere \\(\\mathbf{z}=\\left( z_{1},...z_{n}\\right)\\).\nNow define its joint distribution by\n$$\\Phi_{\\Sigma}\\left( z_{1},...,z_{n}\\right) = \\int_{-\\infty }^{z_{n}}\\int_{-\\infty }^{z_{n-1}}\\cdot \\cdot \\cdot \\int_{-\\infty }^{z_{1}} \\frac{1}{\\sqrt{\\left( 2\\pi \\right)^{n} \\vert \\Sigma \\vert }}\\exp \\left[ -\\frac{1}{2}\\mathbf{z}^{\\prime }\\Sigma^{-1}\\mathbf{z}\\right] dz_{1}\\cdot \\cdot \\cdot dz_{n}$$\nLet \\(\\Phi(\\cdot)\\) denote the standard normal cumulative distribution function. The copula defined by\n$$C\\left( u_{1},...,u_{n}\\right) =\\Phi _{\\Sigma}\\left( \\Phi ^{-1}\\left( u_{1}\\right) ,...,\\Phi ^{-1}\\left( u_{n}\\right) \\right)$$\nis called the Normal (or Gaussian) copula.\n Illustration of the Gaussian copula:\nHere \\(\\rho=0.4\\). Source: Wikipedia (2020)\nThe \\(t\\) copula #  A r vector \\(\\mathbf{Z}=\\left( Z_{1},...Z_{n}\\right) ^{\\prime}\\sim MT \\left( \\mathbf{0},\\Sigma;\\upsilon \\right)\\) if its joint p.d.f. is\n$$f\\left( z_{1},...,z_{n}\\right) = \\frac{\\Gamma \\left( \\frac{\\upsilon +1}{2} \\right) }{\\sqrt{\\left( \\upsilon \\pi \\right) ^{n}\\left\\vert \\Sigma \\right\\vert }\\Gamma \\left( \\frac{\\upsilon }{2}\\right) }\\left( 1+\\frac{1}{ \\upsilon }\\mathbf{z}^{\\prime }\\Sigma^{-1}\\mathbf{z}\\right) ^{-(\\upsilon +n)/2}.$$\nNow define its joint distribution by\n$$T_{\\Sigma ,\\upsilon }\\left( z_{1},...,z_{n}\\right) =\\int_{-\\infty }^{z_{n}}\\int_{-\\infty }^{z_{n-1}}\\cdot \\cdot \\cdot \\int_{-\\infty }^{z_{1}} \\frac{\\Gamma \\left( \\frac{\\upsilon +1}{2}\\right) }{\\sqrt{\\left( \\upsilon \\pi \\right) ^{n}\\left\\vert \\Sigma\\right\\vert }\\Gamma \\left( \\frac{ \\upsilon }{2}\\right) }\\left( 1+\\frac{1}{\\upsilon }\\mathbf{z}^{\\prime } \\Sigma^{-1}\\mathbf{z}\\right) ^{-(\\upsilon +n)/2}dz_{1}\\cdot \\cdot \\cdot dz_{n}.$$\nLet \\(t_{\\upsilon }(\\cdot)\\) denote the cumulative distribution function of a standard univariate \\(t\\) distribution. The copula defined by\n$$C\\left( u_{1},...,u_{n}\\right) =T_{\\Sigma ,\\upsilon }\\left( t_{\\upsilon }^{-1}\\left( u_{1}\\right) ,...,t_{\\upsilon }^{-1}\\left( u_{n}\\right) \\right)$$\nis called the \\(t\\) copula.\nArchimedean copulas #  \\(C\\) is Archimedean if it has the form\n$$C\\left( u_{1},...,u_{n}\\right) =\\psi ^{-1}\\left( \\psi \\left( u_{1}\\right) +\\cdots+\\psi \\left( u_{n}\\right) \\right)$$\nfor all \\(0\\leq u_{1},...,u_{n}\\leq 1\\) and for some function \\(\\psi\\) (called the generator) satisfying:\n \\(\\psi \\left( 1\\right) =0;\\) \\(\\psi\\) is decreasing; and \\(\\psi\\) is convex.  The Clayton copula #  The Clayton copula is defined by\n$$C\\left( u_{1},...,u_{n}\\right) =\\left( \\sum_{k=1}^{n}u_{k}^{-\\theta }-n+1\\right) ^{-1/\\theta }, \\quad \\theta \\in (0,\\infty)$$\nIt is of Archimedean type with:\n \\(\\psi \\left( t\\right) =\\frac{1}{\\theta}(t^{-\\theta }-1)\\) \\(\\psi ^{-1}\\left( s\\right) =\\left( 1+\\theta s\\right) ^{-1/\\theta }\\)  Note the correspondance with Kendall‚Äôs \\(\\tau\\) (for bivariate case): $$\\theta = \\frac{2\\tau}{1-\\tau} \\quad \\Longleftrightarrow \\quad \\tau = \\frac{\\theta}{2+\\theta}$$ This copula is asymmetric with positive dependence in the left tail.\n The Clayton copula for \\(n=2\\) #  We have\n$$C\\left( u_{1},u_{2}\\right) =\\left( u_{1}^{-\\theta }+u_{2}^{-\\theta}-1\\right) ^{-1/\\theta }$$\n With parameter \\(\\tau=0.25\\)\n With parameter \\(\\tau=0.75\\)\nThe Frank copula #  The Frank copula is defined by\n$$C\\left( u_{1},...,u_{n}\\right) = \\dfrac{1}{\\theta }\\log \\left( 1+\\dfrac{\\prod_{i=1}^n\\left( e^{\\theta u_{i}}-1\\right) }{e^{\\theta}-1}\\right) , \\quad \\theta \\in \\mathbb{R} \\backslash \\{0\\} %C\\left( u_{1},...,u_{n}\\right) = \\dfrac{1}{\\alpha }\\log \\left( 1+\\dfrac{\\left( e^{\\alpha u_{1}}-1\\right) \\left( e^{\\alpha u_{2}}-1\\right) }{e^{\\alpha}-1}\\right) %\\frac{1}{\\log \\theta }\\log \\left(1+\\frac{ \\prod\\nolimits_{k=1}^{n}\\left( \\theta ^{u_{k}}-1\\right)}{\\left( \\theta -1\\right) ^{n-1}}\\right) , \\quad \\theta \\in \\mathbb{R}^+ \\backslash \\{1\\}$$\nIt is of Archimedean type with:\n \\(\\psi \\left( t\\right) =-\\log \\left( \\frac{e ^{-\\theta t}-1}{e^{-\\theta} -1}\\right)\\) \\(\\psi ^{-1}\\left( s\\right) =-\\frac{1}{\\theta }\\log \\left(1+e^{-s}\\left( e^{-\\theta}-1\\right) \\right)\\)  Note the correspondance with Kendall‚Äôs \\(\\tau\\) (for bivariate case): $$\\tau = 1-\\frac{4}{\\theta}+\\frac{4}{\\theta^2}\\int_0^\\theta \\frac{t}{e^t-1}dt.$$ This copula is symmetric.\n With parameter \\(\\tau=0.25\\)\n With parameter \\(\\tau=0.75\\)\nThe Gumbel(-Hougard) copula #  The Gumbel copula is defined by\n$$C\\left( u_{1},...,u_{n}\\right) = \\exp \\left[ -\\left(\\sum_{i=1}^n (-\\log u_i)^\\theta\\right)^{1/\\theta}\\right] \\,\\quad \\theta \\in [1,\\infty)$$\nIt is of Archimedean type with:\n \\(\\psi \\left( t\\right) = \\left( -\\log t \\right)^\\theta\\) \\(\\psi ^{-1}\\left( s\\right) =\\exp\\left\\{ -t^{1/\\theta}\\right\\}\\)  Note correspondance with Kendall‚Äôs \\(\\tau\\) (for bivariate case): $$\\theta = \\frac{1}{1-\\tau} \\quad \\Longleftrightarrow \\quad \\tau = \\frac{\\theta-1}{\\theta}$$ This copula is asymmetric with greater dependence in the right tail, which makes it often a good candidate for large claims with a common underlying cause.\n With parameter \\(\\tau=0.25\\)\n With parameter \\(\\tau=0.75\\)\nCopula models in R #  The VineCopula package #   The VineCopula package caters for many of the basic copula modelling requirements. Vine copulas (Kurowicka and Joe 2011) allow for the construction of multivariate copulas with flexible dependence structures; they are outside the scope of this Module. The package, however, has a series of modelling functions specifically designed for bivariate copula modelling via the ‚ÄúBiCop-family.‚Äù    BiCop: Creates a bivariate copula by specifying the family and parameters (or Kendall‚Äôs tau). Returns an object of class BiCop. The class has the following methods:  print, summary: a brief or comprehensive overview of the bivariate copula, respectively. plot, contour: surface/perspective and contour plots of the copula density. Possibly coupled with standard normal margins (default for contour).   For most functions, you can provide an object of class BiCop instead of specifying family, par and par2 manually.  Bivariate copulas in VineCopula #  The following bivariate copulas are available in the VineCopula package within the bicop family:\n   Copula family family par par2     Gaussian 1 (-1, 1) -   Student t 2 (-1, 1) (2,Inf)   (Survival) Clayton 3, 13 (0, Inf) -   Rotated Clayton (90¬∞ and 270¬∞) 23, 33 (-Inf, 0) -   (Survival) Gumbel 4, 14 [1, Inf) -   Rotated Gumbel (90¬∞ and 270¬∞) 24, 34 (-Inf, -1] -   Frank 5 R {0} -   (Survival) Joe 6, 16 (1, Inf) -   Rotated Joe (90¬∞ and 270¬∞) 26, 36 (-Inf, -1) -        Copula family family par par2     (Survival) Clayton-Gumbel (BB1) 7, 17 (0, Inf) [1, Inf)   Rotated Clayton-Gumbel (90¬∞ and 270¬∞) 27, 37 (-Inf, 0) (-Inf, -1]   (Survival) Joe-Gumbel (BB6) 8, 18 [1 ,Inf) [1, Inf)   Rotated Joe-Gumbel (90¬∞ and 270¬∞) 28, 38 (-Inf, -1] (-Inf, -1]   (Survival) Joe-Clayton (BB7) 9, 19 [1, Inf) (0, Inf)   Rotated Joe-Clayton (90¬∞ and 270¬∞) 29, 39 (-Inf, -1] (-Inf, 0)   (Survival) Joe-Frank (BB8) 10, 20 [1, Inf) (0, 1]   Rotated Joe-Frank (90¬∞ and 270¬∞) 30, 40 (-Inf, -1] [-1, 0)   (Survival) Tawn type 1 104, 114 [1, Inf) [0, 1]   Rotated Tawn type 1 (90¬∞ and 270¬∞) 124, 134 (-Inf, -1] [0, 1]   (Survival) Tawn type 2 204, 214 [1, Inf) [0, 1]   Rotated Tawn type 2 (90¬∞ and 270¬∞) 224, 234 (-Inf, -1] [0, 1]    All of these copulas are illustrated in the copulatheque\n Example of Gumbel copula:\ncop \u0026lt;- VineCopula::BiCop(4, 2) print(cop) ## Bivariate copula: Gumbel (par = 2, tau = 0.5)  summary(cop) ## Family ## ------ ## No: 4 ## Name: Gumbel ## ## Parameter(s) ## ------------ ## par: 2 ## ## Dependence measures ## ------------------- ## Kendall's tau: 0.5 ## Upper TD: 0.59 ## Lower TD: 0   plot(cop) Note this is for uniform margins.\n Now with a standard normal margin:\nplot(cop, type = \u0026#34;surface\u0026#34;, margins = \u0026#34;norm\u0026#34;)  Contour plots are done with normal margins as standard:\nplot(cop, type = \u0026#34;contour\u0026#34;)  But uniform margins are still possible:\nplot(cop, type = \u0026#34;contour\u0026#34;, margins = \u0026#34;unif\u0026#34;)  And so are exponential margins in both cases:\nplot(cop, type = \u0026#34;contour\u0026#34;, margins = \u0026#34;exp\u0026#34;) Conversion between dependence measures and parameters (for a given family): #   BiCopPar2Tau: computes the theoretical Kendall‚Äôs tau value of a bivariate copula for given parameter values. BiCopTau2Par: computes the parameter of a (one parameter) bivariate copula for a given value of Kendall‚Äôs tau.  Example of conversion for Clayton:\ntau \u0026lt;- BiCopPar2Tau(3, 2.5) tau ## [1] 0.5555556  theta \u0026lt;- 2 * tau/(1 - tau) theta ## [1] 2.5  BiCopTau2Par(3, tau) ## [1] 2.5  Evaluate functions related to a bivariate copula: #   BiCopPDF/BiCopCDF: evaluates the pdf/cdf of a given parametric bivariate copula. BiCopDeriv: evaluates the derivative of a given parametric bivariate copula density with respect to its parameter(s) or one of its arguments.   plot((1:999)/1000, BiCopCDF((1:999)/1000, rep(0.75, 999), cop), type = \u0026#34;l\u0026#34;, xlab = \u0026#34;\u0026#34;) lines((1:999)/1000, BiCopCDF((1:999)/1000, rep(0.5, 999), cop), type = \u0026#34;l\u0026#34;, col = \u0026#34;green\u0026#34;) lines((1:999)/1000, BiCopCDF((1:999)/1000, rep(0.25, 999), cop), type = \u0026#34;l\u0026#34;, col = \u0026#34;red\u0026#34;)  \\(\\adv\\) Simulation from bivariate copulas #  \\(\\adv\\) Reminder: simulation of a univariate random variable #   Remember that if \\(X \\in \\mathbb{R}\\) has distribution function \\(F\\) then $$ F(X) \\sim \\text{uniform}(0,1).$$ This forms the basis of most simulation techniques, as a pseudo-uniform \\(u \\in (0,1)\\) can then be mapped into a pseudo-random \\(x \\in \\mathbb{R}\\) with df F by applying $$ x = F^{-1}(u).$$  \\(\\adv\\) Overarching strategy #   We will introduce the general conditional distribution method. The overarching idea is (for the bivariate case):  simulate two independent uniform random variable \\(u\\) and \\(t\\); ‚Äútweak‚Äù \\(t\\) into a \\(v\\in[0,1]\\) so that it has the right dependence structure (w.r.t. \\(u\\)) with the help of the copula; map \\(u\\) and \\(v\\) into marginal \\(x\\) and \\(y\\) using their distribution function.   However, there are some specific, more efficient algorithms that are available for certain types of copulas (see, e.g. Nelsen 1999). In R, the function BiCopSim will simulate from a given parametric bivariate copula.  \\(\\adv\\) Preliminary: the conditional distribution function #  For the ‚Äútweak,‚Äù we will need the conditional distribution function for \\(V\\) given \\(U=u\\), which is denoted by \\(c_u(v):\\) \\begin{align*} c_u(v)\u0026amp;=\\Pr[V\\leq v|U=u] \\\\\\ \u0026amp;= \\lim_{\\Delta u \\to 0}\\frac{C(u+\\Delta u,v)-C(u,v)}{\\Delta u} \\\\ \u0026amp;=\\frac{\\partial C(u,v)}{\\partial u}. \\end{align*} In particular, we will need its inverse.\n\\(\\adv\\) Example #  For the copula $$C(u,v)=\\frac{uv}{u+v-uv}$$ we have\n$$\\begin{array}{rcl} c_u(v) \u0026amp;=\u0026amp; \\frac{v(u+v-uv)-uv(1-v)}{(u+v-uv)^2}=\\left(\\frac{v}{u+v-uv}\\right)^2 \\equiv t \\\\ c_u^{-1}(t) \u0026amp;=\u0026amp; \\frac{\\sqrt{t}u}{1-\\sqrt{t}(1-u)} \\equiv v \\end{array}$$\n\\(\\adv\\) In R #  For copulas of the BiCop family:\n BiCopHfunc: evaluates the conditional distribution function \\(c_u(v)\\) (aka h-function) of a given parametric bivariate copula. BiCopHinv: evaluates the inverse conditional distribution function \\(c_u^{-1}(v)\\) (aka inverse h-function) of a given parametric bivariate copula. BiCopHfuncDeriv: evaluates the derivative of a given conditional parametric bivariate copula (h-function) with respect to its parameter(s) or one of its arguments.  \\(\\adv\\) The conditional distribution method #  Goal: generate a pair of pseudo-random variables \\((X,Y)\\) with d.f.‚Äôs \\(F\\) and \\(G\\), respectively, with dependence structure described by the copula \\(C\\).\nAlgorithm\n Generate two independent uniform \\((0,1)\\) pseudo-random variables \\(u\\) and \\(t\\); Set \\(v=c_u^{-1}(t)\\); Map \\((u,v)\\) into \\((x,y)\\):  $$\\begin{array}{rcl} x \u0026amp;=\u0026amp; F^{-1}(u); \\\\ y \u0026amp;=\u0026amp; G^{-1}(v). \\end{array}$$\n\\(\\adv\\) Example #  Let \\(X\\) and \\(Y\\) be exponential with mean 1 and standard Normal, respectively. Furthermore, the copula describing their dependence is such as in the previous example: $$C(u,v)=\\frac{uv}{u+v-uv}$$ Furthermore, you are given the following pseudo-random (independent) uniforms: $$0.3726791, 0.6189313, 0.75949099, 0.01801882$$ Simulate two pairs of outcomes for \\((X,Y)\\).\n Use of the conditional distribution method yields\n We can use the uniforms given in the question such that  $$\\begin{array}{rcl} (u_1,t_1) \u0026amp;=\u0026amp; (0.3726791, 0.6189313) \\\\ (u_2,t_2) \u0026amp;=\u0026amp; (0.75949099, 0.01801882) \\end{array}$$\n Set \\(v_i=\\frac{u_i\\sqrt{t_i}}{1-(1-u_i)\\sqrt{t_i}}\\) for \\(i=1,2\\):  $$\\begin{array}{rcl} v_1\u0026amp;=\u0026amp;0.5788953 \\\\ v_2\u0026amp;=\u0026amp;0.1053509 \\end{array}$$\n Mapping \\((u_i,v_i)\\) into \\((x_i,y_i)\\) using $$x_i=F^{-1}(u_i)=-\\ln (1-u_i) \\text{ and } y_i=\\Phi^{-1}(v_i)\\text{ we have}$$ $$\\begin{array}{rcl} (x_1,y_1) \u0026amp;=\u0026amp; (0.466297, 0.199068) \\\\ (x_2,y_2) \u0026amp;=\u0026amp; (1.424998, -1.251638) \\end{array}$$  \\(\\adv\\) Specific algorithms #  The following algorithms are provided for illustration purposes. They are not assessable.\n\\(\\adv\\) Simulation from a Normal copula #  Let \\(C\\) be a Normal copula. The following algorithm generates \\(\\left( x_{1},...,x_{n}\\right)\\) from a random vector \\((X_1,\\cdots, X_n)\\) with marginal distribution functions \\(F_{X_1}(\\cdot),\\cdots, F_{X_n}(\\cdot)\\), and copula \\(C\\), i.e.¬†\\(Pr(X_1\\le x_1,\\cdots,X_n\\le x_n)=C(F_{X_1}(x_1),\\cdots,F_{X_n}(x_n))\\):\nThe following algorithm generates \\(\\left( x_{1},...,x_{n}\\right)\\) from the Normal copula:\n construct the lower triangular matrix \\(\\mathbf{B}\\) so that the correlation matrix \\(\\Sigma\\mathbf{=BB}^{\\top}\\) using Cholesky‚Äôs. generate a column vector of independent standard Normal rv‚Äôs \\(\\mathbf{Z}=\\left(z_{1},...,z_{n}\\right)\\). take the matrix product of \\(\\mathbf{B}\\) and \\(\\mathbf{Z}\\), i.e.¬†\\(\\mathbf{Y=BZ}\\). set \\(u_{k}=\\Phi \\left( u_{k}\\right)\\) for \\(k=1,2,...,n.\\) set \\(x_{k}=F_{X_{k}}^{-1}\\left( u_{k}\\right)\\) for \\(k=1,2,...,n.\\) \\(\\left( x_{1},...,x_{n}\\right)\\) is the desired vector with marginals \\(F_{X_{k}}\\) for \\(k=1,...,n\\) and Normal copula \\(C\\).  \\(\\adv\\) Simulation from a Clayton copula #  Let \\(C\\) be the Clayton copula. The following algorithm generates \\(\\left( x_{1},...,x_{n}\\right)\\) from a random vector \\((X_1,\\cdots, X_n)\\) with marginal distribution functions \\(F_{X_1}(\\cdot),\\cdots, F_{X_n}(\\cdot)\\), and copula \\(C\\):\n generate a column vector of independent \\(Exp\\left( 1\\right)\\) rv‚Äôs \\(\\mathbf{Y}=\\left( y_{1},...,y_{n}\\right)\\). generate \\(z\\) from a Gamma$\\left( 1/\\theta ,1\\right)$ distribution. set \\(u_{k}=\\left( 1+y_{k}/z\\right) ^{-1/\\theta }\\) for \\(k=1,2,...,n.\\) set \\(x_{k}=F_{X_{k}}^{-1}\\left( u_{k}\\right)\\) for \\(k=1,2,...,n.\\) \\(\\left( x_{1},...,x_{n}\\right)\\) is the desired vector with marginals \\(F_{X_{k}}\\) for \\(k=1,...,n\\) and Clayton copula \\(C\\).  \\(\\adv\\) Example of simulation in R #  We simulate 4000 pairs \\((u_1,u_2)\\) from the Gumbel copula (with parameter 2) defined above:\nSimul.u \u0026lt;- BiCopSim(4000, cop) head(Simul.u, 15) ## [,1] [,2] ## [1,] 0.8668961 0.81666324 ## [2,] 0.8550531 0.74222541 ## [3,] 0.3563844 0.45048881 ## [4,] 0.5360519 0.38322601 ## [5,] 0.8295606 0.79429320 ## [6,] 0.4242030 0.06711167 ## [7,] 0.8307603 0.28355200 ## [8,] 0.1084427 0.35247699 ## [9,] 0.7967743 0.81204251 ## [10,] 0.6488776 0.69449227 ## [11,] 0.8273833 0.79759802 ## [12,] 0.3718999 0.16774458 ## [13,] 0.4303679 0.10838483 ## [14,] 0.7364492 0.44447288 ## [15,] 0.5441401 0.19479543   We then need to map them into the correct margins, say two gammas of shape parameter 10 and mean 100 and 50, respectively:\nSimul.x \u0026lt;- cbind(qgamma(Simul.u[, 1], 10, 0.1), qgamma(Simul.u[, 2], 10, 0.2)) head(Simul.x, 15) ## [,1] [,2] ## [1,] 135.38638 63.72408 ## [2,] 133.32633 59.13820 ## [3,] 85.71913 46.44033 ## [4,] 99.52009 43.88456 ## [5,] 129.30685 62.22275 ## [6,] 90.88140 28.69370 ## [7,] 129.48515 40.00710 ## [8,] 63.29170 42.70960 ## [9,] 124.76622 63.40319 ## [10,] 109.03066 56.66416 ## [11,] 128.98570 62.43667 ## [12,] 86.90599 34.91303 ## [13,] 91.34963 31.64224 ## [14,] 117.64726 46.21116 ## [15,] 100.16557 36.20670   plot(Simul.x, pch = \u0026#34;.\u0026#34;)  If we use the packages ggplot2, ggpubr and ggExtra one can superimpose density plots:\ndata \u0026lt;- tibble(x1 = Simul.x[, 1], x2 = Simul.x[, 2]) sp \u0026lt;- ggscatter(data, x = \u0026#34;x1\u0026#34;, y = \u0026#34;x2\u0026#34;, size = 0.05) ggMarginal(sp, type = \u0026#34;density\u0026#34;)  ‚Ä¶ or boxplots:\nsp2 \u0026lt;- ggscatter(data, x = \u0026#34;x1\u0026#34;, y = \u0026#34;x2\u0026#34;, size = 0.05) ggMarginal(sp2, type = \u0026#34;boxplot\u0026#34;)  ‚Ä¶ or violin plots:\nsp3 \u0026lt;- ggscatter(data, x = \u0026#34;x1\u0026#34;, y = \u0026#34;x2\u0026#34;, size = 0.05) ggMarginal(sp3, type = \u0026#34;violin\u0026#34;)  ‚Ä¶ or densigram plots:\nsp4 \u0026lt;- ggscatter(data, x = \u0026#34;x1\u0026#34;, y = \u0026#34;x2\u0026#34;, size = 0.05) ggMarginal(sp4, type = \u0026#34;densigram\u0026#34;)  par(mfrow = c(1, 2), pty = \u0026#34;s\u0026#34;) plot(pobs(Simul.x[, 1]), pobs(Simul.x[, 2]), pch = \u0026#34;.\u0026#34;) plot(cop, type = \u0026#34;contour\u0026#34;, margins = \u0026#34;unif\u0026#34;)  data2 \u0026lt;- tibble(x1 = pobs(Simul.x[, 1]), x2 = pobs(Simul.x[, 2])) sp3 \u0026lt;- ggscatter(data2, x = \u0026#34;x1\u0026#34;, y = \u0026#34;x2\u0026#34;, size = 0.05) ggMarginal(sp3, type = \u0026#34;densigram\u0026#34;) Ranks have uniform margins as expected.\n\\(\\adv\\) Fitting bivariate copulas #  \\(\\adv\\) Using R #  The VineCopula package offers many functions for fitting copulas:\n BiCopKDE: A kernel density estimate of the copula density is visualised. BiCopSelect: Estimates the parameters of a bivariate copula for a set of families and selects the best fitting model (using either AIC or BIC). Returns an object of class BiCop. BiCopEst: Estimates parameters of a bivariate copula with a prespecified family. Returns an object of class BiCop. Estimation can be done by  maximum likelihood (method = ‚Äúmle‚Äù) or inversion of the empirical Kendall‚Äôs tau (method = ‚Äúitau,‚Äù only available for one-parameter families).   BiCopGofTest: Goodness-of-Fit tests for bivariate copulas.  \\(\\adv\\) Case study with VineCopula: SUVA data #  \\(\\adv\\) Kernel density #  BiCopKDE(pobs(SUVAcom[, 1]), pobs(SUVAcom[, 2]))  BiCopKDE(pobs(SUVAcom[, 1]), pobs(SUVAcom[, 2]), margins = \u0026#34;unif\u0026#34;) \\(\\adv\\) Fitting #  SUVAselect \u0026lt;- BiCopSelect(pobs(SUVAcom[, 1]), pobs(SUVAcom[, 2]), selectioncrit = \u0026#34;BIC\u0026#34;) summary(SUVAselect) ## Family ## ------ ## No: 10 ## Name: BB8 ## ## Parameter(s) ## ------------ ## par: 3.08 ## par2: 0.99 ## Dependence measures ## ------------------- ## Kendall's tau: 0.52 (empirical = 0.52, p value \u0026lt; 0.01) ## Upper TD: 0 ## Lower TD: 0 ## ## Fit statistics ## -------------- ## logLik: 488.99 ## AIC: -973.98 ## BIC: -964  For comparison:\nSUVAsurvClayton \u0026lt;- BiCopEst(pobs(SUVAcom[, 1]), pobs(SUVAcom[, 2]), family = 13) summary(SUVAsurvClayton) ## Family ## ------ ## No: 13 ## Name: Survival Clayton ## ## Parameter(s) ## ------------ ## par: 2.07 ## ## Dependence measures ## ------------------- ## Kendall's tau: 0.51 (empirical = 0.52, p value \u0026lt; 0.01) ## Upper TD: 0.72 ## Lower TD: 0 ## ## Fit statistics ## -------------- ## logLik: 478.14 ## AIC: -954.28 ## BIC: -949.29  \\(\\adv\\) Further GOF #  White‚Äôs test:\nBiCopGofTest(pobs(SUVAcom[, 1]), pobs(SUVAcom[, 2]), SUVAselect) ## Error in BiCopGofTest(pobs(SUVAcom[, 1]), pobs(SUVAcom[, 2]), SUVAselect): The goodness-of-fit test based on White's information matrix equality is not implemented for the BB copulas.  BiCopGofTest(pobs(SUVAcom[, 1]), pobs(SUVAcom[, 2]), SUVAsurvClayton) ## $statistic ## [,1] ## [1,] 1.343252 ## ## $p.value ## [1] 0.23  We cannot perform the test on the BB8 copula (R informs us that The goodness-of-fit test based on White's information matrix equality is not implemented for the BB copulas.), but also cannot reject the null on the Survival Clayton.\n BiCopGofTest(pobs(SUVAcom[, 1]), pobs(SUVAcom[, 2]), SUVAselect, method = \u0026#34;kendall\u0026#34;) ## $p.value.CvM ## [1] 0.16 ## ## $p.value.KS ## [1] 0.22 ## ## $statistic.CvM ## [1] 0.08458458 ## ## $statistic.KS ## [1] 0.738938   BiCopGofTest(pobs(SUVAcom[, 1]), pobs(SUVAcom[, 2]), SUVAsurvClayton, method = \u0026#34;kendall\u0026#34;) ## $p.value.CvM ## [1] 0.21 ## ## $p.value.KS ## [1] 0.18 ## ## $statistic.CvM ## [1] 0.08144334 ## ## $statistic.KS ## [1] 0.7731786   par(mfrow = c(1, 2), pty = \u0026#34;s\u0026#34;) BiCopKDE(pobs(SUVAcom[, 1]), pobs(SUVAcom[, 2]), margins = \u0026#34;unif\u0026#34;) plot(SUVAselect, type = \u0026#34;contour\u0026#34;, margins = \u0026#34;unif\u0026#34;)  par(mfrow = c(1, 2), pty = \u0026#34;s\u0026#34;) BiCopKDE(pobs(SUVAcom[, 1]), pobs(SUVAcom[, 2]), margins = \u0026#34;unif\u0026#34;) plot(SUVAsurvClayton, type = \u0026#34;contour\u0026#34;, margins = \u0026#34;unif\u0026#34;) \\(\\adv\\) Case study with censoring: ISO data #  \\(\\adv\\) Insurance company losses and expenses #   Data consists of 1,500 general liability claims. Provided by the Insurance Services Office, Inc. \\(X_{1}\\) is the loss, or amount of claims paid. \\(X_{2}\\) are the ALAE, or Allocated Loss Adjustment Expenses. Policy contains policy limits, and hence, censoring. \\(\\delta\\) is the indicator for censoring so that the observed data consists of  $$\\left( x_{1i},x_{2i},\\delta _{i}\\right) \\text{ for }i=1,2,...,1500.$$\nWe will fit this data mostly ‚Äúby hand‚Äù for transparency (and since we need to allow for censoring). R codes are provided separately.\n Loss.ALAE \u0026lt;- read_csv(\u0026#34;LossData-FV.csv\u0026#34;) ## Rows: 1500 Columns: 4 ## -- Column specification ---------------------------------------------- ## Delimiter: \u0026quot;,\u0026quot; ## dbl (4): LOSS, ALAE, LIMIT, CENSOR ## ## i Use `spec()` to retrieve the full column specification for this data. ## i Specify the column types or set `show_col_types = FALSE` to quiet this message.  as_tibble(Loss.ALAE) ## # A tibble: 1,500 x 4 ## LOSS ALAE LIMIT CENSOR ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 10 3806 500000 0 ## 2 24 5658 1000000 0 ## 3 45 321 1000000 0 ## 4 51 305 500000 0 ## 5 60 758 500000 0 ## 6 74 8768 2000000 0 ## 7 75 1805 500000 0 ## 8 78 78 500000 0 ## 9 87 46534 500000 0 ## 10 100 489 300000 0 ## # ... with 1,490 more rows  \\(\\adv\\) Summary statistics of data #      Loss ALAE Policy Limit Loss (Uncensored) Loss (Censored)     Number 1,500 1,500 1,352 1,466 34   Mean 41,208 12,588 559,098 37,110 217,491   Median 12,000 5,471 500,000 11,048 100,000   Std Deviation 102,748 28,146 418,649 92,513 258,205   Minimum 10 15 5,000 10 5,000   Maximum 2,173,595 501,863 7,500,000 2,173,595 1,000,000   25th quantile 4,000 2,333 300,000 3,750 50,000   75th quantile 35,000 12,577 1,000,000 32,000 300,000    \\(\\adv\\) loss vs ALAE #  par(mfrow = c(1, 2), pty = \u0026#34;s\u0026#34;) plot(log(Loss.ALAE$LOSS), log(Loss.ALAE$ALAE), main = \u0026#34;LOSS vs ALAE on a log scale\u0026#34;, pch = 20) plot(pobs(log(Loss.ALAE$LOSS)), pobs(log(Loss.ALAE$ALAE)), main = \u0026#34;Empirical copula of LOSS vs ALAE\u0026#34;, pch = 20)  par(mfrow = c(1, 2), pty = \u0026#34;s\u0026#34;) BiCopKDE(pobs(log(Loss.ALAE$LOSS)), pobs(log(Loss.ALAE$ALAE))) BiCopKDE(pobs(log(Loss.ALAE$LOSS)), pobs(log(Loss.ALAE$ALAE)), margins = \u0026#34;unif\u0026#34;) \\(\\adv\\) Maximum likelihood estimation #   Case 1: loss variable is not censored, i.e.¬†\\(\\delta =0.\\)  $$f\\left( x_{1},x_{2}\\right) =f_{1}\\left( x_{1}\\right) f_{2}\\left( x_{2}\\right) C_{12}\\left( F_{1}\\left( x_{1}\\right) ,F_{2}\\left( x_{2}\\right) \\right)$$\nwhere \\(C_{12}(u_1,u_2)=\\frac{\\partial C(u_1,u_2)}{\\partial u_1\\partial u_2}\\).\n Case 2: loss variable is censored, i.e.¬†\\(\\delta =1.\\)  $$\\begin{array}{rcl} \\frac{\\partial }{\\partial x_{2}}P\\left( X_{1}\u0026gt;x_{1},X_{2}\\leq x_{2}\\right) \u0026amp;=\u0026amp;\\frac{\\partial }{\\partial x_{2}}\\left[ F_{2}\\left( x_{2}\\right) -F\\left(x_{1},x_{2}\\right) \\right] \\\\\\\\ \u0026amp;=\u0026amp;f_{2}\\left( x_{2}\\right) -\\frac{\\partial}{\\partial x_2}F\\left( x_{1},x_{2}\\right) \\\\\\\\ \u0026amp;=\u0026amp;f_{2}\\left( x_{2}\\right) \\left[ 1-C_2\\left( F_{1}\\left(x_{1}\\right) ,F_{2}\\left( x_{2}\\right) \\right) \\right] \\end{array}$$\nwhere \\(C_i(u_1,u_2)=\\frac{\\partial C(u_1,u_2)}{\\partial u_i}\\).\n\\(\\adv\\) Choice of marginals and copulas #   Pareto marginals: \\(F_{k}\\left( x_{k}\\right) =1-\\left( \\frac{\\lambda_{k}}{\\lambda _{k}+x_{k}}\\right) ^{\\theta _{k}}\\) for \\(k=1,2.\\) and \\(x\u0026gt;0\\). For the copulas, several candidates were used:  \\(\\adv\\) Parameter estimates #  \\(\\adv\\) AIC criterion #    Akaike Information Criterion (AIC)\n  In the absence of a better way to choosing/selecting a copula model, one may use the AIC criterion defined by\n  $$\\text{AIC}=\\left( -2\\ell +2m\\right)$$\nwhere \\(\\ell\\) is the value of maximised log-likelihood, \\(m\\) is the number of parameters estimated, and \\(n\\) is the sample size.\n Lower AIC generally is preferred.  \\(\\adv\\) Summary #  To find the distribution of the sum of dependent random variables with copulas (one approach):\n Fit marginals independently Describe/fit dependence with a copula (roughly)  Get a sense of data (scatterplots, dependence measures) Choose candidate copulas For each candidate, estimate parameters via MLE Choose a copula based on nll(highest) or AIC(lowest)   If focusing on the sum, one might do simulations to look at the distributions of aggregates, and how they compare with the original data.  Coefficients of tail dependence #  Motivation #   In insurance and investment applications it is the large outcomes (losses) that particularly tend to occur together, whereas small claims tend to be fairly independent This is one of the reasons why tails (especially right tails) tend to be fatter in financial applications. A good understanding of tail behaviour is hence very important. It is possible to derive tail properties due to dependence from a copula model. The indicator we are considering here is the coefficient of tail dependence. Tail dependence can take values between 0 (no dependence) and 1 (full dependence). VineCopula::BiCopPar2TailDep computes the theoretical tail dependence coefficients for copulas of the BiCop family.  Coefficient of lower tail dependence #  The coefficient of lower tail dependence is defined as $$\\lambda_L=\\lim_{u\\rightarrow 0^+} \\Pr\\left[ X_1 \\le F_{X_1}^{-1}(u) \\left| X_2\\le F_{X_2}^{-1}(u)\\right.\\right] = \\lim_{u\\rightarrow 0^+} \\frac{C(u,u)}{u}.$$ Examples (note the extensive use of de l‚ÄôHospital rule):\n$$\\begin{array}{rcl} \\lambda_L^{\\text{ind}}\u0026amp;=\u0026amp;\\lim_{u\\rightarrow 0^+} \\frac{u\\cdot u}{u} =\\lim_{u\\rightarrow 0^+} u = 0 \\\\ \\lambda_L^{\\text{Clayton}}\u0026amp;=\u0026amp;\\lim_{u\\rightarrow 0^+} \\frac{\\left( 2 u^{-\\theta}-1\\right)^{-1/\\theta}}{u} = \\lim_{u\\rightarrow 0^+} \\frac{\\left( 2 -u^\\theta\\right)^{-1/\\theta}u}{u}\\\\ \u0026amp;=\u0026amp;\\lim_{u\\rightarrow 0^+} \\left( 2 -u^\\theta\\right)^{-1/\\theta}=2^{-1/\\theta} = \\left(\\frac{1}{2}\\right)^{\\frac{1}{\\theta}} \\end{array}$$\nThe lower tail of the Clayton copula is comprehensive in that it allows for tail coefficients of 0 (as \\(\\theta \\rightarrow 0\\) ) to 1 (as \\(\\theta \\rightarrow \\infty\\) )\nCoefficient of upper tail dependence #  The coefficient of upper tail dependence is defined similarly but using the survival copula, which yields\n$$\\begin{array}{rcl} \\lambda_U\u0026amp;=\u0026amp;\\lim_{u\\rightarrow 1^-} \\Pr\\left[ X_1 \\ge F_{X_1}^{-1}(u) \\left| X_2\\ge F_{X_2}^{-1}(u)\\right.\\right] \\\\ \u0026amp;=\u0026amp; \\lim_{u\\rightarrow 1^-} \\frac{\\overline{C}(1-u,1-u)}{1-u} = \\lim_{u\\rightarrow 0^+} \\frac{\\overline{C}(u,u)}{u}. \\end{array}$$\nNote \\(\\overline{C}(u,u)=2u-1+C(1-u,1-u).\\) Examples:\n$$\\begin{array}{rcl} \\lambda_U^{\\text{ind}}\u0026amp;=\u0026amp;\\lim_{u\\rightarrow 1^-} \\frac{1-2u+u^2}{1-u} = \\lim_{u\\rightarrow 1^-} 1-u =0 \\\\ \\lambda_U^{\\text{Frank}}\u0026amp;=\u0026amp;\\lim_{u\\rightarrow 1^-} \\frac{1-2u+1/\\theta\\left[\\log(e^{2\\theta u}-2e^{\\theta u}+e^\\theta)-\\log(e^\\theta-1)\\right]}{1-u} \\\\ \u0026amp;=\u0026amp;\\lim_{u\\rightarrow 1^-} \\frac{-2+1/\\theta\\cdot(2\\theta e^{2\\theta u}-2\\theta e^{\\theta u})/(e^{2\\theta u}-2e^{\\theta u}+e^\\theta)}{-1} \\\\ \u0026amp;=\u0026amp;0 \\end{array}$$\nReferences #  Avanzi, Benjamin, Luke C. Cassar, and Bernard Wong. 2011. ‚ÄúModelling Dependence in Insurance Claims Processes with L√©vy Copulas.‚Äù ASTIN Bulletin 41 (2): 575‚Äì609.\n Kurowicka, D., and H. Joe. 2011. Dependence Modeling Vine Copula Handbook.\n Nelsen, R. B. 1999. An Introduction to Copulas. Springer.\n Sklar, A. 1959. ‚ÄúFonctions de r√©partition √† $n$ Dimensions Et Leurs Marges.‚Äù Publications de l‚ÄôInstitut de Statistique de l‚ÄôUniversit√© de Paris 8: 229‚Äì31.\n Vigen, Tyler. 2015. ‚ÄúSpurious Correlations (Last Accessed on 18 March 2015 on http://www.tylervigen.com).‚Äù\n Wikipedia. 2020. ‚ÄúCopula: Probability Theory.‚Äù\n  "},{"id":15,"href":"/docs/1-claims-modelling/m6-extreme-value-theory/","title":"M6 Extreme Value Theory","section":"Claims Modelling (CS2 Section 1)","content":"Introduction #  Extreme events #   financial (and claims) data is often leptokurtic severity (more peaked - heavy tailed - than normal) reasons include heteroscedasticity (stochastic volatility) almost by definition, extreme events happen with low frequency but extreme events are the ones causing to financial distress, possibly ruin accurate fitting is crucial for capital and risk management (risk based capital, reinsurance, ART such as securitisation via CAT bonds) fitting presents specific issues:  there is little data to work from when fitting a whole distribution, the big bulk of the data (around the mode) overpowers observations in the tails in the optimisation routine (MLE), leading to poor fit in the tail sometimes (in fact, almost always as we will see) a good distribution for the far tail is different from that for the more common outcomes    Framework #   Consider iid random variables \\(X_i\\), \\(i=1,\\ldots,n\\), with df \\(F\\) We denote the order statistics $$X_{n,n} \\le X_{n-1,n} \\le \\ldots \\le X_{1,n}$$ such that $$X_{n,n}=\\min(X_1,\\ldots,X_n) \\text{ and }X_{1,n}=\\max(X_1,\\ldots,X_n).$$ Specifically, we may be interested in  $$\\begin{array}{rcl} \\text{average of }k\\text{ largest losses} \u0026amp;=\u0026amp; \\sum_{r=1}^k\\frac{X_{r,n}}{k} \\\\ \\text{empirical mean excess function above }u \u0026amp;=\u0026amp; \\sum_{r=1}^{n_u}\\frac{(X_{r,n}-u)}{n_u},\\text{where} \\end{array}$$\n\\(n_u = \\# \\left\\{ 1\\le r\\le n:X_r\u0026gt;u\\right\\}\\)\nMotivating example (P. Embrechts, Resnick, and Samorodnitsky 1999) #  Consider the following situation:\n Let \\(X_i\\) be exponential mean \\(1/\\lambda=10\\) We have \\(n=100\\) observations The maximum observed was 50. How likely is it if the model is correct? What if the maximum had been 100?  We have $$\\Pr[X_{1,n}\u0026gt;x] =1-\\left( \\Pr[X\\le x]\\right)^{100} = 1-\\left( 1-e^{-x/10}\\right)^{100},$$ and hence\n$$\\begin{array}{rcl} \\Pr[X_{1,n}\u0026gt;50] \u0026amp;=\u0026amp; 0.4919, \\\\ \\Pr[X_{1,n}\u0026gt;100] \u0026amp;=\u0026amp; 0.00453. \\end{array}$$\nA limiting argument #  We now perform the following calculations: $$\\Pr\\left[ \\frac{X_{1,n}}{\\alert{10}}-\\alert{\\log n}\\le x\\right] = \\Pr [X_{1,n} \\le 10(x+\\log n)] = \\left(1-\\frac{e^{-x}}{n}\\right)^n,$$ where the elements in red were chosen based on an educated guess, so that $$\\lim_{n\\rightarrow\\infty} \\Pr\\left[\\frac{X_{1,n}}{\\alert{10}}-\\alert{\\log n} \\le x\\right]=\\lim_{n\\rightarrow\\infty} \\left(1-\\frac{e^{-x}}{n}\\right)^n=e^{-e^{-x}} \\equiv \\Lambda(x),$$ which is the df of a Gumbel (or double exponential) random variable.\nGeneral result for \\(X\\) exponential \\((\\lambda)\\) #  We can easily generalise the previous result to any \\(\\lambda\\) and \\(n\\). Noting that 10 was actually \\(1/\\lambda\\) before, $$\\Pr[X_{1,n}\\le x] \\approx \\Lambda \\left( \\lambda x -\\log n\\right).$$ In the limit \\((n \\rightarrow \\infty)\\), the approximation is exact.\nThis means that for large \\(n\\), one can approximate the distribution of the maximum of a set of iid exponentially distributed \\(X_i\\)‚Äôs (with parameter \\(\\lambda\\)) with a Gumbel distribution.\nIndeed, this works well for our example, especially in the tail:\n$$\\begin{array}{rcl} \\Pr[X_{1,n}\u0026gt;50] \u0026amp;\\approx\u0026amp; 1-e^{-e^{-(50/10-\\log 100)}}=0.4902, \\\\ \\Pr[X_{1,n}\u0026gt;100] \u0026amp;\\approx\u0026amp; 0.00453. \\end{array}$$\nQuestions #   This is nice, but not all random variables are exponential. Can we generalise such a limiting approach to any df?  Yes!\n How do we find the norming constants (such as \\(\\lambda\\) and \\(\\log n\\) in the exponential case) in general, that is, how do we find \\(a_n\\) and \\(b_n\\) so that $$\\lim_{n\\rightarrow \\infty} \\Pr \\left[ \\frac{X_{1,n}-b_n}{a_n} \\le x\\right]$$ exists?  We can (there are only three cases to consider), but we won‚Äôt discuss that here (see, e.g., C. A. M. Embrechts P. AND Kl√ºppelberg 1997 for the proofs)\nThis results in an extreme value complement or homologue, for the tails, of the CLT approximation (which works only around the middle of the distribution).\nGeneralised Extreme Value distribution #  The GEV distribution #  Extreme Value Distributions #  The Gumbel distribution \\(\\Lambda(x)\\) derived earlier is a special case of a general distribution family $$H_{\\gamma;\\mu,\\sigma}(x)=\\exp \\left\\{ - \\left(1+\\gamma\\frac{x-\\mu}{\\sigma}\\right)_+^{-\\frac{1}{\\gamma}}\\right\\},\\;\\gamma \\in \\mathbb{R}, \\mu\\in\\mathbb{R}, \\sigma\u0026gt;0.$$ This Generalised Extreme Value distribution (GEV) encompasses all extreme value distributions, of which we distinguish three cases:\n \\(\\gamma\u0026lt;0\\): upper bounded Weibull (Sometimes referred to as the reverse or reflected Weibull) df with \\(x\u0026lt;\\mu-\\sigma/\\gamma\\) \\(\\gamma=0\\): Gumbel df with \\(x\\in \\mathbb{R}\\) \\(\\gamma\u0026gt;0\\): Fr\u0026rsquo;echet-type df with \\(x\u0026gt;\\mu- \\sigma/\\gamma\\)  The last one relates to the heavy tailed distributions (typically with moments existing only up to a certain order, infinite afterwards), and is the most relevant to actuarial applications.\nThree cases #  (Unit 4, Page 4, IoA 2020) Note \\(\\alpha \\equiv \\mu\\) and \\(\\beta \\equiv \\sigma\\)\n GenEV \u0026lt;- function(x, alpha, beta, gamma) { 1/beta * (1 + gamma * (x - alpha)/beta)^(-(1 + 1/gamma)) * exp(-((1 + gamma * (x - alpha)/beta)^(-1/gamma))) } par(mfrow = c(1, 3), oma = c(0, 0, 3, 0)) plot(-4000:2000/1000, GenEV(-4000:2000/1000, 0, 1, -0.5), main = \u0026#34;Density with gamma=-0.5 (Weibull)\u0026#34;, xlim = c(-4, 4), xlab = \u0026#34;\u0026#34;, ylab = \u0026#34;\u0026#34;, cex = 1) plot(-4000:4000/1000, GenEV(-4000:4000/1000, 0, 1, 1e-05), main = \u0026#34;Density of GEV with gamma=0 (Gumbel)\u0026#34;, xlim = c(-4, 4), xlab = \u0026#34;\u0026#34;, ylab = \u0026#34;\u0026#34;, cex = 1) plot(-2000:4000/1000, GenEV(-2000:4000/1000, 0, 1, 0.5), main = \u0026#34;Density of GEV with gamma=0.5 (Fr√©chet)\u0026#34;, xlim = c(-4, 4), xlab = \u0026#34;\u0026#34;, ylab = \u0026#34;\u0026#34;, cex = 1) mtext(\u0026#34;Density of GEV with mu=0, sigma=1, and various values of gamma\u0026#34;, outer = TRUE, cex = 1.5)  Estimation: Block Maxima #   The EVD is a distribution of maxima To fit it we need a sample of maxima To achieve this, the data is transformed into block maxima  assume we have \\(n\\) data points consider blocks of length \\(m\\) the block maxima are the equal to the maximum observation within a block we have then a sample size of \\(n/m\\)   The block maxima are then fitted to the EVD The larger the block size, the closer to the asymptotic distribution we get, but the less data we have: conflicting requirements In R, this can be done efficiently with the function fevd of the package extRemes (Gilleland and Katz 2016)  Case study #  We consider three simulated data sets with \\(n=12000\\), expected value $ \\(10,000\\) and variance $ \\(9,000^2\\):\n   month beta gamm logg     \\(\\vdots\\)      61 12862.84 33493.02 18773.47   62 408.64 6070.20 7564.21   63 2404.87 26093.36 5083.10   64 4753.73 3773.61 2070.88   65 594.57 3885.25 43601.62   66 16805.64 6881.97 6163.90   67 4335.52 5163.38 8158.35   68 5575.15 24401.76 19172.13   \\(\\vdots\\)       \\(\\quad\\) Simulated data sets are provided along with the lecture notes.\n claims \u0026lt;- as_tibble(read_excel(\u0026#34;simulated-claims.xlsx\u0026#34;)) summary(claims$beta) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0 2102 7585 10078 16532 33287  summary(claims$gamm) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 6.12 3566.46 7668.73 10107.70 14070.25 90712.14  summary(claims$logg) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 432.7 4611.7 7374.7 9931.9 12124.2 157377.1   plotdist(claims$beta)  plotdist(claims$gamm)  plotdist(claims$logg) Block maxima #  Creating block maxima\n# block maxima claims$block \u0026lt;- (claims$month - 1)%/%12 + 1 # %/% gives the integer part of the result of the division blockmax \u0026lt;- tibble(betablock = aggregate(beta ~ block, claims, max)$beta, gammblock = aggregate(gamm ~ block, claims, max)$gamm, loggblock = aggregate(logg ~ block, claims, max)$logg) We now have a sample of \\(12000/12=1000\\) block maxima.\nWhat do these look like in comparison with the original data?\n claims \u0026lt;- as_tibble(read_excel(\u0026#34;simulated-claims.xlsx\u0026#34;)) summary(blockmax$betablock) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 11376 23803 27155 26606 30035 33287  summary(blockmax$gammblock) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 9521 21036 27278 28479 34299 90712  summary(blockmax$loggblock) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 9326 18618 24590 28587 33259 157377   plotdist(blockmax$betablock)  plotdist(blockmax$gammblock)  plotdist(blockmax$loggblock)  par(mfrow = c(1, 2)) plot(density(claims$beta), main = \u0026#34;Density of the beta claims\u0026#34;, xlab = \u0026#34;Claim amounts ($)\u0026#34;, xlim = c(0, max(claims$beta))) plot(density(blockmax$betablock), main = \u0026#34;Density of the beta block maxima\u0026#34;, xlab = \u0026#34;Maximums over consecutive periods of 12 months ($)\u0026#34;, xlim = c(0, max(claims$beta)))  par(mfrow = c(1, 2)) plot(density(claims$gamm), main = \u0026#34;Density of the gammormal claims\u0026#34;, xlab = \u0026#34;Claim amounts ($)\u0026#34;, xlim = c(0, max(claims$gamm))) plot(density(blockmax$gammblock), main = \u0026#34;Density of the gammormal block maxima\u0026#34;, xlab = \u0026#34;Maximums over consecutive periods of 12 months ($)\u0026#34;, xlim = c(0, max(claims$gamm)))  par(mfrow = c(1, 2)) plot(density(claims$logg), main = \u0026#34;Density of the log-gamma claims\u0026#34;, xlab = \u0026#34;Claim amounts ($)\u0026#34;, xlim = c(0, max(claims$logg))) plot(density(blockmax$loggblock), main = \u0026#34;Density of the log-gamma block maxima\u0026#34;, xlab = \u0026#34;Maximums over consecutive periods of 12 months ($)\u0026#34;, xlim = c(0, max(claims$logg)))  par(mfrow = c(1, 3)) plot(density(blockmax$betablock), main = \u0026#34;Density of the beta block maxima\u0026#34;, xlab = \u0026#34;Maximums over consecutive periods of 12 months ($)\u0026#34;) plot(density(blockmax$gammblock), main = \u0026#34;Density of the gammormal block maxima\u0026#34;, xlab = \u0026#34;Maximums over consecutive periods of 12 months ($)\u0026#34;) plot(density(blockmax$loggblock), main = \u0026#34;Density of the loggamma block maxima\u0026#34;, xlab = \u0026#34;Maximums over consecutive periods of 12 months ($)\u0026#34;) Convergence with increasing block sizes #  # block sizes blocksizes \u0026lt;- c(1, 2, 3, 4, 6, 12, 18, 24, 30, 60, c(1:20 * 120)) results \u0026lt;- c() for (i in blocksizes) { # number of full blocks to work with numbers \u0026lt;- floor(length(claims$beta)/max(blocksizes)) # trimming claims vector claims_trimmed \u0026lt;- claims[1:(numbers * i), ] # block maxima claims_trimmed$block \u0026lt;- (claims_trimmed$month - 1)%/%i + 1 blockmax2 \u0026lt;- tibble(betablock2 = aggregate(beta ~ block, claims_trimmed, max)$beta, gammblock2 = aggregate(gamm ~ block, claims_trimmed, max)$gamm, loggblock2 = aggregate(logg ~ block, claims_trimmed, max)$logg) # fitting fit.beta2 \u0026lt;- fevd(blockmax2$betablock2) fit.gamm2 \u0026lt;- fevd(blockmax2$gammblock2) fit.logg2 \u0026lt;- fevd(blockmax2$loggblock2) results \u0026lt;- rbind(results, c(i, as.double(fit.beta2$results$par[1:3]), as.double(fit.gamm2$results$par[1:3]), as.double(fit.logg2$results$par[1:3]))) }  par(mfrow = c(1, 3)) plot(results[, 1], results[, 2], pch = 20, main = \u0026#34;Convergence of mu (betablock)\u0026#34;, xlab = \u0026#34;Block size\u0026#34;) plot(results[, 1], results[, 3], pch = 20, main = \u0026#34;Convergence of sigma (betablock)\u0026#34;, xlab = \u0026#34;Block size\u0026#34;) plot(results[, 1], results[, 4], pch = 20, main = \u0026#34;Convergence of gamma (betablock)\u0026#34;, xlab = \u0026#34;Block size\u0026#34;) par(mfrow = c(1, 3)) plot(results[, 1], results[, 5], pch = 20, main = \u0026#34;Convergence of mu (gammblock)\u0026#34;, xlab = \u0026#34;Block size\u0026#34;) plot(results[, 1], results[, 6], pch = 20, main = \u0026#34;Convergence of sigma (gammblock)\u0026#34;, xlab = \u0026#34;Block size\u0026#34;) plot(results[, 1], results[, 7], pch = 20, main = \u0026#34;Convergence of gamma (gammblock)\u0026#34;, xlab = \u0026#34;Block size\u0026#34;) par(mfrow = c(1, 3)) plot(results[, 1], results[, 8], pch = 20, main = \u0026#34;Convergence of mu (loggblock)\u0026#34;, xlab = \u0026#34;Block size\u0026#34;) plot(results[, 1], results[, 9], pch = 20, main = \u0026#34;Convergence of sigma (loggblock)\u0026#34;, xlab = \u0026#34;Block size\u0026#34;) plot(results[, 1], results[, 10], pch = 20, main = \u0026#34;Convergence of gamma (loggblock)\u0026#34;, xlab = \u0026#34;Block size\u0026#34;)    Fitting results #  For example for the logg BM we use the following code:\n fit.logg \u0026lt;- fevd(blockmax$loggblock) will perform the fit fit.logg will display the results of the fitting plot(fit.logg) will plot goodness of fit graphical analysis plot(fit.logg,\u0026quot;trace\u0026quot;) will plot the likelihood function around the chosen parameters  More details can be found in Gilleland and Katz (2016).\n Results (MLE with standard errors in parentheses) are\n    location \\(\\mu\\) scale \\(\\sigma\\) shape \\(\\gamma\\)     beta 25644.93 (163.26) 4695.76 (130.07) -0.6012 (0.0212)   gamma 24069.68 (288.51) 8225.98 (215.80) -0.0243 (0.0222)   loggamma 21466.22 (305.26) 8496.86 (251.97) 0.2299 (0.0278)    Note:\n These correspond to the three distinct cases introduced before as expected (look at the sign of \\(\\gamma\\)) Whether \\(\\gamma\\neq 0\\) for gamma can be formally tested in different ways (see Gilleland and Katz 2016), but the standard error for \\(\\gamma^{\\text{gamm}}\\) suggests a null hypothesis of \\(\\gamma^{\\text{gamm}} = 0\\) would not be rejected.   fit.beta \u0026lt;- fevd(blockmax$betablock) fit.beta ... ## Estimated parameters: ## location scale shape  ## 25644.9318770 4695.7556362 -0.6011579  ##  ## Standard Error Estimates: ## location scale shape  ## 163.26448100 130.06953700 0.02120487  ##  ## Estimated parameter covariance matrix. ## location scale shape ## location 26655.290757 -8463.804100 -1.2827274803 ## scale -8463.804100 16918.084456 -1.7746705470 ## shape -1.282727 -1.774671 0.0004496465 ##  ## AIC = 19385.23  ##  ## BIC = 19399.96 ...  plot(fit.beta)  plot(fit.beta, \u0026#34;trace\u0026#34;)  fit.gamm \u0026lt;- fevd(blockmax$gammblock) fit.gamm ... ## Estimated parameters: ## location scale shape  ## 2.406968e+04 8.225984e+03 -2.428561e-02  ##  ## Standard Error Estimates: ## location scale shape  ## 288.51126800 215.80132748 0.02220903  ##  ## Estimated parameter covariance matrix. ## location scale shape ## location 83238.751764 21650.966827 -2.0732936569 ## scale 21650.966827 46570.212942 -1.4944274045 ## shape -2.073294 -1.494427 0.0004932409 ##  ## AIC = 21127.17  ##  ## BIC = 21141.89 ...  plot(fit.gamm)  plot(fit.gamm, \u0026#34;trace\u0026#34;)  fit.logg \u0026lt;- fevd(blockmax$loggblock) fit.logg ... ## Estimated parameters: ## location scale shape  ## 21466.224267 8496.863050 0.229869  ##  ## Standard Error Estimates: ## location scale shape  ## 305.26319626 251.96813338 0.02779193  ##  ## Estimated parameter covariance matrix. ## location scale shape ## location 93185.618989 45516.589594 -2.5704729001 ## scale 45516.589594 63487.940241 -0.5065090268 ## shape -2.570473 -0.506509 0.0007723913 ##  ## AIC = 21491.68  ##  ## BIC = 21506.4 ...  plot(fit.logg)  plot(fit.logg, \u0026#34;trace\u0026#34;) Moments and quantiles #  Moments #  Let \\(Z\\) follow a GEV( \\(\\mu\\) , \\(\\sigma\\) , \\(\\gamma\\) ). We have then $$E[Z]=\\left\\{ \\begin{array}{rc} \\mu+\\sigma\\left( \\Gamma(1-\\gamma)-1\\right)/\\gamma, \u0026amp;\\gamma\\neq 0, \\gamma \u0026lt;1, \\\\ \\mu+\\sigma \\cdot e,\u0026amp;\\gamma =0, \\\\ \\infty , \u0026amp;\\gamma \\ge 1. \\end{array} \\right.$$ $$Var(Z) = \\left\\{ \\begin{array}{rc} \\sigma^2\\left[ \\Gamma(1-2\\gamma)-\\Gamma(1-\\gamma)^2\\right]/\\gamma^2, \u0026amp;\\gamma\\neq 0, \\gamma \u0026lt;1/2, \\\\ \\sigma^2 \\cdot\\pi/6,\u0026amp;\\gamma =0, \\\\ \\infty , \u0026amp;\\gamma \\ge 1/2. \\end{array} \\right.$$\n    Empirical Mean Empirical Variance Empirical Mean Empirical Variance     beta 26606.41 18381480 26475.58 18584454   gamm 28141.08 222758930 28242.00 249582072   logg 28586.58 239426765 28842.05 280329757    (Empirical moments are calculated from the data, Theoretical are calculated with parameters estimates as per above)\n\\(t\\)-year events #   Imagine you are focused on yearly outcomes, and want to obtain the outcome that should occur no more often than 200 years (such as in solvency requirements in Australia) This is a ‚Äúone-in-200-years‚Äù event, and is referred to as ‚Äú$t$-year event‚Äù (where \\(t=200\\) above), or outcome with a \\(t\\)-year ‚Äúreturn period.‚Äù This is obviously impossible to estimate from data for large \\(t\\), as it is simply not available - there is not enough information about the tail Although it is possible to get \\(t\\)-year quantiles from the GEV, it is not obvious how to relate those to the above problem as the GEV provides the distribution of a maximum over a certain period (not total losses, unless we are fitting portfolio level data). The interpretation is more natural with climate data, where you have daily maximum temperatures, and you may want to get a maximum with 200 day return period (and the R package allows for seasonality).  Quantiles #  For \\(t\\)-year events we seek \\(u_t\\) such that \\(H_{\\gamma;\\mu,\\sigma}(u_t)=1-1/t\\), that is, $$u_t = H_{\\gamma;\\mu,\\sigma}^{-1}\\left(1-1/t\\right).$$ We have $$ u_t= \\begin{cases} \\mu+\\frac{\\sigma}{\\gamma}\\left[ \\left(-\\frac{1}{\\log (1-1/t)}\\right)^\\gamma -1 \\right]\\quad \u0026amp; \\text{ for }\\gamma \\neq 0. \\\\\n\\mu+\\sigma \\log \\left(-\\frac{1}{\\log (1-1/t)}\\right) \u0026amp; \\text{ for }\\gamma = 0. \\end{cases} $$\n Close examination of those formulas tells us that the curvature of \\(u_t\\) as a function of \\(\\log \\left(-1/\\log (1-1/t)\\right)\\) depends on the distribution:\n \\(\\gamma\u0026lt;0\\) (Weibull): concave with asymptotic upper limit as \\(t\\rightarrow \\infty\\) at its upper bound \\(\\mu-\\sigma/\\gamma\\) \\(\\gamma=0\\) (Gumbel): linear \\(\\gamma\u0026gt;0\\) (Fr\u0026rsquo;echet) convex with no finite upper bound  Case study: quantiles #  First, note that empirical \\((1-1/t)\\)-th quantiles in the data ($X_{n(1/t),n}$) are\nret.per \u0026lt;- c(10, 50, 100, 200, 500) Xnn \u0026lt;- cbind(beta = sort(claims$beta, decreasing = TRUE), gamm = sort(claims$gamm, decreasing = TRUE), logg = sort(claims$logg, decreasing = TRUE)) empquant \u0026lt;- data.frame(Xnn[length(claims$beta) * 1/ret.per, ]) rownames(empquant) \u0026lt;- ret.per empquant ## beta gamm logg ## 10 24308.60 21986.51 19253.06 ## 50 30526.74 35395.89 35621.35 ## 100 31753.68 40474.56 44351.62 ## 200 32235.67 45118.02 54439.08 ## 500 32790.91 53731.02 71437.34  These are not to be compared with the quantiles of the GEV. The GEV models the distribution of maxima.\n Empirical quantiles in the blocks:\nXnn2 \u0026lt;- cbind(betablock = sort(blockmax$betablock, decreasing = TRUE), gammblock = sort(blockmax$gammblock, decreasing = TRUE), loggblock = sort(blockmax$loggblock, decreasing = TRUE)) empquantblock \u0026lt;- data.frame(Xnn2[length(blockmax$betablock) * 1/ret.per, ]) rownames(empquantblock) \u0026lt;- ret.per empquantblock ## betablock gammblock loggblock ## 10 31845.47 41496.29 45836.54 ## 50 32869.52 55387.21 74562.03 ## 100 33042.45 59653.57 88275.45 ## 200 33186.66 63289.25 99494.12 ## 500 33249.33 78734.31 138514.02   Theoretical quantiles in the blocks (maxima):\nparm \u0026lt;- cbind(as.double(fit.beta$results$par[1:3]), as.double(fit.gamm$results$par[1:3]), as.double(fit.logg$results$par[1:3])) ut.theo \u0026lt;- data.frame(cbind(beta = parm[1, 1] + parm[2, 1]/parm[3, 1] * ((-1/(log(1 - 1/ret.per)))^parm[3, 1] - 1), gamm = parm[1, 2] + parm[2, 2]/parm[3, 2] * ((-1/(log(1 - 1/ret.per)))^parm[3, 2] - 1), logg = parm[1, 3] + parm[2, 3]/parm[3, 3] * ((-1/(log(1 - 1/ret.per)))^parm[3, 3] - 1))) rownames(ut.theo) \u0026lt;- ret.per ut.theo ## beta gamm logg ## 10 31436.86 42084.41 46508.37 ## 50 32707.95 54693.10 75140.65 ## 100 32964.40 59873.26 90920.27 ## 200 33132.46 64947.90 109373.87 ## 500 33269.71 71513.08 138703.54   Now this can be done with extRemes (with confidence intervals!)\nreturn.level(fit.beta, return.period = ret.per, do.ci = TRUE) ... ## 95% lower CI Estimate 95% upper CI ## 10-year return level 31263.52 31436.86 31610.19 ## 50-year return level 32573.09 32707.95 32842.81 ## 100-year return level 32821.90 32964.40 33106.90 ## 200-year return level 32978.01 33132.46 33286.90 ## 500-year return level 33099.84 33269.71 33439.57 ...  return.level(fit.gamm, return.period = ret.per, do.ci = TRUE) ... ## 95% lower CI Estimate 95% upper CI ## 10-year return level 40867.26 42084.41 43301.56 ## 50-year return level 52139.71 54693.10 57246.48 ## 100-year return level 56484.68 59873.26 63261.84 ## 200-year return level 60578.48 64947.90 69317.31 ## 500-year return level 65635.36 71513.08 77390.81 ...   return.level(fit.logg, return.period = ret.per, do.ci = TRUE) ... ## 95% lower CI Estimate 95% upper CI ## 10-year return level 44199.82 46508.37 48816.91 ## 50-year return level 68082.59 75140.65 82198.71 ## 100-year return level 80241.17 90920.27 101599.36 ## 200-year return level 93750.30 109373.87 124997.45 ## 500-year return level 113913.35 138703.54 163493.72 ...  Theoretical figures are close (as they should!), but obviously the theoretical versions are smoothed and will be less likely to underestimate extreme events (see, e.g., the big difference in the gamma and log-gamma high return period figures).\nGeneralised Pareto distribution #  Asymptotic properties of the tails #  Main result: Asymptotic properties of the tails #   Let us now consider a distribution \\(F\\) of losses \\(Y\\). We are interested in the asymptotic behaviour of the tail so that we can approximate it due to lack of data [see also Chapter 4 of Wuthrich (2020); \\(S_{sc}\\) vs \\(S_{lc}\\)] If we were pricing an excess of loss (or stop loss on a portfolio), the attachment point or retention level or limit could be expressed as a \\(t\\)-year event or quantile $$ u_t =F^{-1}\\left( 1-\\frac{1}{t}\\right).$$ The tail is the distribution beyond this point. We characterise the tail with the help of the distribution function of the excess over threshold \\(u\\) $$F_{u}(x) = \\Pr[Y-u\\le x|Y\u0026gt;u].$$ The main result is that, for high \\(u\\), \\(F_{u}(x)\\) can be approximated by a Generalised Pareto. (technically, as \\(u\\) increases to the maximum possible value of \\(x\\) then the absolute difference between both distributions is asymptotically zero)  The GP distribution #  Generalised Pareto distribution #   The Generalised Pareto distribution function is given by $$G_{\\gamma,\\sigma}(x) = \\left\\{ \\begin{array}{cc} \\displaystyle 1-\\left( 1+ \\gamma \\frac{x}{\\sigma}\\right)_+^{-\\frac{1}{\\gamma}} \u0026amp; \\gamma \\neq 0 \\\\ \\\\ \\displaystyle 1-\\exp \\left( -\\frac{x}{\\sigma}\\right) \u0026amp; \\gamma = 0 \\end{array}\\right.$$ for a scale parameter \\(\\sigma\u0026gt;0\\). We distinguish again three cases:  \\(\\gamma\u0026lt;0\\) (upper bound, also referred to as ‚ÄúPareto Type II‚Äù): light tail, \\(X \\in (0,\\sigma/|\\gamma|)\\) \\(\\gamma=0\\) (exponential): base case \\(X \\in \\mathbb{R}\\) \\(\\gamma\u0026gt;0\\) (Pareto): heavy tail, \\(X \\in \\mathbb{R}^+\\)    GP df #  GP pdf #   From Wikipedia\n note \\(\\xi \\equiv \\gamma\\)  Moments #  The first central moments of \\(X\\), the excess of \\(u\\), are\n$$\\begin{array}{rcl} E[X] \u0026amp;=\u0026amp; \\frac{\\sigma}{1-\\gamma}, \\quad \\gamma \u0026lt;1, \\\\ Var(X) \u0026amp;=\u0026amp; \\frac{\\sigma^2}{(1-\\gamma)^2(1-2\\gamma)}, \\quad \\gamma \u0026lt;1/2. \\end{array}$$\nNote:\n The first four \\(k\\) moments exist only for \\(\\gamma\u0026lt;1/k\\). \\(E[X]\\) is the stop loss premium (since \\(X\\) is the excess over threshold \\(u\\))  Estimation #  Estimation: choice of threshold \\(u\\) #  We seek to estimate the tail $$\\overline{F}(u+x)=\\overline{F}(u)\\overline{F}_u(x)$$ for a fixed large value \\(u\\) and all \\(x\\ge 0\\). To do so, we need to estimate:\n Probability of exceeding \\(u\\) $$ \\widehat{\\overline{F}(u)} = \\frac{n_u}{n}$$ which will be more accurate for \\(u\\) not too large. Then for given value of \\(u\\) $$\\widehat{\\overline{F}_u(x)}=G_{\\gamma,\\sigma}(x)$$ where \\(\\hat{\\gamma}\\) and \\(\\hat{\\sigma}(u)\\) are estimated from the data in the tail (via MLE). This approximation will work only for large \\(u\\) (as the result is an asymptotic result for \\(u\\rightarrow \\infty\\)).  Estimation: choice of threshold \\(u\\) #  The choice of \\(u\\) presents conflicting requirements for \\(u\\):\n larger \\(u\\) will lead to a better approximation from a distributional perspective but larger \\(u\\) reduces the amount of data to estimate \\(\\overline{F}(u)\\) , \\(\\gamma\\) and \\(\\sigma\\)  The choice is often also based on the following further considerations:\n mean-excess plot: Pareto should be linear (slope same sign as \\(\\gamma\\)) empirical df on a doubly logarithmic scale: Pareto should be linear Hill plot (Wuthrich 2020, 76): stability of Pareto parameter stability of \\(\\hat{\\gamma}\\) for different choices of \\(u\\) (note this requires a full fit for each value of \\(u\\) but the extRemes package does that automatically) goodness of fit for different choices of \\(u\\)  Unfortunately there is no ‚Äúoptimal‚Äù (or ‚Äúautomatic‚Äù) procedure for choosing \\(u\\).\nCase study #  Case study: workers compensation medical costs #   Example of real actuarial data from Avanzi, Cassar, and Wong (2011) Data were provided by the SUVA (Swiss workers compensation insurer) Random sample of 5% of accident claims in construction sector with accident year 1999 (developped as of 2003) Claims are medical costs In Avanzi, Cassar, and Wong (2011) we model those claims with a Gumbel distribution, which suggests heavy tails, which is confirmed by the following graphs. This, however, wasn‚Äôt the main objective of the paper, but was good enough for our purposes.  SUVA \u0026lt;- read_excel(\u0026#34;SUVA.xls\u0026#34;) as_tibble(SUVA) ... ## # A tibble: 2,326 x 2 ## medcosts dailyallow ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 407 0 ## 2 12591 13742 ...  Data exploration #  plot(SUVA$medcosts, ylab = \u0026#34;Medical costs (CHF)\u0026#34;, cex = 1.25, cex.lab = 1.25, col = \u0026#34;darkblue\u0026#34;, bg = \u0026#34;lightblue\u0026#34;, pch = 21)  plot(log(SUVA$medcosts), ylab = \u0026#34;Medical costs (log CHF)\u0026#34;, cex = 1.25, cex.lab = 1.25, col = \u0026#34;darkblue\u0026#34;, bg = \u0026#34;lightblue\u0026#34;, pch = 21)  extRemes::qqnorm(log(SUVA$medcosts), cex.lab = 1.25) Size of tail #  numbexc \u0026lt;- c() for (i in 250:20000) { numbexc \u0026lt;- c(numbexc, length(SUVA$medcosts[SUVA$medcosts \u0026gt; i])) } par(mfrow = c(1, 2)) plot(250:20000, numbexc, xlab = \u0026#34;Threshold ($)\u0026#34;, ylab = \u0026#34;Number of claims exceeding the threshold\u0026#34;, col = \u0026#34;darkblue\u0026#34;, bg = \u0026#34;lightblue\u0026#34;, pch = 21) plot(250:20000, numbexc/length(SUVA$medcosts), xlab = \u0026#34;Threshold ($)\u0026#34;, ylab = \u0026#34;Proportion of claims above threshold\u0026#34;, col = \u0026#34;darkblue\u0026#34;, bg = \u0026#34;lightblue\u0026#34;, pch = 21)  Mean-excess plot #  extRemes::mrlplot(SUVA$medcosts, xlim = c(250, 20000)) Empirical \\(\\overline{F}\\) on log log scale #  evir::emplot(SUVA$medcosts, alog = \u0026#34;xy\u0026#34;, labels = TRUE) Hill plot #  evir::hill(SUVA$medcosts) Stability of \\(\\hat{\\gamma}\\) #  extRemes::threshrange.plot(SUVA$medcosts, r = c(250, 20000), nint = 80) Estimation results for \\(u=500\\) #  fit.SUVA.500 \u0026lt;- fevd(SUVA$medcosts, threshold = 500, type = \u0026#34;GP\u0026#34;, time.units = \u0026#34;1/year\u0026#34;) fit.SUVA.500 ... ## Estimated parameters: ## scale shape  ## 1272.3550452 0.9193739  ##  ## Standard Error Estimates: ## scale shape  ## 126.8004198 0.0849976  ##  ## Estimated parameter covariance matrix. ## scale shape ## scale 16078.346449 -6.336002009 ## shape -6.336002 0.007224592 ##  ## AIC = 11006.09  ##  ## BIC = 11014.93 ...  numbexc[501 - 250] # n_u ## [1] 616  numbexc[501 - 250]/length(SUVA$medcosts) ## [1] 0.2648323  plot(fit.SUVA.500)  Estimation results for \\(u=5000\\) #  fit.SUVA.5000 \u0026lt;- fevd(SUVA$medcosts, threshold = 5000, type = \u0026#34;GP\u0026#34;, time.units = \u0026#34;1/year\u0026#34;) fit.SUVA.5000 ... ## Estimated parameters: ## scale shape  ## 8696.13321 0.37075  ##  ## Standard Error Estimates: ## scale shape  ## 1433.7846731 0.1389474  ##  ## Estimated parameter covariance matrix. ## scale shape ## scale 2055738.4888 -135.87231048 ## shape -135.8723 0.01930639 ##  ## AIC = 2633.894  ##  ## BIC = 2639.566 ...  numbexc[5001 - 250] # n_u ## [1] 126  numbexc[5001 - 250]/length(SUVA$medcosts) ## [1] 0.05417025  plot(fit.SUVA.5000)  Estimation results for \\(u=10000\\) #  fit.SUVA.10000 \u0026lt;- fevd(SUVA$medcosts, threshold = 10000, type = \u0026#34;GP\u0026#34;, time.units = \u0026#34;1/year\u0026#34;) fit.SUVA.10000 ... ## Estimated parameters: ## scale shape  ## 1.633393e+04 7.885925e-02  ##  ## Standard Error Estimates: ## scale shape  ## 3536.9225104 0.1633972  ##  ## Estimated parameter covariance matrix. ## scale shape ## scale 12509820.8444 -446.98853032 ## shape -446.9885 0.02669863 ##  ## AIC = 1408.892  ##  ## BIC = 1413.24 ...  numbexc[10001 - 250] # n_u ## [1] 65  numbexc[10001 - 250]/length(SUVA$medcosts) ## [1] 0.02794497  plot(fit.SUVA.10000)  Estimation results for \\(u=12000\\) #  fit.SUVA.12000 \u0026lt;- fevd(SUVA$medcosts, threshold = 12000, type = \u0026#34;GP\u0026#34;, time.units = \u0026#34;1/year\u0026#34;) fit.SUVA.12000 ... ## Estimated parameters: ## scale shape  ## 1.904175e+04 -4.859538e-03  ##  ## Standard Error Estimates: ## scale shape  ## 4264.9534894 0.1600393  ##  ## Estimated parameter covariance matrix. ## scale shape ## scale 18189828.2668 -534.46782297 ## shape -534.4678 0.02561257 ##  ## AIC = 1238.267  ##  ## BIC = 1242.353 ...  numbexc[12001 - 250] # n_u ## [1] 57  numbexc[12001 - 250]/length(SUVA$medcosts) ## [1] 0.02450559  plot(fit.SUVA.12000)  Measures of tail weight #   Existence of moments (e.g.¬†gamma vs Pareto) Limiting density ratios: relative value of density functions at the far end of the upper tail of two distributions Hazard rates \\(\\lambda(x)=f(x)/(1-F(x))\\): constant for exponential, decreasing for heavy tails Log-log plot: linear decrease for heavy tails Mean excess function: linear increase for heavy tails  Limiting density ratios: gamma example #  plot(1:10000/1000, dgamma(1:10000/1000, shape = 0.75, rate = 0.75)/dexp(1:10000/1000, 1), xlab = \u0026#34;x\u0026#34;, main = \u0026#34;ratio of densities gamma(alpha,beta) to exponential(1)\u0026#34;, ylab = \u0026#34;\u0026#34;, type = \u0026#34;l\u0026#34;, col = \u0026#34;blue\u0026#34;, ylim = c(0, 3.5), xlim = c(0, 6), lwd = 2, cex = 1.5) text(5.5, 2, \u0026#34;alpha=beta=0.75\u0026#34;, col = \u0026#34;blue\u0026#34;, cex = 1.5) lines(1:10000/1000, dgamma(1:10000/1000, shape = 1, rate = 1)/dexp(1:10000/1000, 1), type = \u0026#34;l\u0026#34;, col = \u0026#34;black\u0026#34;, lwd = 2) text(5.5, 1.2, \u0026#34;alpha=beta=1\u0026#34;, col = \u0026#34;black\u0026#34;, cex = 1.5) lines(1:10000/1000, dgamma(1:10000/1000, shape = 2, rate = 2)/dexp(1:10000/1000, 1), type = \u0026#34;l\u0026#34;, col = \u0026#34;red\u0026#34;, lwd = 2) text(5.5, 0.3, \u0026#34;alpha=beta=2\u0026#34;, col = \u0026#34;red\u0026#34;, cex = 1.5) lines(1:10000/1000, dgamma(1:10000/1000, shape = 10, rate = 10)/dexp(1:10000/1000, 1), type = \u0026#34;l\u0026#34;, col = \u0026#34;green\u0026#34;, lwd = 2) text(2.8, 0.2, \u0026#34;alpha=beta=10\u0026#34;, col = \u0026#34;green\u0026#34;, cex = 1.5) lines(1:10000/1000, dgamma(1:10000/1000, shape = 0.5, rate = 0.5)/dexp(1:10000/1000, 1), type = \u0026#34;l\u0026#34;, col = \u0026#34;magenta\u0026#34;, lwd = 2) text(5.5, 3.5, \u0026#34;alpha=beta=0.5\u0026#34;, col = \u0026#34;magenta\u0026#34;, cex = 1.5)  Hazard rates: gamma example with fixed mean #  plot(1:10000/1000, dgamma(1:10000/1000, shape = 0.5, rate = 0.5)/(1 - pgamma(1:10000/1000, shape = 0.5, rate = 0.5)), xlab = \u0026#34;x\u0026#34;, main = \u0026#34;failure rates for gamma(alpha,beta)\u0026#34;, ylab = \u0026#34;\u0026#34;, type = \u0026#34;l\u0026#34;, col = \u0026#34;blue\u0026#34;, ylim = c(0, 4), xlim = c(0, 5), lwd = 2, cex = 1.5) text(4.5, 0.75, \u0026#34;alpha=beta=0.5\u0026#34;, col = \u0026#34;blue\u0026#34;, cex = 1.5) lines(1:10000/1000, dgamma(1:10000/1000, shape = 1, rate = 1)/(1 - pgamma(1:10000/1000, shape = 1, rate = 1)), type = \u0026#34;l\u0026#34;, col = \u0026#34;black\u0026#34;, lwd = 2) text(4.5, 1.2, \u0026#34;alpha=beta=1\u0026#34;, col = \u0026#34;black\u0026#34;, cex = 1.5) lines(1:10000/1000, dgamma(1:10000/1000, shape = 2, rate = 2)/(1 - pgamma(1:10000/1000, shape = 2, rate = 2)), type = \u0026#34;l\u0026#34;, col = \u0026#34;red\u0026#34;, lwd = 2) text(4.5, 2, \u0026#34;alpha=beta=2\u0026#34;, col = \u0026#34;red\u0026#34;, cex = 1.5) lines(1:10000/1000, dgamma(1:10000/1000, shape = 4, rate = 4)/(1 - pgamma(1:10000/1000, shape = 4, rate = 4)), type = \u0026#34;l\u0026#34;, col = \u0026#34;green\u0026#34;, lwd = 2) text(4.5, 3.6, \u0026#34;alpha=beta=4\u0026#34;, col = \u0026#34;green\u0026#34;, cex = 1.5) lines(1:10000/1000, dgamma(1:10000/1000, shape = 0.1, rate = 0.1)/(1 - pgamma(1:10000/1000, shape = 0.1, rate = 0.1)), type = \u0026#34;l\u0026#34;, col = \u0026#34;magenta\u0026#34;, lwd = 2) text(4.5, 0.35, \u0026#34;alpha=beta=0.1\u0026#34;, col = \u0026#34;magenta\u0026#34;, cex = 1.5)  Hazard rates: gamma example with fixed mean #  plot(1:10000/1000, dgamma(1:10000/1000, shape = 0.75, rate = 1)/(1 - pgamma(1:10000/1000, shape = 0.75, rate = 1)), xlab = \u0026#34;x\u0026#34;, main = \u0026#34;failure rates for gamma(alpha,beta=1)\u0026#34;, ylab = \u0026#34;\u0026#34;, type = \u0026#34;l\u0026#34;, col = \u0026#34;blue\u0026#34;, ylim = c(0, 2.5), xlim = c(0, 5), lwd = 2, cex = 1.5) text(2.5, 1.15, \u0026#34;alpha=0.75\u0026#34;, col = \u0026#34;blue\u0026#34;, cex = 1.5) lines(1:10000/1000, dgamma(1:10000/1000, shape = 1, rate = 1)/(1 - pgamma(1:10000/1000, shape = 1, rate = 1)), type = \u0026#34;l\u0026#34;, col = \u0026#34;black\u0026#34;, lwd = 2, cex = 1.5) text(2.5, 0.9, \u0026#34;alpha=1\u0026#34;, col = \u0026#34;black\u0026#34;, cex = 1.5) lines(1:10000/1000, dgamma(1:10000/1000, shape = 2, rate = 1)/(1 - pgamma(1:10000/1000, shape = 2, rate = 1)), type = \u0026#34;l\u0026#34;, col = \u0026#34;red\u0026#34;, lwd = 2, cex = 1.5) text(2.5, 0.6, \u0026#34;alpha=2\u0026#34;, col = \u0026#34;red\u0026#34;, cex = 1.5) lines(1:10000/1000, dgamma(1:10000/1000, shape = 4, rate = 1)/(1 - pgamma(1:10000/1000, shape = 4, rate = 1)), type = \u0026#34;l\u0026#34;, col = \u0026#34;green\u0026#34;, lwd = 2) text(2.5, 0.15, \u0026#34;alpha=4\u0026#34;, col = \u0026#34;green\u0026#34;, cex = 1.5) lines(1:10000/1000, dgamma(1:10000/1000, shape = 0.01, rate = 1)/(1 - pgamma(1:10000/1000, shape = 0.01, rate = 1)), type = \u0026#34;l\u0026#34;, col = \u0026#34;magenta\u0026#34;, lwd = 2) text(2.5, 1.45, \u0026#34;alpha=0.01\u0026#34;, col = \u0026#34;magenta\u0026#34;, cex = 1.5)  References #  Avanzi, Benjamin, Luke C. Cassar, and Bernard Wong. 2011. ‚ÄúModelling Dependence in Insurance Claims Processes with L√©vy Copulas.‚Äù ASTIN Bulletin 41 (2): 575‚Äì609.\n Embrechts, C. AND Mikosch, P. AND Kl√ºppelberg. 1997. Modelling Extremal Events for Insurance and Finance. Springer, Berlin.\n Embrechts, Paul, Sidney I. Resnick, and Gennady Samorodnitsky. 1999. ‚ÄúExtreme Value Theory as a Risk Management Tool.‚Äù North American Actuarial Journal 3 (2): 30‚Äì41.\n Gilleland, Eric, and Richard W. Katz. 2016. ‚ÄúextRemes 2.0: An Extreme Value Analysis Package in R.‚Äù Journal of Statistical Software 72 (8).\n IoA. 2020. Course Notes and Core Reading for Subject Cs2 Models. The Institute of Actuaries.\n Wuthrich, Mario V. 2020. ‚ÄúNon-Life Insurance: Mathematics \u0026amp; Statistics.‚Äù Lecture notes. RiskLab, ETH Zurich; Swiss Finance Institute.\n  "}]